{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Digital Analyst working at SEEK, specialising in Online and Tech. I work closely with product teams to identify opportunities to optimise product and provide value to candidates through data driven insights. I am a proactive, creative person with a passion for learning. I am constantly striving to improving my skill-set. I do this by immersing myself in new challenges. I am passionate about local music, python, data, the arts and travel. My spare time is currently spent educating myself in Deep Learning Skillset Python SQL Adobe Analytics Power BI Web Analytics Data Visualisation Coming soon.. Deep Learning","title":"About"},{"location":"#about","text":"Digital Analyst working at SEEK, specialising in Online and Tech. I work closely with product teams to identify opportunities to optimise product and provide value to candidates through data driven insights. I am a proactive, creative person with a passion for learning. I am constantly striving to improving my skill-set. I do this by immersing myself in new challenges. I am passionate about local music, python, data, the arts and travel. My spare time is currently spent educating myself in Deep Learning","title":"About"},{"location":"#skillset","text":"Python SQL Adobe Analytics Power BI Web Analytics Data Visualisation","title":"Skillset"},{"location":"#coming-soon","text":"Deep Learning","title":"Coming soon.."},{"location":"fastai%20deep%20learning%202020/","text":"About fastai: Deep Learning for Coders 2020 course I am currently making my way through the 2020 version of the course. My primary goal is to share my progress as I go. My secondary goal is to apply my knowledge to some projects and areas I am interested in.","title":"About"},{"location":"fastai%20deep%20learning%202020/#about","text":"","title":"About"},{"location":"fastai%20deep%20learning%202020/#fastai-deep-learning-for-coders-2020-course","text":"I am currently making my way through the 2020 version of the course. My primary goal is to share my progress as I go. My secondary goal is to apply my knowledge to some projects and areas I am interested in.","title":"fastai: Deep Learning for Coders 2020 course"},{"location":"fastai%20deep%20learning%202020/lesson%2001/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 1: Deep Learning for Coders 06-09-2020 This notebook will go over some of the practical material discussed in lesson 1 of the fastai 2020 course. I am going to cover 2 examples here - classification from image data and tabular data Example 1: Computer Vision from fastai.vision.all import * from pathlib import Path download one of the standard datasets provided by fasta, the Oxford-IIIT Pet Dataset which is a 37 category pet dataset with roughly 200 images for each class. path = untar_data ( URLs . PETS ) / 'images' path Path('/storage/data/oxford-iiit-pet/images') Create an ImageDataLoader Fastai needs to know where to get the image labels from. Normally these labels are part of the filenames or folder structure. In this case the filenames contain the animal breeds. american_bulldog_146.jpg and Siamese_56.jpg for example it so happens that cat breeds start with an uppercase letter. For this example, we will not classify all 37 breeds. We will instead classify whether the images are of dogs or cats. First define a function is_cat that checks whether the first letter in the image label is uppercase. is_cat returns a boolean value that will be used as the new image label. - from_name_func applies the function to our data to create the labels we need. valid_pct=0.2 : hold 20% of the data aside for the validation set, 80% will be used for the training set item_tfms=Resize(224) : resize images to 224x224 fastai provides item transforms (applied to each image in this case) and batch transform which are applied to a batch of items at a time. # check a few image names to confirm that # dog images start with lowercase filenames # cat images start with uppercase filenames files = get_image_files ( path ) files [ 0 ], files [ 6 ] (Path('/storage/data/oxford-iiit-pet/images/american_bulldog_146.jpg'), Path('/storage/data/oxford-iiit-pet/images/Siamese_56.jpg')) def is_cat ( x ): return x [ 0 ] . isupper () dls = ImageDataLoaders . from_name_func ( path , get_image_files ( path ), valid_pct = 0.2 , seed = 42 , label_func = is_cat , item_tfms = Resize ( 224 )) # check our function works! is_cat ( files [ 0 ] . name ), is_cat ( files [ 6 ] . name ) (False, True) # take a look at some of the data dls . show_batch ( max_n = 6 ) # check number of items in training and test datasets len ( dls . train_ds ), len ( dls . valid_ds ) (5912, 1478) Create a cnn_learner using the resnet34 architecture resnet paper this is a pretrained learner, which means when we fit the model, we will not need to train from scratch, rather, we will only fine tune the model by default, freeze_epochs is set to 1 learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.158638 0.023677 0.008119 00:45 epoch train_loss valid_loss error_rate time 0 0.061309 0.013070 0.004736 01:01 learn . show_results () Testing the model Lets load in a picture of a cat and a dog to check the model img_01 = Path . cwd () / 'lesson1_assets/img_1.PNG' img_02 = Path . cwd () / 'lesson1_assets/img_2.PNG' im1 = PILImage . create ( img_01 ) im2 = PILImage . create ( img_02 ) im1 . to_thumb ( 192 ) im2 . to_thumb ( 192 ) learn.predict() returns 3 things, the label ( True / False in our case), the class that scored highest (1 or 0) and then the probabilities of each class. As a reminder, let's use learn.dls.vocab.o2i to check how the classes are mapped to our labels # show how our labels map to our vocab learn . dls . vocab . o2i {False: 0, True: 1} is_cat , clas , probs = learn . predict ( im1 ) is_cat , clas , probs ('True', tensor(1), tensor([2.7169e-10, 1.0000e+00])) Let's check both images... images = [ im1 , im2 ] for i in images : is_cat , _ , probs = learn . predict ( i ) print ( f \"Is this a cat?: { is_cat } .\" ) print ( f \"Probability it's a cat: { probs [ 1 ] . item () : .5f } \" ) Is this a cat?: True. Probability it's a cat: 1.00000 Is this a cat?: False. Probability it's a cat: 0.00000 Example 2: Tabular For this example we will use the Adults data set. Our goal is to predict if a person is earning above or below $50k per year using information such as age, working class, education and occupation. There are about 32K rows in the dataset. from fastai.tabular.all import * path = untar_data ( URLs . ADULT_SAMPLE ) path Path('/storage/data/adult_sample') df = pd . read_csv ( path / 'adult.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 49 Private 101320 Assoc-acdm 12.0 Married-civ-spouse NaN Wife White Female 0 1902 40 United-States >=50k 1 44 Private 236746 Masters 14.0 Divorced Exec-managerial Not-in-family White Male 10520 0 45 United-States >=50k 2 38 Private 96185 HS-grad NaN Divorced NaN Unmarried Black Female 0 0 32 United-States <50k 3 38 Self-emp-inc 112847 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband Asian-Pac-Islander Male 0 0 40 United-States >=50k 4 42 Self-emp-not-inc 82297 7th-8th NaN Married-civ-spouse Other-service Wife Black Female 0 0 50 United-States <50k len ( df ) 32561 Create an TabularDataLoader Again we create data loader using the path . We need to specify some information such as the y variable (the value we want to predict), and we also need to specify which columns contain categorical values and which contain continuous variables. Do this using cat_names and cont_names . Some data processing needs to occur.. - we need to specify how to handle missing data. Info below from the docs - FillMissing by default sets fill_strategy=median - Normalize will normalize the continuous variables (substract the mean and divide by the std) - Categorify transform the categorical variables to something similar to pd.Categorical This is another classification problem. Our goal is to predict whether a persons salary was below 50k (0) or above (1). dls = TabularDataLoaders . from_csv ( path / 'adult.csv' , path = path , y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) I'm going to keep some of the data at the end of the set aside for testing. df[:32500] will select from row 0 to 32500, the remaining rows will not be seen by the model splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df [: 32500 ])) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) dls = to . dataloaders ( bs = 64 ) dls . show_batch () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary 0 Private HS-grad Married-spouse-absent Other-service Unmarried White False 32.000000 128016.002920 9.0 <50k 1 Private 7th-8th Married-civ-spouse Exec-managerial Wife White False 52.000000 194259.000001 4.0 <50k 2 Private Some-college Widowed Exec-managerial Unmarried White False 31.000000 73796.004491 10.0 <50k 3 Private Some-college Separated Other-service Not-in-family White False 64.000001 114993.998143 10.0 <50k 4 Self-emp-not-inc Assoc-voc Married-civ-spouse Prof-specialty Husband White False 68.000000 116902.996854 11.0 <50k 5 Private Bachelors Married-civ-spouse Prof-specialty Husband White False 42.000000 190178.999991 13.0 >=50k 6 Self-emp-not-inc Prof-school Married-civ-spouse Prof-specialty Husband White False 66.000000 291362.001320 15.0 <50k 7 Self-emp-not-inc Bachelors Married-civ-spouse Sales Husband White False 63.000001 298249.000475 13.0 >=50k 8 Private Masters Divorced Tech-support Not-in-family White False 47.000000 606752.001736 14.0 <50k 9 State-gov Bachelors Married-civ-spouse Exec-managerial Husband White False 42.000000 345969.005416 13.0 >=50k We can see that our y values have been turned into the categories 0 and 1. dls . y . value_counts () 0 19756 1 6244 Name: salary, dtype: int64 learn = tabular_learner ( dls , metrics = accuracy ) learn . fit_one_cycle ( 3 ) epoch train_loss valid_loss accuracy time 0 0.366288 0.354235 0.834769 00:06 1 0.367247 0.348617 0.839538 00:05 2 0.358275 0.345206 0.839077 00:06 learn . show_results () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary salary_pred 0 5.0 11.0 3.0 11.0 1.0 5.0 1.0 1.494630 1.838917 2.322299 0.0 1.0 1 5.0 12.0 3.0 8.0 1.0 5.0 1.0 -0.558852 -0.690051 -0.421488 0.0 0.0 2 3.0 10.0 3.0 11.0 6.0 3.0 1.0 0.174535 0.000144 1.146390 1.0 1.0 3 5.0 10.0 3.0 5.0 1.0 5.0 1.0 0.467889 -1.014015 1.146390 1.0 1.0 4 5.0 16.0 5.0 9.0 4.0 5.0 1.0 -1.365576 4.387854 -0.029518 0.0 0.0 5 5.0 10.0 1.0 5.0 2.0 5.0 1.0 0.174535 0.616141 1.146390 0.0 0.0 6 5.0 10.0 3.0 2.0 6.0 5.0 1.0 1.494630 0.898075 1.146390 0.0 1.0 7 5.0 12.0 3.0 5.0 6.0 5.0 1.0 0.101196 -0.713219 -0.421488 1.0 1.0 8 7.0 2.0 3.0 4.0 1.0 5.0 1.0 -0.338836 0.932638 -1.205427 0.0 0.0 Check the model by making predictions on the dataset using the data that was held aside which the model has not yet seen. # pick some random rows of the df sample_df = df . iloc [[ 32513 , 32542 , 32553 ]] sample_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 32513 23 Private 209955 HS-grad 9.0 Never-married Craft-repair Not-in-family White Male 0 0 40 United-States <50k 32542 34 Private 98283 Prof-school 15.0 Never-married Tech-support Not-in-family Asian-Pac-Islander Male 0 1564 40 India >=50k 32553 35 Self-emp-inc 135436 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States >=50k Lets loop through these rows and make predictions, printing out the predicted class, the probabilities and the actual class. for i , r in sample_df . iterrows (): row , clas , probs = learn . predict ( r ) print ( f 'the predicted class is { clas } ' ) print ( f 'with a probability of { probs } ' ) print ( f 'the actual class was { r . salary } ' ) the predicted class is 0 with a probability of tensor([0.9911, 0.0089]) the actual class was <50k the predicted class is 0 with a probability of tensor([0.6258, 0.3742]) the actual class was >=50k the predicted class is 1 with a probability of tensor([0.0919, 0.9081]) the actual class was >=50k","title":"Lesson 01"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#lesson-1-deep-learning-for-coders","text":"06-09-2020 This notebook will go over some of the practical material discussed in lesson 1 of the fastai 2020 course. I am going to cover 2 examples here - classification from image data and tabular data","title":"Lesson 1: Deep Learning for Coders"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#example-1-computer-vision","text":"from fastai.vision.all import * from pathlib import Path download one of the standard datasets provided by fasta, the Oxford-IIIT Pet Dataset which is a 37 category pet dataset with roughly 200 images for each class. path = untar_data ( URLs . PETS ) / 'images' path Path('/storage/data/oxford-iiit-pet/images')","title":"Example 1: Computer Vision"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#create-an-imagedataloader","text":"Fastai needs to know where to get the image labels from. Normally these labels are part of the filenames or folder structure. In this case the filenames contain the animal breeds. american_bulldog_146.jpg and Siamese_56.jpg for example it so happens that cat breeds start with an uppercase letter. For this example, we will not classify all 37 breeds. We will instead classify whether the images are of dogs or cats. First define a function is_cat that checks whether the first letter in the image label is uppercase. is_cat returns a boolean value that will be used as the new image label. - from_name_func applies the function to our data to create the labels we need. valid_pct=0.2 : hold 20% of the data aside for the validation set, 80% will be used for the training set item_tfms=Resize(224) : resize images to 224x224 fastai provides item transforms (applied to each image in this case) and batch transform which are applied to a batch of items at a time. # check a few image names to confirm that # dog images start with lowercase filenames # cat images start with uppercase filenames files = get_image_files ( path ) files [ 0 ], files [ 6 ] (Path('/storage/data/oxford-iiit-pet/images/american_bulldog_146.jpg'), Path('/storage/data/oxford-iiit-pet/images/Siamese_56.jpg')) def is_cat ( x ): return x [ 0 ] . isupper () dls = ImageDataLoaders . from_name_func ( path , get_image_files ( path ), valid_pct = 0.2 , seed = 42 , label_func = is_cat , item_tfms = Resize ( 224 )) # check our function works! is_cat ( files [ 0 ] . name ), is_cat ( files [ 6 ] . name ) (False, True) # take a look at some of the data dls . show_batch ( max_n = 6 ) # check number of items in training and test datasets len ( dls . train_ds ), len ( dls . valid_ds ) (5912, 1478)","title":"Create an ImageDataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#create-a-cnn_learner","text":"using the resnet34 architecture resnet paper this is a pretrained learner, which means when we fit the model, we will not need to train from scratch, rather, we will only fine tune the model by default, freeze_epochs is set to 1 learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.158638 0.023677 0.008119 00:45 epoch train_loss valid_loss error_rate time 0 0.061309 0.013070 0.004736 01:01 learn . show_results ()","title":"Create a cnn_learner"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#testing-the-model","text":"Lets load in a picture of a cat and a dog to check the model img_01 = Path . cwd () / 'lesson1_assets/img_1.PNG' img_02 = Path . cwd () / 'lesson1_assets/img_2.PNG' im1 = PILImage . create ( img_01 ) im2 = PILImage . create ( img_02 ) im1 . to_thumb ( 192 ) im2 . to_thumb ( 192 ) learn.predict() returns 3 things, the label ( True / False in our case), the class that scored highest (1 or 0) and then the probabilities of each class. As a reminder, let's use learn.dls.vocab.o2i to check how the classes are mapped to our labels # show how our labels map to our vocab learn . dls . vocab . o2i {False: 0, True: 1} is_cat , clas , probs = learn . predict ( im1 ) is_cat , clas , probs ('True', tensor(1), tensor([2.7169e-10, 1.0000e+00])) Let's check both images... images = [ im1 , im2 ] for i in images : is_cat , _ , probs = learn . predict ( i ) print ( f \"Is this a cat?: { is_cat } .\" ) print ( f \"Probability it's a cat: { probs [ 1 ] . item () : .5f } \" ) Is this a cat?: True. Probability it's a cat: 1.00000 Is this a cat?: False. Probability it's a cat: 0.00000","title":"Testing the model"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#example-2-tabular","text":"For this example we will use the Adults data set. Our goal is to predict if a person is earning above or below $50k per year using information such as age, working class, education and occupation. There are about 32K rows in the dataset. from fastai.tabular.all import * path = untar_data ( URLs . ADULT_SAMPLE ) path Path('/storage/data/adult_sample') df = pd . read_csv ( path / 'adult.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 49 Private 101320 Assoc-acdm 12.0 Married-civ-spouse NaN Wife White Female 0 1902 40 United-States >=50k 1 44 Private 236746 Masters 14.0 Divorced Exec-managerial Not-in-family White Male 10520 0 45 United-States >=50k 2 38 Private 96185 HS-grad NaN Divorced NaN Unmarried Black Female 0 0 32 United-States <50k 3 38 Self-emp-inc 112847 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband Asian-Pac-Islander Male 0 0 40 United-States >=50k 4 42 Self-emp-not-inc 82297 7th-8th NaN Married-civ-spouse Other-service Wife Black Female 0 0 50 United-States <50k len ( df ) 32561","title":"Example 2: Tabular"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#create-an-tabulardataloader","text":"Again we create data loader using the path . We need to specify some information such as the y variable (the value we want to predict), and we also need to specify which columns contain categorical values and which contain continuous variables. Do this using cat_names and cont_names . Some data processing needs to occur.. - we need to specify how to handle missing data. Info below from the docs - FillMissing by default sets fill_strategy=median - Normalize will normalize the continuous variables (substract the mean and divide by the std) - Categorify transform the categorical variables to something similar to pd.Categorical This is another classification problem. Our goal is to predict whether a persons salary was below 50k (0) or above (1). dls = TabularDataLoaders . from_csv ( path / 'adult.csv' , path = path , y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) I'm going to keep some of the data at the end of the set aside for testing. df[:32500] will select from row 0 to 32500, the remaining rows will not be seen by the model splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df [: 32500 ])) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) dls = to . dataloaders ( bs = 64 ) dls . show_batch () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary 0 Private HS-grad Married-spouse-absent Other-service Unmarried White False 32.000000 128016.002920 9.0 <50k 1 Private 7th-8th Married-civ-spouse Exec-managerial Wife White False 52.000000 194259.000001 4.0 <50k 2 Private Some-college Widowed Exec-managerial Unmarried White False 31.000000 73796.004491 10.0 <50k 3 Private Some-college Separated Other-service Not-in-family White False 64.000001 114993.998143 10.0 <50k 4 Self-emp-not-inc Assoc-voc Married-civ-spouse Prof-specialty Husband White False 68.000000 116902.996854 11.0 <50k 5 Private Bachelors Married-civ-spouse Prof-specialty Husband White False 42.000000 190178.999991 13.0 >=50k 6 Self-emp-not-inc Prof-school Married-civ-spouse Prof-specialty Husband White False 66.000000 291362.001320 15.0 <50k 7 Self-emp-not-inc Bachelors Married-civ-spouse Sales Husband White False 63.000001 298249.000475 13.0 >=50k 8 Private Masters Divorced Tech-support Not-in-family White False 47.000000 606752.001736 14.0 <50k 9 State-gov Bachelors Married-civ-spouse Exec-managerial Husband White False 42.000000 345969.005416 13.0 >=50k We can see that our y values have been turned into the categories 0 and 1. dls . y . value_counts () 0 19756 1 6244 Name: salary, dtype: int64 learn = tabular_learner ( dls , metrics = accuracy ) learn . fit_one_cycle ( 3 ) epoch train_loss valid_loss accuracy time 0 0.366288 0.354235 0.834769 00:06 1 0.367247 0.348617 0.839538 00:05 2 0.358275 0.345206 0.839077 00:06 learn . show_results () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary salary_pred 0 5.0 11.0 3.0 11.0 1.0 5.0 1.0 1.494630 1.838917 2.322299 0.0 1.0 1 5.0 12.0 3.0 8.0 1.0 5.0 1.0 -0.558852 -0.690051 -0.421488 0.0 0.0 2 3.0 10.0 3.0 11.0 6.0 3.0 1.0 0.174535 0.000144 1.146390 1.0 1.0 3 5.0 10.0 3.0 5.0 1.0 5.0 1.0 0.467889 -1.014015 1.146390 1.0 1.0 4 5.0 16.0 5.0 9.0 4.0 5.0 1.0 -1.365576 4.387854 -0.029518 0.0 0.0 5 5.0 10.0 1.0 5.0 2.0 5.0 1.0 0.174535 0.616141 1.146390 0.0 0.0 6 5.0 10.0 3.0 2.0 6.0 5.0 1.0 1.494630 0.898075 1.146390 0.0 1.0 7 5.0 12.0 3.0 5.0 6.0 5.0 1.0 0.101196 -0.713219 -0.421488 1.0 1.0 8 7.0 2.0 3.0 4.0 1.0 5.0 1.0 -0.338836 0.932638 -1.205427 0.0 0.0","title":"Create an TabularDataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#check-the-model-by-making-predictions-on-the-dataset","text":"using the data that was held aside which the model has not yet seen. # pick some random rows of the df sample_df = df . iloc [[ 32513 , 32542 , 32553 ]] sample_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 32513 23 Private 209955 HS-grad 9.0 Never-married Craft-repair Not-in-family White Male 0 0 40 United-States <50k 32542 34 Private 98283 Prof-school 15.0 Never-married Tech-support Not-in-family Asian-Pac-Islander Male 0 1564 40 India >=50k 32553 35 Self-emp-inc 135436 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States >=50k Lets loop through these rows and make predictions, printing out the predicted class, the probabilities and the actual class. for i , r in sample_df . iterrows (): row , clas , probs = learn . predict ( r ) print ( f 'the predicted class is { clas } ' ) print ( f 'with a probability of { probs } ' ) print ( f 'the actual class was { r . salary } ' ) the predicted class is 0 with a probability of tensor([0.9911, 0.0089]) the actual class was <50k the predicted class is 0 with a probability of tensor([0.6258, 0.3742]) the actual class was >=50k the predicted class is 1 with a probability of tensor([0.0919, 0.9081]) the actual class was >=50k","title":"Check the model by making predictions on the dataset"},{"location":"fastai%20deep%20learning%202020/lesson%2002/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 2: Deep Learning for Coders 12-09-2020 Lesson 2 goes a little deeper into computer vision and the fastai library by going a little deeper into the DataBlocks and DataLoaders . The course notebook uses Bing Images to download image data, the idea being that we curate our own data set for this exercise. Fastai provides some methods and instructions for doing this, you can see details in the notebook I have taken a different route to gathering data. My goal for this notebook is to build a model that is able to classify musical pitches. Audio Data generate audio samples using MIDI. I will not be worrying about sharps/flats simply to reduce complexity use librosa to process audio signals and generate chromagrams using the Constant Q Transform The Constant Q does a good job at isolating pitch but is not sensitive to octaves, thus, all audio samples are in the same octave. stackexchange # !conda install -c conda-forge librosa -y from fastai.vision.all import * from fastai.vision.data import * import matplotlib.pyplot as plt # hi-res plots % config InlineBackend . figure_format = 'retina' from pathlib import Path # sound library & widget to play audio import librosa import librosa.display import IPython.display as ipd Load in Data path = Path . cwd () data_path = path / 'lesson2_assets/cqt_data' audio_file = path / 'lesson2_assets/A3_1.wav' # take a look at the filenames data_path . ls ()[ 1 ] Path('/notebooks/lesson2_assets/cqt_data/A3_1.jpg') sidebar.. generating a chromagram Below is a sample note (A3 on the piano) followed by a demonstration of how to generate a Constant-Q chromagram. The Y axis is displaying the note name for convenience. These were removed to create the training data set. y , sr = librosa . load ( audio_file , mono = True ) ipd . Audio ( y , rate = sr ) Your browser does not support the audio element. doc ( librosa . feature . chroma_cqt ) plt . figure ( figsize = ( 5 , 5 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C , y_axis = 'chroma' ); For comparison, here is the same note visualised using a spectrogram... \"A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.\" wiki There is a lot more information within this plot (such as the fundamental frequency and harmonics above it), however using these images would make our classification task much harder. plt . figure ( figsize = ( 5 , 5 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' ); Image Data The filenames contain the classes that we are trying to predict. We need to define a function that will grab the first 2 characters of each file name to use as labels. This will look familiar to lesson one, excet we have more classes to predict fnames = get_image_files ( data_path ) def label_func ( fname ): return fname . name [: 2 ] label_func ( fnames [ 37 ]) # verify the funciton works 'B3' From Data to DataLoader What is a DataBlock? - \"The data block API takes its name from the way it's designed: every bit needed to build the DataLoaders object (type of inputs, targets, how to label, split...) is encapsulated in a block, and you can mix and match those blocks\" - docs Breaking down the Block - the tutorial in the docs does a good job of stepping through building a block from scratch.. Steps Start with an empty DataBlock . dblock = DataBlock() Tell the block how you want to assemble your items using a get_items function. we will use get_image_files as we did in lesson 1. Let the block know how/where to get our labels from in get_y . the lesson notebook uses parent_label which inherits the label from the parent folder. We need to use the label_func we created for this task. Specify the types of our data (images and labels). ImageBlock and CategoryBlock . blocks=(ImageBlock, CategoryBlock) . Decide how we want to split our data into training and valid datasets. we will randomly split (80% training, 20% validation). Specify any item transforms or batch transforms. audio = DataBlock ( blocks = ( ImageBlock , CategoryBlock ), get_items = get_image_files , splitter = RandomSplitter ( valid_pct = 0.2 , seed = 42 ), get_y = label_func , item_tfms = Resize ( 128 ) ) dls = audio . dataloaders ( data_path , bs = 32 ) dls . show_batch () Sidebar: Data Augmentation and Transforms fastai provides a number of transforms that can be applied to data. In the case of computer vision, augmentation is useful for introducing variety into the dataset. Consider facial recognition, in production, you may not always be dealing with descent portraits; camera angle, lighting, perspective and lighting conditions may vary. Augmentation introduces some of these concepts into our traing and validation set. In the context of the data I am working with, not all transformations may be useful. I would not expect these images to suffer from perspective warping or rotation, however, mirroring the image on the vertical could be useful, as could increasing and decreasing brightness and contrast. Here is a quick example of how to apply some transforms to a batch of images at a time using aug_transforms I am not going to apply any of these for training in this notebook. # no transformation applied dls . valid . show_batch ( max_n = 4 , nrows = 1 ) Here is a list of available transforms.. aug_transforms ( mult = 1.0 , do_flip = True , flip_vert = False , max_rotate = 10.0 , min_zoom = 1.0 , max_zoom = 1.1 , max_lighting = 0.2 , max_warp = 0.2 , p_affine = 0.75 , p_lighting = 0.75 , xtra_tfms = None , size = None , mode = 'bilinear' , pad_mode = 'reflection' , align_corners = True , batch = False , min_scale = 1.0 , ) aug_tfms = aug_transforms ( max_lighting = 0.8 , do_flip = True , flip_vert = True , max_rotate = 0 ) audio = audio . new ( item_tfms = Resize ( 128 ), batch_tfms = aug_tfms ) dls = audio . dataloaders ( data_path ) dls . train . show_batch ( max_n = 4 , nrows = 1 ) Create a cnn_learner and Train the model learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) Before training, let's use learn.lr_find to help find a good learning rate. Two values are returned by running lr_find one tenth of the minimum before the divergence when the slope is the steepest lr_min , lr_steep = learn . lr_find () # plot the values returned by lr_find learn . recorder . plot_lr_find () plt . axvline ( x = lr_min , color = 'red' ) plt . axvline ( x = 3e-3 , color = 'green' ) plt . axvline ( x = lr_steep , color = 'red' ); I'm going to pick a value inbetween the suggested lr's for training. lr_max = 3e-3 learn . fit_one_cycle ( n_epoch = 5 , lr_max = lr_max ) epoch train_loss valid_loss error_rate time 0 2.412440 1.861958 0.642857 00:02 1 1.202116 0.856408 0.261905 00:01 2 0.763907 0.548905 0.190476 00:01 3 0.543375 0.206846 0.095238 00:01 4 0.412221 0.038176 0.023810 00:01 learn . recorder . plot_loss () Interpretation the model is performing quite well, only one note was incorrectly predicted (G3 predicted for F3 actual) interp . plot_top_losses ( k = 4 , figsize = ( 6 , 6 )) interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix () learn . save ( 'base_cqt_model' ) Path('models/base_cqt_model.pth') Fine tune to try improve accuracy.. learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.000892 0.002096 0.000000 00:01 epoch train_loss valid_loss error_rate time 0 0.000499 0.000041 0.000000 00:02 interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix () Summary This wasn't the hardest problem in the world. The Constant Q transform really simplifies pitch detection. I think this is an interesting problem space because there are opportunities to progress these examples; I'd like to try classify all 12 notes (by adding sharps/flats), then try multi-label classification using a phrase of notes, and hopefully then addressing the issue of identifying notes across octaves.","title":"Lesson 02"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#lesson-2-deep-learning-for-coders","text":"12-09-2020 Lesson 2 goes a little deeper into computer vision and the fastai library by going a little deeper into the DataBlocks and DataLoaders . The course notebook uses Bing Images to download image data, the idea being that we curate our own data set for this exercise. Fastai provides some methods and instructions for doing this, you can see details in the notebook I have taken a different route to gathering data. My goal for this notebook is to build a model that is able to classify musical pitches.","title":"Lesson 2: Deep Learning for Coders"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#audio-data","text":"generate audio samples using MIDI. I will not be worrying about sharps/flats simply to reduce complexity use librosa to process audio signals and generate chromagrams using the Constant Q Transform The Constant Q does a good job at isolating pitch but is not sensitive to octaves, thus, all audio samples are in the same octave. stackexchange # !conda install -c conda-forge librosa -y from fastai.vision.all import * from fastai.vision.data import * import matplotlib.pyplot as plt # hi-res plots % config InlineBackend . figure_format = 'retina' from pathlib import Path # sound library & widget to play audio import librosa import librosa.display import IPython.display as ipd","title":"Audio Data"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#load-in-data","text":"path = Path . cwd () data_path = path / 'lesson2_assets/cqt_data' audio_file = path / 'lesson2_assets/A3_1.wav' # take a look at the filenames data_path . ls ()[ 1 ] Path('/notebooks/lesson2_assets/cqt_data/A3_1.jpg')","title":"Load in Data"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#sidebar-generating-a-chromagram","text":"Below is a sample note (A3 on the piano) followed by a demonstration of how to generate a Constant-Q chromagram. The Y axis is displaying the note name for convenience. These were removed to create the training data set. y , sr = librosa . load ( audio_file , mono = True ) ipd . Audio ( y , rate = sr ) Your browser does not support the audio element. doc ( librosa . feature . chroma_cqt ) plt . figure ( figsize = ( 5 , 5 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C , y_axis = 'chroma' ); For comparison, here is the same note visualised using a spectrogram... \"A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.\" wiki There is a lot more information within this plot (such as the fundamental frequency and harmonics above it), however using these images would make our classification task much harder. plt . figure ( figsize = ( 5 , 5 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' );","title":"sidebar.. generating a chromagram"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#image-data","text":"The filenames contain the classes that we are trying to predict. We need to define a function that will grab the first 2 characters of each file name to use as labels. This will look familiar to lesson one, excet we have more classes to predict fnames = get_image_files ( data_path ) def label_func ( fname ): return fname . name [: 2 ] label_func ( fnames [ 37 ]) # verify the funciton works 'B3'","title":"Image Data"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#from-data-to-dataloader","text":"What is a DataBlock? - \"The data block API takes its name from the way it's designed: every bit needed to build the DataLoaders object (type of inputs, targets, how to label, split...) is encapsulated in a block, and you can mix and match those blocks\" - docs Breaking down the Block - the tutorial in the docs does a good job of stepping through building a block from scratch..","title":"From Data to DataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#steps","text":"Start with an empty DataBlock . dblock = DataBlock() Tell the block how you want to assemble your items using a get_items function. we will use get_image_files as we did in lesson 1. Let the block know how/where to get our labels from in get_y . the lesson notebook uses parent_label which inherits the label from the parent folder. We need to use the label_func we created for this task. Specify the types of our data (images and labels). ImageBlock and CategoryBlock . blocks=(ImageBlock, CategoryBlock) . Decide how we want to split our data into training and valid datasets. we will randomly split (80% training, 20% validation). Specify any item transforms or batch transforms. audio = DataBlock ( blocks = ( ImageBlock , CategoryBlock ), get_items = get_image_files , splitter = RandomSplitter ( valid_pct = 0.2 , seed = 42 ), get_y = label_func , item_tfms = Resize ( 128 ) ) dls = audio . dataloaders ( data_path , bs = 32 ) dls . show_batch ()","title":"Steps"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#sidebar-data-augmentation-and-transforms","text":"fastai provides a number of transforms that can be applied to data. In the case of computer vision, augmentation is useful for introducing variety into the dataset. Consider facial recognition, in production, you may not always be dealing with descent portraits; camera angle, lighting, perspective and lighting conditions may vary. Augmentation introduces some of these concepts into our traing and validation set. In the context of the data I am working with, not all transformations may be useful. I would not expect these images to suffer from perspective warping or rotation, however, mirroring the image on the vertical could be useful, as could increasing and decreasing brightness and contrast. Here is a quick example of how to apply some transforms to a batch of images at a time using aug_transforms I am not going to apply any of these for training in this notebook. # no transformation applied dls . valid . show_batch ( max_n = 4 , nrows = 1 ) Here is a list of available transforms.. aug_transforms ( mult = 1.0 , do_flip = True , flip_vert = False , max_rotate = 10.0 , min_zoom = 1.0 , max_zoom = 1.1 , max_lighting = 0.2 , max_warp = 0.2 , p_affine = 0.75 , p_lighting = 0.75 , xtra_tfms = None , size = None , mode = 'bilinear' , pad_mode = 'reflection' , align_corners = True , batch = False , min_scale = 1.0 , ) aug_tfms = aug_transforms ( max_lighting = 0.8 , do_flip = True , flip_vert = True , max_rotate = 0 ) audio = audio . new ( item_tfms = Resize ( 128 ), batch_tfms = aug_tfms ) dls = audio . dataloaders ( data_path ) dls . train . show_batch ( max_n = 4 , nrows = 1 )","title":"Sidebar: Data Augmentation and Transforms"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#create-a-cnn_learner-and-train-the-model","text":"learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) Before training, let's use learn.lr_find to help find a good learning rate. Two values are returned by running lr_find one tenth of the minimum before the divergence when the slope is the steepest lr_min , lr_steep = learn . lr_find () # plot the values returned by lr_find learn . recorder . plot_lr_find () plt . axvline ( x = lr_min , color = 'red' ) plt . axvline ( x = 3e-3 , color = 'green' ) plt . axvline ( x = lr_steep , color = 'red' ); I'm going to pick a value inbetween the suggested lr's for training. lr_max = 3e-3 learn . fit_one_cycle ( n_epoch = 5 , lr_max = lr_max ) epoch train_loss valid_loss error_rate time 0 2.412440 1.861958 0.642857 00:02 1 1.202116 0.856408 0.261905 00:01 2 0.763907 0.548905 0.190476 00:01 3 0.543375 0.206846 0.095238 00:01 4 0.412221 0.038176 0.023810 00:01 learn . recorder . plot_loss ()","title":"Create a cnn_learner and Train the model"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#interpretation","text":"the model is performing quite well, only one note was incorrectly predicted (G3 predicted for F3 actual) interp . plot_top_losses ( k = 4 , figsize = ( 6 , 6 )) interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix () learn . save ( 'base_cqt_model' ) Path('models/base_cqt_model.pth') Fine tune to try improve accuracy.. learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.000892 0.002096 0.000000 00:01 epoch train_loss valid_loss error_rate time 0 0.000499 0.000041 0.000000 00:02 interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix ()","title":"Interpretation"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#summary","text":"This wasn't the hardest problem in the world. The Constant Q transform really simplifies pitch detection. I think this is an interesting problem space because there are opportunities to progress these examples; I'd like to try classify all 12 notes (by adding sharps/flats), then try multi-label classification using a phrase of notes, and hopefully then addressing the issue of identifying notes across octaves.","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 3: Under the Hood: Training a Digit Classifier 20-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. Primer load in some data visualise it from fastai.vision.all import * from pathlib import Path For this model we are going to create a digit classifier that will be able to classify an image as a 3 or a 7. Fastai has a sample of the MNIST dataset that we will be using. First, let's load the data and check that there are indeed, 3's an 7's in one of the folders. The folder layout is fairly typical, separate training and validation sets. path = untar_data ( URLs . MNIST_SAMPLE ) ( path / 'train' ) . ls () (#2) [Path('/storage/data/mnist_sample/train/7'),Path('/storage/data/mnist_sample/train/3')] Visualising Data There are a number of ways we can check our data to get a better understanding of how it is structured and how it looks Take a look inside one of the folders and check file names Use PIL to open one of the images Use PyTorch/Numpy to check the tensor/array values Get creative # 1 - check file names threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () threes (#6131) [Path('/storage/data/mnist_sample/train/3/10.png'),Path('/storage/data/mnist_sample/train/3/10000.png'),Path('/storage/data/mnist_sample/train/3/10011.png'),Path('/storage/data/mnist_sample/train/3/10031.png'),Path('/storage/data/mnist_sample/train/3/10034.png'),Path('/storage/data/mnist_sample/train/3/10042.png'),Path('/storage/data/mnist_sample/train/3/10052.png'),Path('/storage/data/mnist_sample/train/3/1007.png'),Path('/storage/data/mnist_sample/train/3/10074.png'),Path('/storage/data/mnist_sample/train/3/10091.png')...] # 2 - use PIL to open image im3_path = threes [ 1 ] im3 = Image . open ( im3_path ) im3 # 3 - use PyTorch to view tensor values tensor ( im3 )[ 4 : 10 , 4 : 10 ] tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) # 4 - Getting creative with Pandas im3_t = tensor ( im3 ) df = pd . DataFrame ( im3_t [ 4 : 15 , 4 : 22 ]) df . style . set_properties ( ** { 'font-size' : '6pt' }) . background_gradient ( 'Greys' ) #T_71814984_fd78_11ea_966e_0242ac110002row0_col0,#T_71814984_fd78_11ea_966e_0242ac110002row0_col1,#T_71814984_fd78_11ea_966e_0242ac110002row0_col2,#T_71814984_fd78_11ea_966e_0242ac110002row0_col3,#T_71814984_fd78_11ea_966e_0242ac110002row0_col4,#T_71814984_fd78_11ea_966e_0242ac110002row0_col5,#T_71814984_fd78_11ea_966e_0242ac110002row0_col6,#T_71814984_fd78_11ea_966e_0242ac110002row0_col7,#T_71814984_fd78_11ea_966e_0242ac110002row0_col8,#T_71814984_fd78_11ea_966e_0242ac110002row0_col9,#T_71814984_fd78_11ea_966e_0242ac110002row0_col10,#T_71814984_fd78_11ea_966e_0242ac110002row0_col11,#T_71814984_fd78_11ea_966e_0242ac110002row0_col12,#T_71814984_fd78_11ea_966e_0242ac110002row0_col13,#T_71814984_fd78_11ea_966e_0242ac110002row0_col14,#T_71814984_fd78_11ea_966e_0242ac110002row0_col15,#T_71814984_fd78_11ea_966e_0242ac110002row0_col16,#T_71814984_fd78_11ea_966e_0242ac110002row0_col17,#T_71814984_fd78_11ea_966e_0242ac110002row1_col0,#T_71814984_fd78_11ea_966e_0242ac110002row1_col1,#T_71814984_fd78_11ea_966e_0242ac110002row1_col2,#T_71814984_fd78_11ea_966e_0242ac110002row1_col3,#T_71814984_fd78_11ea_966e_0242ac110002row1_col4,#T_71814984_fd78_11ea_966e_0242ac110002row1_col15,#T_71814984_fd78_11ea_966e_0242ac110002row1_col16,#T_71814984_fd78_11ea_966e_0242ac110002row1_col17,#T_71814984_fd78_11ea_966e_0242ac110002row2_col0,#T_71814984_fd78_11ea_966e_0242ac110002row2_col1,#T_71814984_fd78_11ea_966e_0242ac110002row2_col2,#T_71814984_fd78_11ea_966e_0242ac110002row2_col15,#T_71814984_fd78_11ea_966e_0242ac110002row2_col16,#T_71814984_fd78_11ea_966e_0242ac110002row2_col17,#T_71814984_fd78_11ea_966e_0242ac110002row3_col0,#T_71814984_fd78_11ea_966e_0242ac110002row3_col15,#T_71814984_fd78_11ea_966e_0242ac110002row3_col16,#T_71814984_fd78_11ea_966e_0242ac110002row3_col17,#T_71814984_fd78_11ea_966e_0242ac110002row4_col0,#T_71814984_fd78_11ea_966e_0242ac110002row4_col6,#T_71814984_fd78_11ea_966e_0242ac110002row4_col7,#T_71814984_fd78_11ea_966e_0242ac110002row4_col8,#T_71814984_fd78_11ea_966e_0242ac110002row4_col9,#T_71814984_fd78_11ea_966e_0242ac110002row4_col10,#T_71814984_fd78_11ea_966e_0242ac110002row4_col15,#T_71814984_fd78_11ea_966e_0242ac110002row4_col16,#T_71814984_fd78_11ea_966e_0242ac110002row4_col17,#T_71814984_fd78_11ea_966e_0242ac110002row5_col0,#T_71814984_fd78_11ea_966e_0242ac110002row5_col5,#T_71814984_fd78_11ea_966e_0242ac110002row5_col6,#T_71814984_fd78_11ea_966e_0242ac110002row5_col7,#T_71814984_fd78_11ea_966e_0242ac110002row5_col8,#T_71814984_fd78_11ea_966e_0242ac110002row5_col9,#T_71814984_fd78_11ea_966e_0242ac110002row5_col15,#T_71814984_fd78_11ea_966e_0242ac110002row5_col16,#T_71814984_fd78_11ea_966e_0242ac110002row5_col17,#T_71814984_fd78_11ea_966e_0242ac110002row6_col0,#T_71814984_fd78_11ea_966e_0242ac110002row6_col1,#T_71814984_fd78_11ea_966e_0242ac110002row6_col2,#T_71814984_fd78_11ea_966e_0242ac110002row6_col3,#T_71814984_fd78_11ea_966e_0242ac110002row6_col4,#T_71814984_fd78_11ea_966e_0242ac110002row6_col5,#T_71814984_fd78_11ea_966e_0242ac110002row6_col6,#T_71814984_fd78_11ea_966e_0242ac110002row6_col7,#T_71814984_fd78_11ea_966e_0242ac110002row6_col8,#T_71814984_fd78_11ea_966e_0242ac110002row6_col9,#T_71814984_fd78_11ea_966e_0242ac110002row6_col14,#T_71814984_fd78_11ea_966e_0242ac110002row6_col15,#T_71814984_fd78_11ea_966e_0242ac110002row6_col16,#T_71814984_fd78_11ea_966e_0242ac110002row6_col17,#T_71814984_fd78_11ea_966e_0242ac110002row7_col0,#T_71814984_fd78_11ea_966e_0242ac110002row7_col1,#T_71814984_fd78_11ea_966e_0242ac110002row7_col2,#T_71814984_fd78_11ea_966e_0242ac110002row7_col3,#T_71814984_fd78_11ea_966e_0242ac110002row7_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col5,#T_71814984_fd78_11ea_966e_0242ac110002row7_col6,#T_71814984_fd78_11ea_966e_0242ac110002row7_col13,#T_71814984_fd78_11ea_966e_0242ac110002row7_col14,#T_71814984_fd78_11ea_966e_0242ac110002row7_col15,#T_71814984_fd78_11ea_966e_0242ac110002row7_col16,#T_71814984_fd78_11ea_966e_0242ac110002row7_col17,#T_71814984_fd78_11ea_966e_0242ac110002row8_col0,#T_71814984_fd78_11ea_966e_0242ac110002row8_col1,#T_71814984_fd78_11ea_966e_0242ac110002row8_col2,#T_71814984_fd78_11ea_966e_0242ac110002row8_col3,#T_71814984_fd78_11ea_966e_0242ac110002row8_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col13,#T_71814984_fd78_11ea_966e_0242ac110002row8_col14,#T_71814984_fd78_11ea_966e_0242ac110002row8_col15,#T_71814984_fd78_11ea_966e_0242ac110002row8_col16,#T_71814984_fd78_11ea_966e_0242ac110002row8_col17,#T_71814984_fd78_11ea_966e_0242ac110002row9_col0,#T_71814984_fd78_11ea_966e_0242ac110002row9_col1,#T_71814984_fd78_11ea_966e_0242ac110002row9_col2,#T_71814984_fd78_11ea_966e_0242ac110002row9_col3,#T_71814984_fd78_11ea_966e_0242ac110002row9_col4,#T_71814984_fd78_11ea_966e_0242ac110002row9_col16,#T_71814984_fd78_11ea_966e_0242ac110002row9_col17,#T_71814984_fd78_11ea_966e_0242ac110002row10_col0,#T_71814984_fd78_11ea_966e_0242ac110002row10_col1,#T_71814984_fd78_11ea_966e_0242ac110002row10_col2,#T_71814984_fd78_11ea_966e_0242ac110002row10_col3,#T_71814984_fd78_11ea_966e_0242ac110002row10_col4,#T_71814984_fd78_11ea_966e_0242ac110002row10_col5,#T_71814984_fd78_11ea_966e_0242ac110002row10_col6,#T_71814984_fd78_11ea_966e_0242ac110002row10_col17{ font-size: 6pt; background-color: #ffffff; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col5{ font-size: 6pt; background-color: #efefef; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col6,#T_71814984_fd78_11ea_966e_0242ac110002row1_col13{ font-size: 6pt; background-color: #7c7c7c; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col7{ font-size: 6pt; background-color: #4a4a4a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col8,#T_71814984_fd78_11ea_966e_0242ac110002row1_col9,#T_71814984_fd78_11ea_966e_0242ac110002row1_col10,#T_71814984_fd78_11ea_966e_0242ac110002row2_col5,#T_71814984_fd78_11ea_966e_0242ac110002row2_col6,#T_71814984_fd78_11ea_966e_0242ac110002row2_col7,#T_71814984_fd78_11ea_966e_0242ac110002row2_col11,#T_71814984_fd78_11ea_966e_0242ac110002row2_col12,#T_71814984_fd78_11ea_966e_0242ac110002row2_col13,#T_71814984_fd78_11ea_966e_0242ac110002row3_col4,#T_71814984_fd78_11ea_966e_0242ac110002row3_col12,#T_71814984_fd78_11ea_966e_0242ac110002row3_col13,#T_71814984_fd78_11ea_966e_0242ac110002row4_col1,#T_71814984_fd78_11ea_966e_0242ac110002row4_col2,#T_71814984_fd78_11ea_966e_0242ac110002row4_col3,#T_71814984_fd78_11ea_966e_0242ac110002row4_col12,#T_71814984_fd78_11ea_966e_0242ac110002row4_col13,#T_71814984_fd78_11ea_966e_0242ac110002row5_col12,#T_71814984_fd78_11ea_966e_0242ac110002row6_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col12,#T_71814984_fd78_11ea_966e_0242ac110002row10_col13,#T_71814984_fd78_11ea_966e_0242ac110002row10_col14,#T_71814984_fd78_11ea_966e_0242ac110002row10_col15,#T_71814984_fd78_11ea_966e_0242ac110002row10_col16{ font-size: 6pt; background-color: #000000; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col11{ font-size: 6pt; background-color: #606060; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col12{ font-size: 6pt; background-color: #4d4d4d; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col14{ font-size: 6pt; background-color: #bbbbbb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col3{ font-size: 6pt; background-color: #e4e4e4; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col6{ font-size: 6pt; background-color: #6b6b6b; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col8,#T_71814984_fd78_11ea_966e_0242ac110002row2_col14,#T_71814984_fd78_11ea_966e_0242ac110002row3_col14{ font-size: 6pt; background-color: #171717; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col9,#T_71814984_fd78_11ea_966e_0242ac110002row3_col11{ font-size: 6pt; background-color: #4b4b4b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col10,#T_71814984_fd78_11ea_966e_0242ac110002row8_col8,#T_71814984_fd78_11ea_966e_0242ac110002row8_col10,#T_71814984_fd78_11ea_966e_0242ac110002row9_col8,#T_71814984_fd78_11ea_966e_0242ac110002row9_col10{ font-size: 6pt; background-color: #010101; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col1{ font-size: 6pt; background-color: #272727; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col2{ font-size: 6pt; background-color: #0a0a0a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col3{ font-size: 6pt; background-color: #050505; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col5{ font-size: 6pt; background-color: #333333; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col6{ font-size: 6pt; background-color: #e6e6e6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col7,#T_71814984_fd78_11ea_966e_0242ac110002row3_col10{ font-size: 6pt; background-color: #fafafa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col8{ font-size: 6pt; background-color: #fbfbfb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col9{ font-size: 6pt; background-color: #fdfdfd; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col4{ font-size: 6pt; background-color: #1b1b1b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col5{ font-size: 6pt; background-color: #e0e0e0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col11{ font-size: 6pt; background-color: #4e4e4e; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col14{ font-size: 6pt; background-color: #767676; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col1{ font-size: 6pt; background-color: #fcfcfc; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col2,#T_71814984_fd78_11ea_966e_0242ac110002row5_col3{ font-size: 6pt; background-color: #f6f6f6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col7{ font-size: 6pt; background-color: #f8f8f8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col10,#T_71814984_fd78_11ea_966e_0242ac110002row10_col7{ font-size: 6pt; background-color: #e8e8e8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col11{ font-size: 6pt; background-color: #222222; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col13,#T_71814984_fd78_11ea_966e_0242ac110002row6_col12{ font-size: 6pt; background-color: #090909; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col14{ font-size: 6pt; background-color: #d0d0d0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col6{ font-size: 6pt; background-color: #060606; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col13{ font-size: 6pt; background-color: #979797; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col8{ font-size: 6pt; background-color: #b6b6b6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col9{ font-size: 6pt; background-color: #252525; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col12{ font-size: 6pt; background-color: #999999; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col5{ font-size: 6pt; background-color: #f9f9f9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col7{ font-size: 6pt; background-color: #101010; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col9,#T_71814984_fd78_11ea_966e_0242ac110002row9_col9{ font-size: 6pt; background-color: #020202; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col11{ font-size: 6pt; background-color: #545454; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col12{ font-size: 6pt; background-color: #f1f1f1; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col5{ font-size: 6pt; background-color: #f7f7f7; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col7{ font-size: 6pt; background-color: #030303; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col12{ font-size: 6pt; background-color: #181818; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col13{ font-size: 6pt; background-color: #303030; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col14{ font-size: 6pt; background-color: #a9a9a9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col15{ font-size: 6pt; background-color: #fefefe; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col8,#T_71814984_fd78_11ea_966e_0242ac110002row10_col9{ font-size: 6pt; background-color: #bababa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col10{ font-size: 6pt; background-color: #393939; color: #f1f1f1; } 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 29 150 195 254 255 254 176 193 150 96 0 0 0 2 0 0 0 48 166 224 253 253 234 196 253 253 253 253 233 0 0 0 3 0 93 244 249 253 187 46 10 8 4 10 194 253 253 233 0 0 0 4 0 107 253 253 230 48 0 0 0 0 0 192 253 253 156 0 0 0 5 0 3 20 20 15 0 0 0 0 0 43 224 253 245 74 0 0 0 6 0 0 0 0 0 0 0 0 0 0 249 253 245 126 0 0 0 0 7 0 0 0 0 0 0 0 14 101 223 253 248 124 0 0 0 0 0 8 0 0 0 0 0 11 166 239 253 253 253 187 30 0 0 0 0 0 9 0 0 0 0 0 16 248 250 253 253 253 253 232 213 111 2 0 0 10 0 0 0 0 0 0 0 43 98 98 208 253 253 253 253 187 22 0 Sidebar - think about the problem Before jumping into solution mode (ie apply deep learning to everything!) think about the problem space and how you might be able to solve it. For this problem (digit recognition) using a simple average might be enough to get a descent result. That is exactly what we will do first. Method 1: Pixel Similarity find the average pixel value for every pixel of the 3s and 7s this will give us 2 group averages that represent the \"ideal\" 3 and 7 to classify a digit, check the similarity against the ideal this method will form our baseline that we will improve upon later Step 1: Organise data create a tensor by stacking all of our 3s together we will use list comprehension for this # open all images, convert to tensor, store in list seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] len ( three_tensors ), len ( seven_tensors ) (6131, 6265) # lets check one of show_image ( three_tensors [ 1 ]); Step 2: Compute the average pixel value For every pixel position, compute the average over all the images of the intensity of that pixel. To do this combine all images into a single 3-dimensional tensor using stack which \" Concatenates sequence of tensors along a new dimension .\" PuyTorch needs us to cast the int values to floats in order to compute the average. \"Generally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here\" source stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 stacked_threes . shape torch.Size([6131, 28, 28]) tensor jargon - rank : the number of axis - shape : the size of each axis print ( 'rank:' , stacked_threes . ndim , ' \\n ' , 'shape:' , stacked_threes . shape ) rank: 3 shape: torch.Size([6131, 28, 28]) We have 6,131 images of size 28x28 Step 3: Compute the ideal digits compute the mean along the 0th dimension by visualising this ideal 3 we can indeed see that it represents a 3! mean3 = stacked_threes . mean ( dim = 0 ) show_image ( mean3 ); mean7 = stacked_sevens . mean ( dim = 0 ) show_image ( mean7 ); # here is a random three for comparison a_3 = stacked_threes [ 1 ] show_image ( a_3 ); How could we calculate how similar a 3 is from this ideal 3? Typically there are two methods take the absolute value of differences (where there are negatives, replace with postive) This is called the mean absolute difference or L1 norm take the mean squared difference (which also makes all results positive) then take the square root This is called the root mean squared error (RMSE) or L2 norm . # 3 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean3 ) . abs () . mean () dist_3_sqr = (( a_3 - mean3 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1114), tensor(0.2021)) # 7 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean7 ) . abs () . mean () dist_3_sqr = (( a_3 - mean7 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1586), tensor(0.3021)) The distance between our \"ideal\" 3 and the real 3 is less than the distance from the real 3 to \"ideal\" 7. This is good - it means both methods will work and our simple model will give the correct prediction. PyTorch already provides these loss functions for us (though RMSE is only MSE, but we can work with that). # check results with PyTorch F . l1_loss ( a_3 . float (), mean7 ), F . mse_loss ( a_3 , mean7 ) . sqrt () (tensor(0.1586), tensor(0.3021)) A simple model A validation set is usually used to help avoid overfitting, our model has no trained components so this isn't going to be an issue, but let's stick with best practices. We will also define a function that will decide if an arbitrary image is a 3 or a 7. This will be achieved by measuring the distance between this digit and our ideal digits and determining which ideal it is closer to. valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 valid_3_tens . shape , valid_7_tens . shape (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) mnist_distance calculates the difference between our ideal 3 and every 3 in the validation set. We call mean((-1,-2)) in our fuction. The tuple (-1,-2) represents a range of axes, so the last -1, and second last -2. mean((-1,-2)) says; take the mean over the last two axes in the tensor. Why? These axes represent the verticle and horizontal dimensions of an image. After taking the mean over the these axes, we have one axis left which indexes over the 1,010 images we have. # calculate the mean absolute error def mnist_distance ( a , b ): return ( a - b ) . abs () . mean (( - 1 , - 2 )) # check the function mnist_distance ( a_3 , mean3 ) # great! tensor(0.1114) This works for a single image, but in order to calculate the overall accuracy, we want to calculate this distance to the ideal 3 for all images in the validation set. This can be achieved using a loop, but there is another way - broadcasting. Take a look at the shape of valid_3_tens and mean3 , they are different... valid_3_tens . shape , mean3 . shape (torch.Size([1010, 28, 28]), torch.Size([28, 28])) Now calculate the distance between the two... valid_3_dist = mnist_distance ( valid_3_tens , mean3 ) valid_3_dist , valid_3_dist . shape (tensor([0.1290, 0.1223, 0.1380, ..., 0.1337, 0.1132, 0.1097]), torch.Size([1010])) our function has returned the distance for every single image as a rank-1 tensor of length 1,010. This is because we have added a subtraction (a-b) into our distance function and when PyTorch performs this subtraction, it uses broadcasting which will automatically expand the tensor with smaller rank to have the same size as the one with larger rank. Once this has happened, PyTorch will perform an element wise operation over the two tensors. In our case, PyTorch is treating mean3 (a rank 2 tensor) as if it were 1,010 copies of that tensor. You can see that by performing a subtraction and checking the shape ( valid_3_tens - mean3 ) . shape torch.Size([1010, 28, 28]) There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance: PyTorch doesn't actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn't actually allocate any additional memory It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!). source A funtion to make a decision is_3 is going to use mnist_distance to figure out whether an image is a 3 or a 7. To do this it will check whether the distance between a digit, x and mean3 is less than the difference between x and mean7 . def is_3 ( x ): return mnist_distance ( x , mean3 ) < mnist_distance ( x , mean7 ) # let's test it # you can convert a boolean to a float is_3 ( a_3 ), is_3 ( a_3 ) . float () (tensor(True), tensor(1.)) Calculate the accuracy for each 3 and 7 by taking the average of is_3 for all 3s and it's inverse for all 7s. accuracy_3s = is_3 ( valid_3_tens ) . float () . mean () accuracy_7s = ( 1 - is_3 ( valid_7_tens ) . float ()) . mean () accuracy_3s , accuracy_7s , ( accuracy_3s + accuracy_7s ) / 2 (tensor(0.9168), tensor(0.9854), tensor(0.9511)) Not bad, over 90% accuracy using a very simple model. This was also a very simple problem, 3s and 7s look very different so it's not really a surprise that this was so effective. We will nowe look at a system that will do some learning (automatically modify itself to improve its performance) In the next part, we will implement this more advanced model","title":"Lesson 03 pt 1"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#lesson-3-under-the-hood-training-a-digit-classifier","text":"20-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set.","title":"Lesson 3: Under the Hood: Training a Digit Classifier"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#primer","text":"load in some data visualise it from fastai.vision.all import * from pathlib import Path For this model we are going to create a digit classifier that will be able to classify an image as a 3 or a 7. Fastai has a sample of the MNIST dataset that we will be using. First, let's load the data and check that there are indeed, 3's an 7's in one of the folders. The folder layout is fairly typical, separate training and validation sets. path = untar_data ( URLs . MNIST_SAMPLE ) ( path / 'train' ) . ls () (#2) [Path('/storage/data/mnist_sample/train/7'),Path('/storage/data/mnist_sample/train/3')]","title":"Primer"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#visualising-data","text":"There are a number of ways we can check our data to get a better understanding of how it is structured and how it looks Take a look inside one of the folders and check file names Use PIL to open one of the images Use PyTorch/Numpy to check the tensor/array values Get creative # 1 - check file names threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () threes (#6131) [Path('/storage/data/mnist_sample/train/3/10.png'),Path('/storage/data/mnist_sample/train/3/10000.png'),Path('/storage/data/mnist_sample/train/3/10011.png'),Path('/storage/data/mnist_sample/train/3/10031.png'),Path('/storage/data/mnist_sample/train/3/10034.png'),Path('/storage/data/mnist_sample/train/3/10042.png'),Path('/storage/data/mnist_sample/train/3/10052.png'),Path('/storage/data/mnist_sample/train/3/1007.png'),Path('/storage/data/mnist_sample/train/3/10074.png'),Path('/storage/data/mnist_sample/train/3/10091.png')...] # 2 - use PIL to open image im3_path = threes [ 1 ] im3 = Image . open ( im3_path ) im3 # 3 - use PyTorch to view tensor values tensor ( im3 )[ 4 : 10 , 4 : 10 ] tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) # 4 - Getting creative with Pandas im3_t = tensor ( im3 ) df = pd . DataFrame ( im3_t [ 4 : 15 , 4 : 22 ]) df . style . set_properties ( ** { 'font-size' : '6pt' }) . background_gradient ( 'Greys' ) #T_71814984_fd78_11ea_966e_0242ac110002row0_col0,#T_71814984_fd78_11ea_966e_0242ac110002row0_col1,#T_71814984_fd78_11ea_966e_0242ac110002row0_col2,#T_71814984_fd78_11ea_966e_0242ac110002row0_col3,#T_71814984_fd78_11ea_966e_0242ac110002row0_col4,#T_71814984_fd78_11ea_966e_0242ac110002row0_col5,#T_71814984_fd78_11ea_966e_0242ac110002row0_col6,#T_71814984_fd78_11ea_966e_0242ac110002row0_col7,#T_71814984_fd78_11ea_966e_0242ac110002row0_col8,#T_71814984_fd78_11ea_966e_0242ac110002row0_col9,#T_71814984_fd78_11ea_966e_0242ac110002row0_col10,#T_71814984_fd78_11ea_966e_0242ac110002row0_col11,#T_71814984_fd78_11ea_966e_0242ac110002row0_col12,#T_71814984_fd78_11ea_966e_0242ac110002row0_col13,#T_71814984_fd78_11ea_966e_0242ac110002row0_col14,#T_71814984_fd78_11ea_966e_0242ac110002row0_col15,#T_71814984_fd78_11ea_966e_0242ac110002row0_col16,#T_71814984_fd78_11ea_966e_0242ac110002row0_col17,#T_71814984_fd78_11ea_966e_0242ac110002row1_col0,#T_71814984_fd78_11ea_966e_0242ac110002row1_col1,#T_71814984_fd78_11ea_966e_0242ac110002row1_col2,#T_71814984_fd78_11ea_966e_0242ac110002row1_col3,#T_71814984_fd78_11ea_966e_0242ac110002row1_col4,#T_71814984_fd78_11ea_966e_0242ac110002row1_col15,#T_71814984_fd78_11ea_966e_0242ac110002row1_col16,#T_71814984_fd78_11ea_966e_0242ac110002row1_col17,#T_71814984_fd78_11ea_966e_0242ac110002row2_col0,#T_71814984_fd78_11ea_966e_0242ac110002row2_col1,#T_71814984_fd78_11ea_966e_0242ac110002row2_col2,#T_71814984_fd78_11ea_966e_0242ac110002row2_col15,#T_71814984_fd78_11ea_966e_0242ac110002row2_col16,#T_71814984_fd78_11ea_966e_0242ac110002row2_col17,#T_71814984_fd78_11ea_966e_0242ac110002row3_col0,#T_71814984_fd78_11ea_966e_0242ac110002row3_col15,#T_71814984_fd78_11ea_966e_0242ac110002row3_col16,#T_71814984_fd78_11ea_966e_0242ac110002row3_col17,#T_71814984_fd78_11ea_966e_0242ac110002row4_col0,#T_71814984_fd78_11ea_966e_0242ac110002row4_col6,#T_71814984_fd78_11ea_966e_0242ac110002row4_col7,#T_71814984_fd78_11ea_966e_0242ac110002row4_col8,#T_71814984_fd78_11ea_966e_0242ac110002row4_col9,#T_71814984_fd78_11ea_966e_0242ac110002row4_col10,#T_71814984_fd78_11ea_966e_0242ac110002row4_col15,#T_71814984_fd78_11ea_966e_0242ac110002row4_col16,#T_71814984_fd78_11ea_966e_0242ac110002row4_col17,#T_71814984_fd78_11ea_966e_0242ac110002row5_col0,#T_71814984_fd78_11ea_966e_0242ac110002row5_col5,#T_71814984_fd78_11ea_966e_0242ac110002row5_col6,#T_71814984_fd78_11ea_966e_0242ac110002row5_col7,#T_71814984_fd78_11ea_966e_0242ac110002row5_col8,#T_71814984_fd78_11ea_966e_0242ac110002row5_col9,#T_71814984_fd78_11ea_966e_0242ac110002row5_col15,#T_71814984_fd78_11ea_966e_0242ac110002row5_col16,#T_71814984_fd78_11ea_966e_0242ac110002row5_col17,#T_71814984_fd78_11ea_966e_0242ac110002row6_col0,#T_71814984_fd78_11ea_966e_0242ac110002row6_col1,#T_71814984_fd78_11ea_966e_0242ac110002row6_col2,#T_71814984_fd78_11ea_966e_0242ac110002row6_col3,#T_71814984_fd78_11ea_966e_0242ac110002row6_col4,#T_71814984_fd78_11ea_966e_0242ac110002row6_col5,#T_71814984_fd78_11ea_966e_0242ac110002row6_col6,#T_71814984_fd78_11ea_966e_0242ac110002row6_col7,#T_71814984_fd78_11ea_966e_0242ac110002row6_col8,#T_71814984_fd78_11ea_966e_0242ac110002row6_col9,#T_71814984_fd78_11ea_966e_0242ac110002row6_col14,#T_71814984_fd78_11ea_966e_0242ac110002row6_col15,#T_71814984_fd78_11ea_966e_0242ac110002row6_col16,#T_71814984_fd78_11ea_966e_0242ac110002row6_col17,#T_71814984_fd78_11ea_966e_0242ac110002row7_col0,#T_71814984_fd78_11ea_966e_0242ac110002row7_col1,#T_71814984_fd78_11ea_966e_0242ac110002row7_col2,#T_71814984_fd78_11ea_966e_0242ac110002row7_col3,#T_71814984_fd78_11ea_966e_0242ac110002row7_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col5,#T_71814984_fd78_11ea_966e_0242ac110002row7_col6,#T_71814984_fd78_11ea_966e_0242ac110002row7_col13,#T_71814984_fd78_11ea_966e_0242ac110002row7_col14,#T_71814984_fd78_11ea_966e_0242ac110002row7_col15,#T_71814984_fd78_11ea_966e_0242ac110002row7_col16,#T_71814984_fd78_11ea_966e_0242ac110002row7_col17,#T_71814984_fd78_11ea_966e_0242ac110002row8_col0,#T_71814984_fd78_11ea_966e_0242ac110002row8_col1,#T_71814984_fd78_11ea_966e_0242ac110002row8_col2,#T_71814984_fd78_11ea_966e_0242ac110002row8_col3,#T_71814984_fd78_11ea_966e_0242ac110002row8_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col13,#T_71814984_fd78_11ea_966e_0242ac110002row8_col14,#T_71814984_fd78_11ea_966e_0242ac110002row8_col15,#T_71814984_fd78_11ea_966e_0242ac110002row8_col16,#T_71814984_fd78_11ea_966e_0242ac110002row8_col17,#T_71814984_fd78_11ea_966e_0242ac110002row9_col0,#T_71814984_fd78_11ea_966e_0242ac110002row9_col1,#T_71814984_fd78_11ea_966e_0242ac110002row9_col2,#T_71814984_fd78_11ea_966e_0242ac110002row9_col3,#T_71814984_fd78_11ea_966e_0242ac110002row9_col4,#T_71814984_fd78_11ea_966e_0242ac110002row9_col16,#T_71814984_fd78_11ea_966e_0242ac110002row9_col17,#T_71814984_fd78_11ea_966e_0242ac110002row10_col0,#T_71814984_fd78_11ea_966e_0242ac110002row10_col1,#T_71814984_fd78_11ea_966e_0242ac110002row10_col2,#T_71814984_fd78_11ea_966e_0242ac110002row10_col3,#T_71814984_fd78_11ea_966e_0242ac110002row10_col4,#T_71814984_fd78_11ea_966e_0242ac110002row10_col5,#T_71814984_fd78_11ea_966e_0242ac110002row10_col6,#T_71814984_fd78_11ea_966e_0242ac110002row10_col17{ font-size: 6pt; background-color: #ffffff; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col5{ font-size: 6pt; background-color: #efefef; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col6,#T_71814984_fd78_11ea_966e_0242ac110002row1_col13{ font-size: 6pt; background-color: #7c7c7c; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col7{ font-size: 6pt; background-color: #4a4a4a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col8,#T_71814984_fd78_11ea_966e_0242ac110002row1_col9,#T_71814984_fd78_11ea_966e_0242ac110002row1_col10,#T_71814984_fd78_11ea_966e_0242ac110002row2_col5,#T_71814984_fd78_11ea_966e_0242ac110002row2_col6,#T_71814984_fd78_11ea_966e_0242ac110002row2_col7,#T_71814984_fd78_11ea_966e_0242ac110002row2_col11,#T_71814984_fd78_11ea_966e_0242ac110002row2_col12,#T_71814984_fd78_11ea_966e_0242ac110002row2_col13,#T_71814984_fd78_11ea_966e_0242ac110002row3_col4,#T_71814984_fd78_11ea_966e_0242ac110002row3_col12,#T_71814984_fd78_11ea_966e_0242ac110002row3_col13,#T_71814984_fd78_11ea_966e_0242ac110002row4_col1,#T_71814984_fd78_11ea_966e_0242ac110002row4_col2,#T_71814984_fd78_11ea_966e_0242ac110002row4_col3,#T_71814984_fd78_11ea_966e_0242ac110002row4_col12,#T_71814984_fd78_11ea_966e_0242ac110002row4_col13,#T_71814984_fd78_11ea_966e_0242ac110002row5_col12,#T_71814984_fd78_11ea_966e_0242ac110002row6_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col12,#T_71814984_fd78_11ea_966e_0242ac110002row10_col13,#T_71814984_fd78_11ea_966e_0242ac110002row10_col14,#T_71814984_fd78_11ea_966e_0242ac110002row10_col15,#T_71814984_fd78_11ea_966e_0242ac110002row10_col16{ font-size: 6pt; background-color: #000000; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col11{ font-size: 6pt; background-color: #606060; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col12{ font-size: 6pt; background-color: #4d4d4d; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col14{ font-size: 6pt; background-color: #bbbbbb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col3{ font-size: 6pt; background-color: #e4e4e4; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col6{ font-size: 6pt; background-color: #6b6b6b; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col8,#T_71814984_fd78_11ea_966e_0242ac110002row2_col14,#T_71814984_fd78_11ea_966e_0242ac110002row3_col14{ font-size: 6pt; background-color: #171717; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col9,#T_71814984_fd78_11ea_966e_0242ac110002row3_col11{ font-size: 6pt; background-color: #4b4b4b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col10,#T_71814984_fd78_11ea_966e_0242ac110002row8_col8,#T_71814984_fd78_11ea_966e_0242ac110002row8_col10,#T_71814984_fd78_11ea_966e_0242ac110002row9_col8,#T_71814984_fd78_11ea_966e_0242ac110002row9_col10{ font-size: 6pt; background-color: #010101; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col1{ font-size: 6pt; background-color: #272727; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col2{ font-size: 6pt; background-color: #0a0a0a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col3{ font-size: 6pt; background-color: #050505; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col5{ font-size: 6pt; background-color: #333333; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col6{ font-size: 6pt; background-color: #e6e6e6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col7,#T_71814984_fd78_11ea_966e_0242ac110002row3_col10{ font-size: 6pt; background-color: #fafafa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col8{ font-size: 6pt; background-color: #fbfbfb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col9{ font-size: 6pt; background-color: #fdfdfd; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col4{ font-size: 6pt; background-color: #1b1b1b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col5{ font-size: 6pt; background-color: #e0e0e0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col11{ font-size: 6pt; background-color: #4e4e4e; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col14{ font-size: 6pt; background-color: #767676; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col1{ font-size: 6pt; background-color: #fcfcfc; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col2,#T_71814984_fd78_11ea_966e_0242ac110002row5_col3{ font-size: 6pt; background-color: #f6f6f6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col7{ font-size: 6pt; background-color: #f8f8f8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col10,#T_71814984_fd78_11ea_966e_0242ac110002row10_col7{ font-size: 6pt; background-color: #e8e8e8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col11{ font-size: 6pt; background-color: #222222; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col13,#T_71814984_fd78_11ea_966e_0242ac110002row6_col12{ font-size: 6pt; background-color: #090909; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col14{ font-size: 6pt; background-color: #d0d0d0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col6{ font-size: 6pt; background-color: #060606; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col13{ font-size: 6pt; background-color: #979797; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col8{ font-size: 6pt; background-color: #b6b6b6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col9{ font-size: 6pt; background-color: #252525; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col12{ font-size: 6pt; background-color: #999999; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col5{ font-size: 6pt; background-color: #f9f9f9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col7{ font-size: 6pt; background-color: #101010; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col9,#T_71814984_fd78_11ea_966e_0242ac110002row9_col9{ font-size: 6pt; background-color: #020202; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col11{ font-size: 6pt; background-color: #545454; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col12{ font-size: 6pt; background-color: #f1f1f1; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col5{ font-size: 6pt; background-color: #f7f7f7; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col7{ font-size: 6pt; background-color: #030303; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col12{ font-size: 6pt; background-color: #181818; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col13{ font-size: 6pt; background-color: #303030; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col14{ font-size: 6pt; background-color: #a9a9a9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col15{ font-size: 6pt; background-color: #fefefe; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col8,#T_71814984_fd78_11ea_966e_0242ac110002row10_col9{ font-size: 6pt; background-color: #bababa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col10{ font-size: 6pt; background-color: #393939; color: #f1f1f1; } 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 29 150 195 254 255 254 176 193 150 96 0 0 0 2 0 0 0 48 166 224 253 253 234 196 253 253 253 253 233 0 0 0 3 0 93 244 249 253 187 46 10 8 4 10 194 253 253 233 0 0 0 4 0 107 253 253 230 48 0 0 0 0 0 192 253 253 156 0 0 0 5 0 3 20 20 15 0 0 0 0 0 43 224 253 245 74 0 0 0 6 0 0 0 0 0 0 0 0 0 0 249 253 245 126 0 0 0 0 7 0 0 0 0 0 0 0 14 101 223 253 248 124 0 0 0 0 0 8 0 0 0 0 0 11 166 239 253 253 253 187 30 0 0 0 0 0 9 0 0 0 0 0 16 248 250 253 253 253 253 232 213 111 2 0 0 10 0 0 0 0 0 0 0 43 98 98 208 253 253 253 253 187 22 0","title":"Visualising Data"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#sidebar-think-about-the-problem","text":"Before jumping into solution mode (ie apply deep learning to everything!) think about the problem space and how you might be able to solve it. For this problem (digit recognition) using a simple average might be enough to get a descent result. That is exactly what we will do first.","title":"Sidebar - think about the problem"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#method-1-pixel-similarity","text":"find the average pixel value for every pixel of the 3s and 7s this will give us 2 group averages that represent the \"ideal\" 3 and 7 to classify a digit, check the similarity against the ideal this method will form our baseline that we will improve upon later","title":"Method 1: Pixel Similarity"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#step-1-organise-data","text":"create a tensor by stacking all of our 3s together we will use list comprehension for this # open all images, convert to tensor, store in list seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] len ( three_tensors ), len ( seven_tensors ) (6131, 6265) # lets check one of show_image ( three_tensors [ 1 ]);","title":"Step 1: Organise data"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#step-2-compute-the-average-pixel-value","text":"For every pixel position, compute the average over all the images of the intensity of that pixel. To do this combine all images into a single 3-dimensional tensor using stack which \" Concatenates sequence of tensors along a new dimension .\" PuyTorch needs us to cast the int values to floats in order to compute the average. \"Generally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here\" source stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 stacked_threes . shape torch.Size([6131, 28, 28]) tensor jargon - rank : the number of axis - shape : the size of each axis print ( 'rank:' , stacked_threes . ndim , ' \\n ' , 'shape:' , stacked_threes . shape ) rank: 3 shape: torch.Size([6131, 28, 28]) We have 6,131 images of size 28x28","title":"Step 2: Compute the average pixel value"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#step-3-compute-the-ideal-digits","text":"compute the mean along the 0th dimension by visualising this ideal 3 we can indeed see that it represents a 3! mean3 = stacked_threes . mean ( dim = 0 ) show_image ( mean3 ); mean7 = stacked_sevens . mean ( dim = 0 ) show_image ( mean7 ); # here is a random three for comparison a_3 = stacked_threes [ 1 ] show_image ( a_3 ); How could we calculate how similar a 3 is from this ideal 3? Typically there are two methods take the absolute value of differences (where there are negatives, replace with postive) This is called the mean absolute difference or L1 norm take the mean squared difference (which also makes all results positive) then take the square root This is called the root mean squared error (RMSE) or L2 norm . # 3 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean3 ) . abs () . mean () dist_3_sqr = (( a_3 - mean3 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1114), tensor(0.2021)) # 7 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean7 ) . abs () . mean () dist_3_sqr = (( a_3 - mean7 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1586), tensor(0.3021)) The distance between our \"ideal\" 3 and the real 3 is less than the distance from the real 3 to \"ideal\" 7. This is good - it means both methods will work and our simple model will give the correct prediction. PyTorch already provides these loss functions for us (though RMSE is only MSE, but we can work with that). # check results with PyTorch F . l1_loss ( a_3 . float (), mean7 ), F . mse_loss ( a_3 , mean7 ) . sqrt () (tensor(0.1586), tensor(0.3021))","title":"Step 3: Compute the ideal digits"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#a-simple-model","text":"A validation set is usually used to help avoid overfitting, our model has no trained components so this isn't going to be an issue, but let's stick with best practices. We will also define a function that will decide if an arbitrary image is a 3 or a 7. This will be achieved by measuring the distance between this digit and our ideal digits and determining which ideal it is closer to. valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 valid_3_tens . shape , valid_7_tens . shape (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) mnist_distance calculates the difference between our ideal 3 and every 3 in the validation set. We call mean((-1,-2)) in our fuction. The tuple (-1,-2) represents a range of axes, so the last -1, and second last -2. mean((-1,-2)) says; take the mean over the last two axes in the tensor. Why? These axes represent the verticle and horizontal dimensions of an image. After taking the mean over the these axes, we have one axis left which indexes over the 1,010 images we have. # calculate the mean absolute error def mnist_distance ( a , b ): return ( a - b ) . abs () . mean (( - 1 , - 2 )) # check the function mnist_distance ( a_3 , mean3 ) # great! tensor(0.1114) This works for a single image, but in order to calculate the overall accuracy, we want to calculate this distance to the ideal 3 for all images in the validation set. This can be achieved using a loop, but there is another way - broadcasting. Take a look at the shape of valid_3_tens and mean3 , they are different... valid_3_tens . shape , mean3 . shape (torch.Size([1010, 28, 28]), torch.Size([28, 28])) Now calculate the distance between the two... valid_3_dist = mnist_distance ( valid_3_tens , mean3 ) valid_3_dist , valid_3_dist . shape (tensor([0.1290, 0.1223, 0.1380, ..., 0.1337, 0.1132, 0.1097]), torch.Size([1010])) our function has returned the distance for every single image as a rank-1 tensor of length 1,010. This is because we have added a subtraction (a-b) into our distance function and when PyTorch performs this subtraction, it uses broadcasting which will automatically expand the tensor with smaller rank to have the same size as the one with larger rank. Once this has happened, PyTorch will perform an element wise operation over the two tensors. In our case, PyTorch is treating mean3 (a rank 2 tensor) as if it were 1,010 copies of that tensor. You can see that by performing a subtraction and checking the shape ( valid_3_tens - mean3 ) . shape torch.Size([1010, 28, 28]) There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance: PyTorch doesn't actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn't actually allocate any additional memory It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!). source","title":"A simple model"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#a-funtion-to-make-a-decision","text":"is_3 is going to use mnist_distance to figure out whether an image is a 3 or a 7. To do this it will check whether the distance between a digit, x and mean3 is less than the difference between x and mean7 . def is_3 ( x ): return mnist_distance ( x , mean3 ) < mnist_distance ( x , mean7 ) # let's test it # you can convert a boolean to a float is_3 ( a_3 ), is_3 ( a_3 ) . float () (tensor(True), tensor(1.)) Calculate the accuracy for each 3 and 7 by taking the average of is_3 for all 3s and it's inverse for all 7s. accuracy_3s = is_3 ( valid_3_tens ) . float () . mean () accuracy_7s = ( 1 - is_3 ( valid_7_tens ) . float ()) . mean () accuracy_3s , accuracy_7s , ( accuracy_3s + accuracy_7s ) / 2 (tensor(0.9168), tensor(0.9854), tensor(0.9511)) Not bad, over 90% accuracy using a very simple model. This was also a very simple problem, 3s and 7s look very different so it's not really a surprise that this was so effective. We will nowe look at a system that will do some learning (automatically modify itself to improve its performance) In the next part, we will implement this more advanced model","title":"A funtion to make a decision"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 3: Under the Hood: Training a Digit Classifier 26-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. In part 1 we used a simple model that had no learned components. In this notebook, we will explore a smarter solution. We will apply this method back to the MNIST problem in the next notebook. Stocastic Gradient Descent (SGD) Instead of measuring how close something is to an \"ideal\" image, we could find a set of weights for each pixel, the highest weights will be associated with pixels that are most likely to be black for a particular category (in our case, number). This can be represented by a function and a set of weight values for each possible category - ie the probability of a category being an 8. def prob_eight(x,w) = (x*w)sum() Here x is a vector that represents an image (with all rows stacked up) and w is a vector of weights. With this function, we now just need a way to gradually update the weights to make them better and better until they are as good as they can get. In other words, we want to find the specific values of w that will cause the result of our function to be high when passed images of 8s and low for other digits. So by updating w we are optimising the function to recognise 8s. The steps we will follow are: 1. Initialize the weights. - start out with a random guess. 2. For each image, use these weights to predict whether it appears to be a 3 or a 7. 3. Based on these predictions, calculate how good the model is (its loss). 4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss. 5. Step (that is, update) all the weights based on that calculation. 6. Go back to the step 2, repeat the process. 7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer). source We will use a very simple example for illustration purposes. from fastai.vision.all import * from utils import * # define a quadratic function def f ( x ): return x ** 2 I had some trouble finding the plot_function function so found the source code from the forums. def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title ) # plot that function plot_function ( f , 'x' , 'x**2' ) # start with a random value for a parameter plt . scatter ( - 1.5 , f ( - 1.5 ), color = 'red' ); Now we need to see what would happen if we increase or decrease our parameter by a small amout. The goal, to find the lowest point in the curve. We do this by calculating the gradient at a particular point. We can change our weight by a small amount in the direction of the slope, then calculate the loss, make an adjustment and repeat until we reach our goal. Calculating Gradients The slope of a line can be described as the rate of change of a verticle variable with respect to a horizontal variabe. This is the gradient. By calculating the gradient, it will tell us how much we have to change each weight to make our model better. A Derivative is the instantaneous rate of change at a particular point. So how much is y changing with respect to x at that point. You can achieve this by calculating the slope of a tangent line. This video provides a good explanation of the concept. We can calculate the derivative for any function. For the quadratic above, the derivative is another function that calculates change, rather than the value. If we know how our function changes at a particular value, then we know how to minimize it. \"This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradients of our functions\" source PyTorch helps us do this using vector calculus. eg. # define a vector xt # requires_grad_ lets OyTorch know we need to calculate gradients xt = tensor ([ 3. , 4. , 10. ]) . requires_grad_ () xt tensor([ 3., 4., 10.], requires_grad=True) # define a function x that takes a vector (rank 1 tensor) # and returns a scalar (rank 0 tensor) # do this by summing the result of x**2 def f ( x ): return ( x ** 2 ) . sum () yt = f ( xt ) yt tensor(125., grad_fn=<SumBackward0>) calling backward refers to back propagation, which is the process of calculating the derivative of each layer. We can then use .grad to view the gradients. We can confirm that the derivative of x**2 is 2*x yt . backward () xt . grad tensor([ 6., 8., 20.]) Stepping with a learning rate Now that we know how to calculate the slope of our function, that tells us if we change our input a little bit, how will our out change correspondingly. let's call the weights w to update them we do the following w -= gradient(w) * lr So update w by subtracting the gradient of w multiplied by the learning rate lr . This process is known as, stepping your parameters, using and optimiser step. Defining a good learning rate is one of the key principles in machine learning. An intuitive way to think about it is, if the lr is too small, it will take you forever to reach the goal, if it is too big, you will over shoot that target. End-to-end Gradient Descent example. Use gradient descent to see how finding a minimum can be used to train a model to fir better data.. Let's measure the speed of a rollercoaster as it goes over a rise - starting fast, then slowing down at the peak, then speeding up again. If we measured the speed at 20 second intervals it might look something like this. time = torch . arange ( 0 , 20 ) . float () # speed is a quadratic with some added noise speed = torch . randn ( 20 ) * 3 + 0.75 * ( time - 9.5 ) ** 2 + 1 plt . scatter ( time , speed ); We need to create a function that estimates at any time, the speed of the rollercoaster. Start with a random guess, here let's use a quadratic a*(time**2) + (b*time) + c # here the input t is time, params will be a list a,b,c def f ( t , params ): a , b , c = params return a * ( t ** 2 ) + ( b * t ) + c So the goal is to find some function, or the best imaginable function that fits the data (we have simplified to finding the best quadratic function which is defined by the params a , b and c ). So finding the best f can be achieved by finding the best a , b and c values. We will need a loss function for this task. def mse ( preds , targets ): return (( preds - targets ) ** 2 ) . mean () Implement the 7 steps 1. Initialize the weights (params) to random values. Let PyTorch know that we will need to calculate the gradients. params = torch . randn ( 3 ) . requires_grad_ () orig_params = params . clone () # save these to check later 2. Calculate the predictions using our function Then check how close/far the predictions are from the targets. preds = f ( time , params ) def show_preds ( preds , ax = None ): if ax is None : ax = plt . subplots ()[ 1 ] ax . scatter ( time , speed ) ax . scatter ( time , to_np ( preds ), color = 'r' ) ax . set_ylim ( - 300 , 100 ) show_preds ( preds ) 3. Calculate the loss the goal is to improve the loss so we will need to know the gradients loss = mse ( preds , speed ) loss tensor(4422.5112, grad_fn=<MeanBackward0>) 4. Calculate the gradients Then use these to improve the parameters. We need a learning rate for this loss . backward () params . grad tensor([-20635.2617, -1319.6385, -108.2016]) lr = 1e-5 params . grad * lr tensor([-0.2064, -0.0132, -0.0011]) 5. Step the weights update the parameters based on the gradients we have calculated. Stepping the weights is w -= gradient(w) * lr .data is a special attribute in torch that means we don't want the gradient calculated. Here, we do not want the gradient calculated for the step we are doing, we only want the gradiend of the function f to be calculated params . data -= lr * params . grad . data params . grad = None # delete the gradients we already had # check if loss has improved # previous was 4422.5 preds = f ( time , params ) mse ( preds , speed ) tensor(1354.7021, grad_fn=<MeanBackward0>) show_preds ( preds ) # the preds have indeed improved! # repeat a few times def apply_step ( params , prn = True ): preds = f ( time , params ) loss = mse ( preds , speed ) loss . backward () params . data -= lr * params . grad . data params . grad = None if prn : print ( loss . item ()) return preds 6. Repeat the process by looping through and making improvements # repeat 10 times for i in range ( 10 ): apply_step ( params ) 1354.7021484375 774.1762084960938 664.3204956054688 643.5296630859375 639.5927734375 638.8450317382812 638.7008666992188 638.6709594726562 638.6625366210938 638.6583862304688 We can visualise this to see that for each step, an entirely different quadratic function is being tried params = orig_params . detach () . requires_grad_ () _ , axs = plt . subplots ( 1 , 4 , figsize = ( 12 , 3 )) for ax in axs : show_preds ( apply_step ( params , False ), ax ) plt . tight_layout () 7. Stop Summary We have just seen that by comparing the outputs of our model to our targets using a loss function, we are able to minimize the loss by gradually improving our weights (params).","title":"Lesson 03 pt 2"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#lesson-3-under-the-hood-training-a-digit-classifier","text":"26-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. In part 1 we used a simple model that had no learned components. In this notebook, we will explore a smarter solution. We will apply this method back to the MNIST problem in the next notebook.","title":"Lesson 3: Under the Hood: Training a Digit Classifier"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#stocastic-gradient-descent-sgd","text":"Instead of measuring how close something is to an \"ideal\" image, we could find a set of weights for each pixel, the highest weights will be associated with pixels that are most likely to be black for a particular category (in our case, number). This can be represented by a function and a set of weight values for each possible category - ie the probability of a category being an 8. def prob_eight(x,w) = (x*w)sum() Here x is a vector that represents an image (with all rows stacked up) and w is a vector of weights. With this function, we now just need a way to gradually update the weights to make them better and better until they are as good as they can get. In other words, we want to find the specific values of w that will cause the result of our function to be high when passed images of 8s and low for other digits. So by updating w we are optimising the function to recognise 8s. The steps we will follow are: 1. Initialize the weights. - start out with a random guess. 2. For each image, use these weights to predict whether it appears to be a 3 or a 7. 3. Based on these predictions, calculate how good the model is (its loss). 4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss. 5. Step (that is, update) all the weights based on that calculation. 6. Go back to the step 2, repeat the process. 7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer). source We will use a very simple example for illustration purposes. from fastai.vision.all import * from utils import * # define a quadratic function def f ( x ): return x ** 2 I had some trouble finding the plot_function function so found the source code from the forums. def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title ) # plot that function plot_function ( f , 'x' , 'x**2' ) # start with a random value for a parameter plt . scatter ( - 1.5 , f ( - 1.5 ), color = 'red' ); Now we need to see what would happen if we increase or decrease our parameter by a small amout. The goal, to find the lowest point in the curve. We do this by calculating the gradient at a particular point. We can change our weight by a small amount in the direction of the slope, then calculate the loss, make an adjustment and repeat until we reach our goal.","title":"Stocastic Gradient Descent (SGD)"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#calculating-gradients","text":"The slope of a line can be described as the rate of change of a verticle variable with respect to a horizontal variabe. This is the gradient. By calculating the gradient, it will tell us how much we have to change each weight to make our model better. A Derivative is the instantaneous rate of change at a particular point. So how much is y changing with respect to x at that point. You can achieve this by calculating the slope of a tangent line. This video provides a good explanation of the concept. We can calculate the derivative for any function. For the quadratic above, the derivative is another function that calculates change, rather than the value. If we know how our function changes at a particular value, then we know how to minimize it. \"This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradients of our functions\" source PyTorch helps us do this using vector calculus. eg. # define a vector xt # requires_grad_ lets OyTorch know we need to calculate gradients xt = tensor ([ 3. , 4. , 10. ]) . requires_grad_ () xt tensor([ 3., 4., 10.], requires_grad=True) # define a function x that takes a vector (rank 1 tensor) # and returns a scalar (rank 0 tensor) # do this by summing the result of x**2 def f ( x ): return ( x ** 2 ) . sum () yt = f ( xt ) yt tensor(125., grad_fn=<SumBackward0>) calling backward refers to back propagation, which is the process of calculating the derivative of each layer. We can then use .grad to view the gradients. We can confirm that the derivative of x**2 is 2*x yt . backward () xt . grad tensor([ 6., 8., 20.])","title":"Calculating Gradients"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#stepping-with-a-learning-rate","text":"Now that we know how to calculate the slope of our function, that tells us if we change our input a little bit, how will our out change correspondingly. let's call the weights w to update them we do the following w -= gradient(w) * lr So update w by subtracting the gradient of w multiplied by the learning rate lr . This process is known as, stepping your parameters, using and optimiser step. Defining a good learning rate is one of the key principles in machine learning. An intuitive way to think about it is, if the lr is too small, it will take you forever to reach the goal, if it is too big, you will over shoot that target.","title":"Stepping with a learning rate"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#end-to-end-gradient-descent-example","text":"Use gradient descent to see how finding a minimum can be used to train a model to fir better data.. Let's measure the speed of a rollercoaster as it goes over a rise - starting fast, then slowing down at the peak, then speeding up again. If we measured the speed at 20 second intervals it might look something like this. time = torch . arange ( 0 , 20 ) . float () # speed is a quadratic with some added noise speed = torch . randn ( 20 ) * 3 + 0.75 * ( time - 9.5 ) ** 2 + 1 plt . scatter ( time , speed ); We need to create a function that estimates at any time, the speed of the rollercoaster. Start with a random guess, here let's use a quadratic a*(time**2) + (b*time) + c # here the input t is time, params will be a list a,b,c def f ( t , params ): a , b , c = params return a * ( t ** 2 ) + ( b * t ) + c So the goal is to find some function, or the best imaginable function that fits the data (we have simplified to finding the best quadratic function which is defined by the params a , b and c ). So finding the best f can be achieved by finding the best a , b and c values. We will need a loss function for this task. def mse ( preds , targets ): return (( preds - targets ) ** 2 ) . mean ()","title":"End-to-end Gradient Descent example."},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#implement-the-7-steps","text":"1. Initialize the weights (params) to random values. Let PyTorch know that we will need to calculate the gradients. params = torch . randn ( 3 ) . requires_grad_ () orig_params = params . clone () # save these to check later 2. Calculate the predictions using our function Then check how close/far the predictions are from the targets. preds = f ( time , params ) def show_preds ( preds , ax = None ): if ax is None : ax = plt . subplots ()[ 1 ] ax . scatter ( time , speed ) ax . scatter ( time , to_np ( preds ), color = 'r' ) ax . set_ylim ( - 300 , 100 ) show_preds ( preds ) 3. Calculate the loss the goal is to improve the loss so we will need to know the gradients loss = mse ( preds , speed ) loss tensor(4422.5112, grad_fn=<MeanBackward0>) 4. Calculate the gradients Then use these to improve the parameters. We need a learning rate for this loss . backward () params . grad tensor([-20635.2617, -1319.6385, -108.2016]) lr = 1e-5 params . grad * lr tensor([-0.2064, -0.0132, -0.0011]) 5. Step the weights update the parameters based on the gradients we have calculated. Stepping the weights is w -= gradient(w) * lr .data is a special attribute in torch that means we don't want the gradient calculated. Here, we do not want the gradient calculated for the step we are doing, we only want the gradiend of the function f to be calculated params . data -= lr * params . grad . data params . grad = None # delete the gradients we already had # check if loss has improved # previous was 4422.5 preds = f ( time , params ) mse ( preds , speed ) tensor(1354.7021, grad_fn=<MeanBackward0>) show_preds ( preds ) # the preds have indeed improved! # repeat a few times def apply_step ( params , prn = True ): preds = f ( time , params ) loss = mse ( preds , speed ) loss . backward () params . data -= lr * params . grad . data params . grad = None if prn : print ( loss . item ()) return preds 6. Repeat the process by looping through and making improvements # repeat 10 times for i in range ( 10 ): apply_step ( params ) 1354.7021484375 774.1762084960938 664.3204956054688 643.5296630859375 639.5927734375 638.8450317382812 638.7008666992188 638.6709594726562 638.6625366210938 638.6583862304688 We can visualise this to see that for each step, an entirely different quadratic function is being tried params = orig_params . detach () . requires_grad_ () _ , axs = plt . subplots ( 1 , 4 , figsize = ( 12 , 3 )) for ax in axs : show_preds ( apply_step ( params , False ), ax ) plt . tight_layout () 7. Stop","title":"Implement the 7 steps"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#summary","text":"We have just seen that by comparing the outputs of our model to our targets using a loss function, we are able to minimize the loss by gradually improving our weights (params).","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2004/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 4: Under the Hood: Training a Digit Classifier 28-09-2020 This notebook will go over some of the practical material discussed in lesson 4 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. The lesson 4 video is an extension on the lesson 3 video. There is a lot to cover... In the last notebook we looked at some simple examples of using SGD to optimise a model. In this notebook we will apply the concepts to the MNIST problem from scratch then leter, we will refactor the code using PyTorch and fastai modules. # imports and things we need from previous notebooks from fastai.vision.all import * # data path = untar_data ( URLs . MNIST_SAMPLE ) threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title ) MNIST Loss function Our X values will be pixels, we need to reshape the data using view . We want to concatenate our x's into a single tensor, then change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). Why? Because this example is meant to be simplified. view will return a new tensor with the same data as the original tensor but with a different shape that we define. # concat 3s and 7s, then reshape into a matrix # so that each row is 1 image, with all rows and columns in a single vector train_x = torch . cat ([ stacked_threes , stacked_sevens ]) . view ( - 1 , 28 * 28 ) # label the data # 3 == 1 # 7 == 0 # we need this to be a matrix # unsqueeze will do this for us train_y = tensor ([ 1 ] * len ( threes ) + [ 0 ] * len ( sevens )) . unsqueeze ( 1 ) # check the shape train_x . shape , train_y . shape (torch.Size([12396, 784]), torch.Size([12396, 1])) # in PyTorch we need data to be in a tuple for each row # zip will help us with this dset = list ( zip ( train_x , train_y )) # take a look at the first thing x , y = dset [ 0 ] x . shape , y (torch.Size([784]), tensor([1])) (torch.Size([784]), tensor([1])) this matches what we would expect # repeat for validation valid_x = torch . cat ([ valid_3_tens , valid_7_tens ]) . view ( - 1 , 28 * 28 ) valid_y = tensor ([ 1 ] * len ( valid_3_tens ) + [ 0 ] * len ( valid_7_tens )) . unsqueeze ( 1 ) valid_dset = list ( zip ( valid_x , valid_y )) Now we have training and validation data sets 1. Randomly initialise weights for each pixel - use torch.randn to create tensor of randomly initialised weights def init_params ( size , var = 1.0 ): return ( torch . randn ( size ) * var ) . requires_grad_ () weights = init_params (( 28 * 28 , 1 )) weights . shape torch.Size([784, 1]) We need to add a bias term because just using weights*pixels will not be flexible enough. Our function will always be equal to zero when the pixels are equal to zero. bias = init_params ( 1 ) y = w*x+b is the formula for a line, where w are the weights, b is the bias. In neural network jargon, the weights and bias will be our parameters . This linear equation is one of the two fundamental equations of any neural network. The other is an activation function that we will see shortly. Let's use this to calculate a prediction for one image... weights.T will transpose the weights, this is done to make sure the rows and columns match up for our multiplication ( train_x [ 0 ] * weights . T ) . sum () + bias tensor([13.3326], grad_fn=<AddBackward0>) Now we need to do this for all images. A for loop will be too slow. In PyTorch we can perform matrix multiplication using the @ operator OR by using torch.matmul() . # define a linear function that will # multiple the input by weights then add a bias term def linear1 ( xb ): return xb @weights + bias preds = linear1 ( train_x ) preds tensor([[13.3326], [ 9.1011], [ 9.4999], ..., [-1.0068], [15.9130], [12.6228]], grad_fn=<AddBackward0>) Notice the result are the same as we just saw above. We can confirm our function is working and can also see that the operation is performed for every image in train_x checking accuracy - if a prediction is above the threshold, ie if > 0 then it is a 3, less than 0, 7. - so we check if a prediction is greater than our threshold of 0, then check these against the validation set. - this will return true when a row is correctly predicted - we can convert these to floats using .float() then take their mean to check overall accuracy of our randomly initialised model threshold = 0.0 accuracy = ( preds > threshold ) . float () == train_y accuracy tensor([[ True], [ True], [ True], ..., [ True], [False], [False]]) accuracy . float () . mean () . item () 0.484188437461853 Let's change one of the weights by a small amount to see how accuracy is affected. weights [ 0 ] += 1.0001 # increase the weigh a little preds = linear1 ( train_x ) accuracy2 = (( preds > threshold ) . float () == train_y ) . float () . mean () . item () accuracy2 0.484188437461853 This is exactly the same as before. We have a problem, when we calculate the change, our gradient is now 0, this is because if we change a single pixel by a very small amount we might not change an actual prediction. So because our gradient is 0, our step will be 0 which means our prediction will be unchanged. So our accuracy loss function is not very good. A small change in our weights does not result in a small change in accuracy, so we will have zero gradients. We need a new function that won't have a zero gradient, it needs to be more sensitive to small changes, so that a slightly better prediction needs to have a slightly better loss. In other words, then the predictions are close to the targets the loss needs to be small, when they are far away, it needs to be big. So let's create a new function to address this issue. # MNIST loss def mnist_loss ( preds , targets ): return torch . where ( targets == 1. , 1. - preds , preds ) . mean () # test case t = torch . tensor ([ 1 , 0 , 1 ]) # targets p = torch . tensor ([ 0.9 , 0.4 , 0.2 ]) # predictions # this is the same as mnist_loss but before the mean torch . where ( t == 1 , 1 - p , p ) tensor([0.1000, 0.4000, 0.8000]) torch.where is like list comprehension for tensors. This function returns a lower loss when predictions are more accurate and a higher loss when they are not. But for this to work, we need our predictions to be between 0 and 1, otherwise things do not work. p2 = torch . tensor ([ 1.2 , - 1 , 0 ]) # predictions outside 0, 1 range torch . where ( t == 1 , 1 - p2 , p2 ) tensor([-0.2000, -1.0000, 1.0000]) The Sigmoid function This function will constrain our numbers between 0 and 1. It squashes any input in the range (-inf, inf) to some value in the range (0, 1) def sigmoid ( x ) : return 1 / ( 1 + torch . exp ( - x )) plot_function ( torch . sigmoid , title = 'Sigmoid' , min =- 4 , max = 4 ) # MNIST loss with sigmoid def mnist_loss ( predictions , targets ): preds = predictions . sigmoid () return torch . where ( targets == 1. , 1. - preds , preds ) . mean () SGD and Mini-batches By batching images and running computations over them is a way to compromise between speed and computational efficiency. The size of the batch will impact your accuracy and estimates as well as the speed at which you are able to run computations. The batch size is something to be considered during training. The DataLoader class in pytorch helps with batching. It returns an iterator which we can loop through. coll = range ( 15 ) dl = DataLoader ( coll , batch_size = 5 , shuffle = True ) list ( dl ) [tensor([ 4, 12, 5, 6, 3]), tensor([10, 9, 2, 0, 14]), tensor([ 7, 13, 8, 11, 1])] Putting it together # re-initialise weights and params weights = init_params (( 28 * 28 , 1 )) bias = init_params ( 1 ) # create a data loader dl = DataLoader ( dset , batch_size = 256 ) # grab the first x and y xb , yb = first ( dl ) # check the shape xb . shape , yb . shape (torch.Size([256, 784]), torch.Size([256, 1])) # repeat for validation set valid_dl = DataLoader ( valid_dset , batch_size = 256 ) # grab a mini batch to test on batch = train_x [: 4 ] batch . shape torch.Size([4, 784]) # make some predictions preds = linear1 ( batch ) preds tensor([[-1.1306], [-3.9293], [-0.6736], [-6.9805]], grad_fn=<AddBackward0>) loss = mnist_loss ( preds , train_y [: 4 ]) loss tensor(0.8495, grad_fn=<MeanBackward0>) # calculate gradients loss . backward () weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0153), tensor([-0.1070])) # take those 3 steps and put it in a function def calc_grad ( xb , yb , model ): preds = model ( xb ) loss = mnist_loss ( preds , yb ) loss . backward () # test it calc_grad ( batch , train_y [: 4 ], linear1 ) weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0306), tensor([-0.2140])) # zero the gradients weights . grad . zero_ () bias . grad . zero_ () tensor([0.]) The last step is to work out how to update the weights and bias based on the gradient and learning rate. train_epoch loops through the data loader, grab x batch and y batch, calculate the gradient, make a prediction and calculate the loss. Go through each parameter (weights and bias) and for each update with gradient * lr, then zero these in prep for the next loop. p.data is used because PyTorch keeps track of all operations so it can calculate the gradients, but we do not want the gradients to be calculated on the gradient descent step. def train_epoch ( model , lr , params ): for xb , yb in dl : calc_grad ( xb , yb , model ) for p in params : p . data -= p . grad * lr p . grad . zero_ () batch_accuracy is similar to the previous loss function, but since we use a sigmoid, which constrains our preds between 0 and 1, we need to check whether preds > 0.5. def batch_accuracy ( xb , yb ): preds = xb . sigmoid () correct = ( preds > 0.5 ) == yb # check predictions against target return correct . float () . mean () batch_accuracy ( linear1 ( train_x [: 4 ]), train_y [: 4 ]) tensor(0.) # check accuracy for every batch in the validation set # stack converts the list of items into tensor def validate_epoch ( model ): accs = [ batch_accuracy ( model ( xb ), yb ) for xb , yb in valid_dl ] return round ( torch . stack ( accs ) . mean () . item (), 4 ) validate_epoch ( linear1 ) 0.407 This is a starting point, let's train for one epoch and see if accuracy improves. as a reminder, the linear1 function was... - def linear1(xb): return xb@weights + bias lr = 1. params = weights , bias train_epoch ( linear1 , lr , params ) validate_epoch ( linear1 ) 0.6932 for i in range ( 20 ): train_epoch ( linear1 , lr , params ) print ( validate_epoch ( linear1 ), end = ' ' ) 0.8242 0.9042 0.9355 0.9501 0.9555 0.9614 0.9638 0.9677 0.9736 0.9751 0.9751 0.976 0.977 0.9775 0.9775 0.978 0.9785 0.979 0.9795 0.979 Accuracy has indeed improved! We have built an SGD optimizer that has reached about 97% accuracy. Refactor and clean up create an optimiser use PyTorch modules and functions where available like nn.Linear which \"Applies a linear transformation to the incoming data: $y = xA^T + b$\" nn . Linear ? # remove our linear function # in place for torch module # creates a matrix of size 28*28 # with bias of 1 linear_model = nn . Linear ( 28 * 28 , 1 ) # check model params w , b = linear_model . parameters () w . shape , b . shape (torch.Size([1, 784]), torch.Size([1])) Create a basic optimiser pass in params to optimise and lr store these away step though each param (weights and bias) and for each, update with gradient * lr zero the gradients in prep for the next step class BasicOptim : def __init__ ( self , params , lr ): self . params , self . lr = list ( params ), lr def step ( self , * args , ** kwargs ): for p in self . params : p . data -= p . grad . data * self . lr def zero_grad ( self , * args , ** kwargs ): for p in self . params : p . grad = None # create an optimiser by passing in parameters from model opt = BasicOptim ( linear_model . parameters (), lr ) # simplify the training loop def train_epoch ( model ): for xb , yb in dl : calc_grad ( xb , yb , model ) opt . step () opt . zero_grad () validate_epoch ( linear_model ) 0.5075 Now create a function train_model that will call train_epoch on our model for the specified number of epochs def train_model ( model , epochs ): for i in range ( epochs ): train_epoch ( model ) print ( validate_epoch ( model ), end = ' ' ) train_model ( linear_model , 20 ) 0.4932 0.7876 0.852 0.916 0.9345 0.9497 0.957 0.9638 0.9658 0.9677 0.9697 0.9721 0.9731 0.9751 0.9755 0.9765 0.9775 0.9775 0.978 0.9785 The results are very similar to what we have seen before. Fastai provides SGD that we can use instead of writing our own, again the results are very similar. linear_model = nn . Linear ( 28 * 28 , 1 ) opt = SGD ( linear_model . parameters (), lr ) train_model ( linear_model , 20 ) 0.4932 0.9091 0.8056 0.9043 0.9316 0.9443 0.9546 0.9619 0.9648 0.9668 0.9692 0.9707 0.9731 0.9746 0.976 0.976 0.9775 0.9775 0.9785 0.979 Let's refactor some more, using some fastai classes. The Learner implements everything we have implemented manually. # Previously we used DataLoader not DataLoaders dls = DataLoaders ( dl , valid_dl ) learn = Learner ( dls , nn . Linear ( 28 * 28 , 1 ), opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 10 ) epoch train_loss valid_loss batch_accuracy time 0 0.480832 0.461122 0.843474 00:00 1 0.466804 0.441299 0.908734 00:00 2 0.450758 0.422136 0.934249 00:00 3 0.433590 0.403750 0.943572 00:00 4 0.416052 0.386223 0.949460 00:00 5 0.398675 0.369610 0.952404 00:00 6 0.381799 0.353943 0.956820 00:00 7 0.365630 0.339228 0.957311 00:00 8 0.350289 0.325455 0.959764 00:00 9 0.335835 0.312599 0.960255 00:00 The results again are very similar, but with some additional functionality (like printing out results in a pretty table! Non-Linearity To create a simple neural net, using a linear function like we did before is not enough. We need to add in a non-linearity between two linear functions. This is the basic definition for a neural net.. The universal approximation theorem says, that given any arbitrarily complex continuous function, we can approximate it with a neural network. I found this useful for visualising how this works. This is what we are trying to do. In our basic_net , each line represents a layer in our network, the first and 3rd layers are known as linear layers the second, as a nonlinearity or an activation. res.max(tensor(0.0)) takes the result of our linear function and sets any negative value to 0.0 while maintaining any positive values. def basic_net ( xb ): res = xb @w1 + b1 res = res . max ( tensor ( 0.0 )) res = res @w2 + b2 return res plot_function ( F . relu ) Like we have seen previously.. - w1 and w2 are weight tensors - b1 and b2 are bias tensors we can initialise these the same as we have done previously.. w1 has 30 output activations, so in order for w2 to match it require 30 input activations. w1 = init_params (( 28 * 28 , 30 )) b1 = init_params ( 30 ) w2 = init_params (( 30 , 1 )) b2 = init_params ( 1 ) We can simplify further using PyTorch... What we did in basic_net was called function composition, where we passed the results of one function into another function and then into another function. This is what neural nets are doing with linear layers and activation functions. nn.Sequential() will do this for us... simple_net = nn . Sequential ( nn . Linear ( 28 * 28 , 30 ), # 28*28 in, 30 out nn . ReLU (), nn . Linear ( 30 , 1 ) # 30 in 1 out ) learn = Learner ( dls , simple_net , opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 40 , 0.1 ) epoch train_loss valid_loss batch_accuracy time 0 0.301185 0.414520 0.506379 00:00 1 0.142869 0.223012 0.814033 00:00 2 0.079959 0.114103 0.916094 00:00 3 0.053115 0.077652 0.939156 00:00 4 0.040578 0.060868 0.953876 00:00 5 0.034118 0.051373 0.963690 00:00 6 0.030368 0.045362 0.965653 00:00 7 0.027905 0.041246 0.965162 00:00 8 0.026117 0.038246 0.968597 00:00 9 0.024726 0.035950 0.969087 00:00 10 0.023596 0.034124 0.971050 00:00 11 0.022651 0.032629 0.972031 00:00 12 0.021847 0.031376 0.973503 00:00 13 0.021151 0.030301 0.974485 00:00 14 0.020542 0.029363 0.974485 00:00 15 0.020002 0.028535 0.975957 00:00 16 0.019519 0.027797 0.976448 00:00 17 0.019083 0.027134 0.976938 00:00 18 0.018687 0.026535 0.977920 00:00 19 0.018325 0.025991 0.978901 00:00 20 0.017992 0.025495 0.978901 00:00 21 0.017684 0.025040 0.978901 00:00 22 0.017398 0.024621 0.979392 00:00 23 0.017131 0.024233 0.979392 00:00 24 0.016881 0.023874 0.980373 00:00 25 0.016645 0.023541 0.980373 00:00 26 0.016424 0.023232 0.980373 00:00 27 0.016214 0.022943 0.980864 00:00 28 0.016015 0.022673 0.980864 00:00 29 0.015827 0.022421 0.981354 00:00 30 0.015648 0.022185 0.981845 00:00 31 0.015476 0.021963 0.982336 00:00 32 0.015313 0.021756 0.982336 00:00 33 0.015156 0.021561 0.982336 00:00 34 0.015006 0.021378 0.982826 00:00 35 0.014863 0.021204 0.982826 00:00 36 0.014725 0.021041 0.982336 00:00 37 0.014592 0.020886 0.982336 00:00 38 0.014464 0.020740 0.982336 00:00 39 0.014341 0.020601 0.982336 00:00 # this is what our model now looks like learn . model Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) # plot the loss learn . recorder . plot_loss () # learn.recorder.values hold the table values above # lets plot the accuracy plt . plot ( L ( learn . recorder . values ) . itemgot ( 2 )); Looking inside... # let's visualise some of the parameters # 1. grab your model m = learn . model # (0): Linear(in_features=784, out_features=30, bias=True) # 2. look inside and grab the weights and biases w , b = m [ 0 ] . parameters () # 3. grab first (or any) row, reshape, and plot show_image ( w [ 0 ] . view ( 28 , 28 ), figsize = ( 4 , 4 )) <AxesSubplot:> fastai in full from fastai.vision.all import * from pathlib import Path path = Path . cwd () / 'datasets/fastai/mnist_sample' dls = ImageDataLoaders . from_folder ( path ) learn = cnn_learner ( dls , resnet18 , pretrained = False , loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 1 , 0.1 ) epoch train_loss valid_loss accuracy time 0 0.086805 0.025215 0.994603 00:11 Summary We have gone over creating and training a neural network from scratch using the simple example of a digit classifier. The key idea for the last few notebooks was to start with planning out the problem and identifying a way to solve it using a simple common sense solution - the pixel similarity model. This proved successful but it was not really robust beyond the straightforward example we chose - identifying 3s and 7s. We then implemented a more complex solution that could be applied to more complicated problems. After each step or concept had been implemented manually, we refactored the code to use convenient PyTorch functions and modules, eventually ending up with using fastai's implementation which abstracts away from all of the underlying heavy lifting. This is done for convenience and in my own opinion, to help lower the entry barrier into deep learning. Ultimately I believe it is fundamentally important to understand the concepts and implementation if your goal (and this is my goal) is to implement deep learning solutions to solve business problems within your industry.","title":"Lesson 04"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#lesson-4-under-the-hood-training-a-digit-classifier","text":"28-09-2020 This notebook will go over some of the practical material discussed in lesson 4 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. The lesson 4 video is an extension on the lesson 3 video. There is a lot to cover... In the last notebook we looked at some simple examples of using SGD to optimise a model. In this notebook we will apply the concepts to the MNIST problem from scratch then leter, we will refactor the code using PyTorch and fastai modules. # imports and things we need from previous notebooks from fastai.vision.all import * # data path = untar_data ( URLs . MNIST_SAMPLE ) threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title )","title":"Lesson 4: Under the Hood: Training a Digit Classifier"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#mnist-loss-function","text":"Our X values will be pixels, we need to reshape the data using view . We want to concatenate our x's into a single tensor, then change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). Why? Because this example is meant to be simplified. view will return a new tensor with the same data as the original tensor but with a different shape that we define. # concat 3s and 7s, then reshape into a matrix # so that each row is 1 image, with all rows and columns in a single vector train_x = torch . cat ([ stacked_threes , stacked_sevens ]) . view ( - 1 , 28 * 28 ) # label the data # 3 == 1 # 7 == 0 # we need this to be a matrix # unsqueeze will do this for us train_y = tensor ([ 1 ] * len ( threes ) + [ 0 ] * len ( sevens )) . unsqueeze ( 1 ) # check the shape train_x . shape , train_y . shape (torch.Size([12396, 784]), torch.Size([12396, 1])) # in PyTorch we need data to be in a tuple for each row # zip will help us with this dset = list ( zip ( train_x , train_y )) # take a look at the first thing x , y = dset [ 0 ] x . shape , y (torch.Size([784]), tensor([1])) (torch.Size([784]), tensor([1])) this matches what we would expect # repeat for validation valid_x = torch . cat ([ valid_3_tens , valid_7_tens ]) . view ( - 1 , 28 * 28 ) valid_y = tensor ([ 1 ] * len ( valid_3_tens ) + [ 0 ] * len ( valid_7_tens )) . unsqueeze ( 1 ) valid_dset = list ( zip ( valid_x , valid_y )) Now we have training and validation data sets 1. Randomly initialise weights for each pixel - use torch.randn to create tensor of randomly initialised weights def init_params ( size , var = 1.0 ): return ( torch . randn ( size ) * var ) . requires_grad_ () weights = init_params (( 28 * 28 , 1 )) weights . shape torch.Size([784, 1]) We need to add a bias term because just using weights*pixels will not be flexible enough. Our function will always be equal to zero when the pixels are equal to zero. bias = init_params ( 1 ) y = w*x+b is the formula for a line, where w are the weights, b is the bias. In neural network jargon, the weights and bias will be our parameters . This linear equation is one of the two fundamental equations of any neural network. The other is an activation function that we will see shortly. Let's use this to calculate a prediction for one image... weights.T will transpose the weights, this is done to make sure the rows and columns match up for our multiplication ( train_x [ 0 ] * weights . T ) . sum () + bias tensor([13.3326], grad_fn=<AddBackward0>) Now we need to do this for all images. A for loop will be too slow. In PyTorch we can perform matrix multiplication using the @ operator OR by using torch.matmul() . # define a linear function that will # multiple the input by weights then add a bias term def linear1 ( xb ): return xb @weights + bias preds = linear1 ( train_x ) preds tensor([[13.3326], [ 9.1011], [ 9.4999], ..., [-1.0068], [15.9130], [12.6228]], grad_fn=<AddBackward0>) Notice the result are the same as we just saw above. We can confirm our function is working and can also see that the operation is performed for every image in train_x checking accuracy - if a prediction is above the threshold, ie if > 0 then it is a 3, less than 0, 7. - so we check if a prediction is greater than our threshold of 0, then check these against the validation set. - this will return true when a row is correctly predicted - we can convert these to floats using .float() then take their mean to check overall accuracy of our randomly initialised model threshold = 0.0 accuracy = ( preds > threshold ) . float () == train_y accuracy tensor([[ True], [ True], [ True], ..., [ True], [False], [False]]) accuracy . float () . mean () . item () 0.484188437461853 Let's change one of the weights by a small amount to see how accuracy is affected. weights [ 0 ] += 1.0001 # increase the weigh a little preds = linear1 ( train_x ) accuracy2 = (( preds > threshold ) . float () == train_y ) . float () . mean () . item () accuracy2 0.484188437461853 This is exactly the same as before. We have a problem, when we calculate the change, our gradient is now 0, this is because if we change a single pixel by a very small amount we might not change an actual prediction. So because our gradient is 0, our step will be 0 which means our prediction will be unchanged. So our accuracy loss function is not very good. A small change in our weights does not result in a small change in accuracy, so we will have zero gradients. We need a new function that won't have a zero gradient, it needs to be more sensitive to small changes, so that a slightly better prediction needs to have a slightly better loss. In other words, then the predictions are close to the targets the loss needs to be small, when they are far away, it needs to be big. So let's create a new function to address this issue. # MNIST loss def mnist_loss ( preds , targets ): return torch . where ( targets == 1. , 1. - preds , preds ) . mean () # test case t = torch . tensor ([ 1 , 0 , 1 ]) # targets p = torch . tensor ([ 0.9 , 0.4 , 0.2 ]) # predictions # this is the same as mnist_loss but before the mean torch . where ( t == 1 , 1 - p , p ) tensor([0.1000, 0.4000, 0.8000]) torch.where is like list comprehension for tensors. This function returns a lower loss when predictions are more accurate and a higher loss when they are not. But for this to work, we need our predictions to be between 0 and 1, otherwise things do not work. p2 = torch . tensor ([ 1.2 , - 1 , 0 ]) # predictions outside 0, 1 range torch . where ( t == 1 , 1 - p2 , p2 ) tensor([-0.2000, -1.0000, 1.0000])","title":"MNIST Loss function"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#the-sigmoid-function","text":"This function will constrain our numbers between 0 and 1. It squashes any input in the range (-inf, inf) to some value in the range (0, 1) def sigmoid ( x ) : return 1 / ( 1 + torch . exp ( - x )) plot_function ( torch . sigmoid , title = 'Sigmoid' , min =- 4 , max = 4 ) # MNIST loss with sigmoid def mnist_loss ( predictions , targets ): preds = predictions . sigmoid () return torch . where ( targets == 1. , 1. - preds , preds ) . mean ()","title":"The Sigmoid function"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#sgd-and-mini-batches","text":"By batching images and running computations over them is a way to compromise between speed and computational efficiency. The size of the batch will impact your accuracy and estimates as well as the speed at which you are able to run computations. The batch size is something to be considered during training. The DataLoader class in pytorch helps with batching. It returns an iterator which we can loop through. coll = range ( 15 ) dl = DataLoader ( coll , batch_size = 5 , shuffle = True ) list ( dl ) [tensor([ 4, 12, 5, 6, 3]), tensor([10, 9, 2, 0, 14]), tensor([ 7, 13, 8, 11, 1])]","title":"SGD and Mini-batches"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#putting-it-together","text":"# re-initialise weights and params weights = init_params (( 28 * 28 , 1 )) bias = init_params ( 1 ) # create a data loader dl = DataLoader ( dset , batch_size = 256 ) # grab the first x and y xb , yb = first ( dl ) # check the shape xb . shape , yb . shape (torch.Size([256, 784]), torch.Size([256, 1])) # repeat for validation set valid_dl = DataLoader ( valid_dset , batch_size = 256 ) # grab a mini batch to test on batch = train_x [: 4 ] batch . shape torch.Size([4, 784]) # make some predictions preds = linear1 ( batch ) preds tensor([[-1.1306], [-3.9293], [-0.6736], [-6.9805]], grad_fn=<AddBackward0>) loss = mnist_loss ( preds , train_y [: 4 ]) loss tensor(0.8495, grad_fn=<MeanBackward0>) # calculate gradients loss . backward () weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0153), tensor([-0.1070])) # take those 3 steps and put it in a function def calc_grad ( xb , yb , model ): preds = model ( xb ) loss = mnist_loss ( preds , yb ) loss . backward () # test it calc_grad ( batch , train_y [: 4 ], linear1 ) weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0306), tensor([-0.2140])) # zero the gradients weights . grad . zero_ () bias . grad . zero_ () tensor([0.]) The last step is to work out how to update the weights and bias based on the gradient and learning rate. train_epoch loops through the data loader, grab x batch and y batch, calculate the gradient, make a prediction and calculate the loss. Go through each parameter (weights and bias) and for each update with gradient * lr, then zero these in prep for the next loop. p.data is used because PyTorch keeps track of all operations so it can calculate the gradients, but we do not want the gradients to be calculated on the gradient descent step. def train_epoch ( model , lr , params ): for xb , yb in dl : calc_grad ( xb , yb , model ) for p in params : p . data -= p . grad * lr p . grad . zero_ () batch_accuracy is similar to the previous loss function, but since we use a sigmoid, which constrains our preds between 0 and 1, we need to check whether preds > 0.5. def batch_accuracy ( xb , yb ): preds = xb . sigmoid () correct = ( preds > 0.5 ) == yb # check predictions against target return correct . float () . mean () batch_accuracy ( linear1 ( train_x [: 4 ]), train_y [: 4 ]) tensor(0.) # check accuracy for every batch in the validation set # stack converts the list of items into tensor def validate_epoch ( model ): accs = [ batch_accuracy ( model ( xb ), yb ) for xb , yb in valid_dl ] return round ( torch . stack ( accs ) . mean () . item (), 4 ) validate_epoch ( linear1 ) 0.407 This is a starting point, let's train for one epoch and see if accuracy improves. as a reminder, the linear1 function was... - def linear1(xb): return xb@weights + bias lr = 1. params = weights , bias train_epoch ( linear1 , lr , params ) validate_epoch ( linear1 ) 0.6932 for i in range ( 20 ): train_epoch ( linear1 , lr , params ) print ( validate_epoch ( linear1 ), end = ' ' ) 0.8242 0.9042 0.9355 0.9501 0.9555 0.9614 0.9638 0.9677 0.9736 0.9751 0.9751 0.976 0.977 0.9775 0.9775 0.978 0.9785 0.979 0.9795 0.979 Accuracy has indeed improved! We have built an SGD optimizer that has reached about 97% accuracy.","title":"Putting it together"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#refactor-and-clean-up","text":"create an optimiser use PyTorch modules and functions where available like nn.Linear which \"Applies a linear transformation to the incoming data: $y = xA^T + b$\" nn . Linear ? # remove our linear function # in place for torch module # creates a matrix of size 28*28 # with bias of 1 linear_model = nn . Linear ( 28 * 28 , 1 ) # check model params w , b = linear_model . parameters () w . shape , b . shape (torch.Size([1, 784]), torch.Size([1])) Create a basic optimiser pass in params to optimise and lr store these away step though each param (weights and bias) and for each, update with gradient * lr zero the gradients in prep for the next step class BasicOptim : def __init__ ( self , params , lr ): self . params , self . lr = list ( params ), lr def step ( self , * args , ** kwargs ): for p in self . params : p . data -= p . grad . data * self . lr def zero_grad ( self , * args , ** kwargs ): for p in self . params : p . grad = None # create an optimiser by passing in parameters from model opt = BasicOptim ( linear_model . parameters (), lr ) # simplify the training loop def train_epoch ( model ): for xb , yb in dl : calc_grad ( xb , yb , model ) opt . step () opt . zero_grad () validate_epoch ( linear_model ) 0.5075 Now create a function train_model that will call train_epoch on our model for the specified number of epochs def train_model ( model , epochs ): for i in range ( epochs ): train_epoch ( model ) print ( validate_epoch ( model ), end = ' ' ) train_model ( linear_model , 20 ) 0.4932 0.7876 0.852 0.916 0.9345 0.9497 0.957 0.9638 0.9658 0.9677 0.9697 0.9721 0.9731 0.9751 0.9755 0.9765 0.9775 0.9775 0.978 0.9785 The results are very similar to what we have seen before. Fastai provides SGD that we can use instead of writing our own, again the results are very similar. linear_model = nn . Linear ( 28 * 28 , 1 ) opt = SGD ( linear_model . parameters (), lr ) train_model ( linear_model , 20 ) 0.4932 0.9091 0.8056 0.9043 0.9316 0.9443 0.9546 0.9619 0.9648 0.9668 0.9692 0.9707 0.9731 0.9746 0.976 0.976 0.9775 0.9775 0.9785 0.979 Let's refactor some more, using some fastai classes. The Learner implements everything we have implemented manually. # Previously we used DataLoader not DataLoaders dls = DataLoaders ( dl , valid_dl ) learn = Learner ( dls , nn . Linear ( 28 * 28 , 1 ), opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 10 ) epoch train_loss valid_loss batch_accuracy time 0 0.480832 0.461122 0.843474 00:00 1 0.466804 0.441299 0.908734 00:00 2 0.450758 0.422136 0.934249 00:00 3 0.433590 0.403750 0.943572 00:00 4 0.416052 0.386223 0.949460 00:00 5 0.398675 0.369610 0.952404 00:00 6 0.381799 0.353943 0.956820 00:00 7 0.365630 0.339228 0.957311 00:00 8 0.350289 0.325455 0.959764 00:00 9 0.335835 0.312599 0.960255 00:00 The results again are very similar, but with some additional functionality (like printing out results in a pretty table!","title":"Refactor and clean up"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#non-linearity","text":"To create a simple neural net, using a linear function like we did before is not enough. We need to add in a non-linearity between two linear functions. This is the basic definition for a neural net.. The universal approximation theorem says, that given any arbitrarily complex continuous function, we can approximate it with a neural network. I found this useful for visualising how this works. This is what we are trying to do. In our basic_net , each line represents a layer in our network, the first and 3rd layers are known as linear layers the second, as a nonlinearity or an activation. res.max(tensor(0.0)) takes the result of our linear function and sets any negative value to 0.0 while maintaining any positive values. def basic_net ( xb ): res = xb @w1 + b1 res = res . max ( tensor ( 0.0 )) res = res @w2 + b2 return res plot_function ( F . relu ) Like we have seen previously.. - w1 and w2 are weight tensors - b1 and b2 are bias tensors we can initialise these the same as we have done previously.. w1 has 30 output activations, so in order for w2 to match it require 30 input activations. w1 = init_params (( 28 * 28 , 30 )) b1 = init_params ( 30 ) w2 = init_params (( 30 , 1 )) b2 = init_params ( 1 ) We can simplify further using PyTorch... What we did in basic_net was called function composition, where we passed the results of one function into another function and then into another function. This is what neural nets are doing with linear layers and activation functions. nn.Sequential() will do this for us... simple_net = nn . Sequential ( nn . Linear ( 28 * 28 , 30 ), # 28*28 in, 30 out nn . ReLU (), nn . Linear ( 30 , 1 ) # 30 in 1 out ) learn = Learner ( dls , simple_net , opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 40 , 0.1 ) epoch train_loss valid_loss batch_accuracy time 0 0.301185 0.414520 0.506379 00:00 1 0.142869 0.223012 0.814033 00:00 2 0.079959 0.114103 0.916094 00:00 3 0.053115 0.077652 0.939156 00:00 4 0.040578 0.060868 0.953876 00:00 5 0.034118 0.051373 0.963690 00:00 6 0.030368 0.045362 0.965653 00:00 7 0.027905 0.041246 0.965162 00:00 8 0.026117 0.038246 0.968597 00:00 9 0.024726 0.035950 0.969087 00:00 10 0.023596 0.034124 0.971050 00:00 11 0.022651 0.032629 0.972031 00:00 12 0.021847 0.031376 0.973503 00:00 13 0.021151 0.030301 0.974485 00:00 14 0.020542 0.029363 0.974485 00:00 15 0.020002 0.028535 0.975957 00:00 16 0.019519 0.027797 0.976448 00:00 17 0.019083 0.027134 0.976938 00:00 18 0.018687 0.026535 0.977920 00:00 19 0.018325 0.025991 0.978901 00:00 20 0.017992 0.025495 0.978901 00:00 21 0.017684 0.025040 0.978901 00:00 22 0.017398 0.024621 0.979392 00:00 23 0.017131 0.024233 0.979392 00:00 24 0.016881 0.023874 0.980373 00:00 25 0.016645 0.023541 0.980373 00:00 26 0.016424 0.023232 0.980373 00:00 27 0.016214 0.022943 0.980864 00:00 28 0.016015 0.022673 0.980864 00:00 29 0.015827 0.022421 0.981354 00:00 30 0.015648 0.022185 0.981845 00:00 31 0.015476 0.021963 0.982336 00:00 32 0.015313 0.021756 0.982336 00:00 33 0.015156 0.021561 0.982336 00:00 34 0.015006 0.021378 0.982826 00:00 35 0.014863 0.021204 0.982826 00:00 36 0.014725 0.021041 0.982336 00:00 37 0.014592 0.020886 0.982336 00:00 38 0.014464 0.020740 0.982336 00:00 39 0.014341 0.020601 0.982336 00:00 # this is what our model now looks like learn . model Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) # plot the loss learn . recorder . plot_loss () # learn.recorder.values hold the table values above # lets plot the accuracy plt . plot ( L ( learn . recorder . values ) . itemgot ( 2 ));","title":"Non-Linearity"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#looking-inside","text":"# let's visualise some of the parameters # 1. grab your model m = learn . model # (0): Linear(in_features=784, out_features=30, bias=True) # 2. look inside and grab the weights and biases w , b = m [ 0 ] . parameters () # 3. grab first (or any) row, reshape, and plot show_image ( w [ 0 ] . view ( 28 , 28 ), figsize = ( 4 , 4 )) <AxesSubplot:>","title":"Looking inside..."},{"location":"fastai%20deep%20learning%202020/lesson%2004/#fastai-in-full","text":"from fastai.vision.all import * from pathlib import Path path = Path . cwd () / 'datasets/fastai/mnist_sample' dls = ImageDataLoaders . from_folder ( path ) learn = cnn_learner ( dls , resnet18 , pretrained = False , loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 1 , 0.1 ) epoch train_loss valid_loss accuracy time 0 0.086805 0.025215 0.994603 00:11","title":"fastai in full"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#summary","text":"We have gone over creating and training a neural network from scratch using the simple example of a digit classifier. The key idea for the last few notebooks was to start with planning out the problem and identifying a way to solve it using a simple common sense solution - the pixel similarity model. This proved successful but it was not really robust beyond the straightforward example we chose - identifying 3s and 7s. We then implemented a more complex solution that could be applied to more complicated problems. After each step or concept had been implemented manually, we refactored the code to use convenient PyTorch functions and modules, eventually ending up with using fastai's implementation which abstracts away from all of the underlying heavy lifting. This is done for convenience and in my own opinion, to help lower the entry barrier into deep learning. Ultimately I believe it is fundamentally important to understand the concepts and implementation if your goal (and this is my goal) is to implement deep learning solutions to solve business problems within your industry.","title":"Summary"}]}