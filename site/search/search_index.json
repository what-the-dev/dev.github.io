{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Digital Analyst working at SEEK, specialising in Online and Tech. I work closely with product teams to identify opportunities to optimise product and provide value to candidates through data driven insights. I am a proactive, creative person with a passion for learning. I am constantly striving to improving my skill-set. I do this by immersing myself in new challenges. I am passionate about local music, python, data, the arts and travel. My spare time is currently spent educating myself in Deep Learning Skillset Python SQL Adobe Analytics Power BI Web Analytics Data Visualisation","title":"About"},{"location":"#about","text":"Digital Analyst working at SEEK, specialising in Online and Tech. I work closely with product teams to identify opportunities to optimise product and provide value to candidates through data driven insights. I am a proactive, creative person with a passion for learning. I am constantly striving to improving my skill-set. I do this by immersing myself in new challenges. I am passionate about local music, python, data, the arts and travel. My spare time is currently spent educating myself in Deep Learning","title":"About"},{"location":"#skillset","text":"Python SQL Adobe Analytics Power BI Web Analytics Data Visualisation","title":"Skillset"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); Lesson 1: Deep Learning for Coders \u00b6 This notebook will go over some of the practical material discussed in lesson 1 of the fastai 2020 course. Example 1: Computer Vision \u00b6 In [1]: from fastai.vision.all import * from pathlib import Path download one of the standard datasets provided by fasta, the Oxford-IIIT Pet Dataset which is a 37 category pet dataset with roughly 200 images for each class. In [2]: path = untar_data ( URLs . PETS ) / 'images' path Out[2]: Path('/storage/data/oxford-iiit-pet/images') Create an ImageDataLoader \u00b6 Fastai needs to know where to get the image labels from. Normally these labels are part of the filenames or folder structure. In this case the filenames contain the animal breeds. american_bulldog_146.jpg and Siamese_56.jpg for example it so happens that cat breeds start with an uppercase letter. For this example, we will not classify all 37 breeds. We will instead classify whether the images are of dogs or cats. First define a function is_cat that checks whether the first letter in the image label is uppercase. is_cat returns a boolean value that will be used as the new image label. from_name_func applies the function to our data to create the labels we need. valid_pct=0.2 : hold 20% of the data aside for the validation set, 80% will be used for the training set item_tfms=Resize(224) : resize images to 224x224 fastai provides item transforms (applied to each image in this case) and batch transform which are applied to a batch of items at a time. In [12]: # check a few image names to confirm that # dog images start with lowercase filenames # cat images start with uppercase filenames files = get_image_files ( path ) files [ 0 ], files [ 6 ] Out[12]: (Path('/storage/data/oxford-iiit-pet/images/american_bulldog_146.jpg'), Path('/storage/data/oxford-iiit-pet/images/Siamese_56.jpg')) In [3]: def is_cat ( x ): return x [ 0 ] . isupper () dls = ImageDataLoaders . from_name_func ( path , get_image_files ( path ), valid_pct = 0.2 , seed = 42 , label_func = is_cat , item_tfms = Resize ( 224 )) In [4]: # take a look at some of the data dls . show_batch ( max_n = 6 ) In [30]: # check number of items in training and test datasets len ( dls . train_ds ), len ( dls . valid_ds ) Out[30]: (5912, 1478) Create a cnn_learner \u00b6 using the resnet34 architecture resnet paper this is a pretrained learner, which means when we fit the model, we will not need to train from scratch, rather, we will only fine tune the model by default, freeze_epochs is set to 1 In [68]: learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) In [7]: learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.149828 0.034039 0.008796 00:45 epoch train_loss valid_loss error_rate time 0 0.078255 0.016130 0.006089 01:01 In [8]: learn . show_results () Testing the model \u00b6 Lets load in a picture of a cat and a dog to check the model In [41]: img_01 = Path . cwd () / 'img_1.PNG' img_02 = Path . cwd () / 'img_2.PNG' In [54]: im1 = PILImage . create ( img_01 ) im2 = PILImage . create ( img_02 ) im1 . to_thumb ( 192 ) Out[54]: In [55]: im2 . to_thumb ( 192 ) Out[55]: In [70]: images = [ im1 , im2 ] for i in images : is_cat , _ , probs = learn . predict ( i ) print ( f \"Is this a cat?: { is_cat } .\" ) print ( f \"Probability it's a cat: { probs [ 1 ] . item () : .5f } \" ) Is this a cat?: False. Probability it's a cat: 0.01819 Is this a cat?: True. Probability it's a cat: 0.88616 Example 2: Tabular \u00b6 For this example we will use the Adults data set. Our goal is to predict if a person is earning above or below $50k per year using information such as age, working class, education and occupation. There are about 32K rows in the dataset. In [1]: from fastai.tabular.all import * path = untar_data ( URLs . ADULT_SAMPLE ) path Out[1]: Path('/storage/data/adult_sample') In [2]: df = pd . read_csv ( path / 'adult.csv' ) df . head () Out[2]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 49 Private 101320 Assoc-acdm 12.0 Married-civ-spouse NaN Wife White Female 0 1902 40 United-States >=50k 1 44 Private 236746 Masters 14.0 Divorced Exec-managerial Not-in-family White Male 10520 0 45 United-States >=50k 2 38 Private 96185 HS-grad NaN Divorced NaN Unmarried Black Female 0 0 32 United-States <50k 3 38 Self-emp-inc 112847 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband Asian-Pac-Islander Male 0 0 40 United-States >=50k 4 42 Self-emp-not-inc 82297 7th-8th NaN Married-civ-spouse Other-service Wife Black Female 0 0 50 United-States <50k In [3]: len ( df ) Out[3]: 32561 Create an TabularDataLoader \u00b6 Again we create data loader using the path . We need to specify some information such as the y variable (the value we want to predict), and we also need to specify which columns contain categorical values and which contain continuous variables. Do this using cat_names and cont_names . Some data processing needs to occur.. we need to specify how to handle missing data. Info below from the docs FillMissing by default sets fill_strategy=median Normalize will normalize the continuous variables (substract the mean and divide by the std) Categorify transform the categorical variables to something similar to pd.Categorical This is another classification problem. Our goal is to predict whether a persons salary was below 50k (0) or above (1). In [4]: dls = TabularDataLoaders . from_csv ( path / 'adult.csv' , path = path , y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) I'm going to keep some of the data at the end of the set aside for testing. df[:32500] will select from row 0 to 32500, the remaining rows will not be seen by the model In [5]: splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df [: 32500 ])) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) In [6]: dls = to . dataloaders ( bs = 64 ) In [7]: dls . show_batch () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary 0 Private HS-grad Married-spouse-absent Other-service Unmarried White False 32.000000 128016.002920 9.0 <50k 1 Private 7th-8th Married-civ-spouse Exec-managerial Wife White False 52.000000 194259.000001 4.0 <50k 2 Private Some-college Widowed Exec-managerial Unmarried White False 31.000000 73796.004491 10.0 <50k 3 Private Some-college Separated Other-service Not-in-family White False 64.000001 114993.998143 10.0 <50k 4 Self-emp-not-inc Assoc-voc Married-civ-spouse Prof-specialty Husband White False 68.000000 116902.996854 11.0 <50k 5 Private Bachelors Married-civ-spouse Prof-specialty Husband White False 42.000000 190178.999991 13.0 >=50k 6 Self-emp-not-inc Prof-school Married-civ-spouse Prof-specialty Husband White False 66.000000 291362.001320 15.0 <50k 7 Self-emp-not-inc Bachelors Married-civ-spouse Sales Husband White False 63.000001 298249.000475 13.0 >=50k 8 Private Masters Divorced Tech-support Not-in-family White False 47.000000 606752.001736 14.0 <50k 9 State-gov Bachelors Married-civ-spouse Exec-managerial Husband White False 42.000000 345969.005416 13.0 >=50k We can see that our y values have been turned into the categories 0 and 1. In [8]: dls . y . value_counts () Out[8]: 0 19756 1 6244 Name: salary, dtype: int64 In [9]: learn = tabular_learner ( dls , metrics = accuracy ) In [10]: learn . fit_one_cycle ( 3 ) epoch train_loss valid_loss accuracy time 0 0.366288 0.354235 0.834769 00:06 1 0.367247 0.348617 0.839538 00:05 2 0.358275 0.345206 0.839077 00:06 In [11]: learn . show_results () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary salary_pred 0 5.0 11.0 3.0 11.0 1.0 5.0 1.0 1.494630 1.838917 2.322299 0.0 1.0 1 5.0 12.0 3.0 8.0 1.0 5.0 1.0 -0.558852 -0.690051 -0.421488 0.0 0.0 2 3.0 10.0 3.0 11.0 6.0 3.0 1.0 0.174535 0.000144 1.146390 1.0 1.0 3 5.0 10.0 3.0 5.0 1.0 5.0 1.0 0.467889 -1.014015 1.146390 1.0 1.0 4 5.0 16.0 5.0 9.0 4.0 5.0 1.0 -1.365576 4.387854 -0.029518 0.0 0.0 5 5.0 10.0 1.0 5.0 2.0 5.0 1.0 0.174535 0.616141 1.146390 0.0 0.0 6 5.0 10.0 3.0 2.0 6.0 5.0 1.0 1.494630 0.898075 1.146390 0.0 1.0 7 5.0 12.0 3.0 5.0 6.0 5.0 1.0 0.101196 -0.713219 -0.421488 1.0 1.0 8 7.0 2.0 3.0 4.0 1.0 5.0 1.0 -0.338836 0.932638 -1.205427 0.0 0.0 Check the model by making predictions on the dataset \u00b6 using the data that was held aside which the model has not yet seen. In [22]: # pick some random rows of the df sample_df = df . iloc [[ 32513 , 32542 , 32553 ]] sample_df Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 32513 23 Private 209955 HS-grad 9.0 Never-married Craft-repair Not-in-family White Male 0 0 40 United-States <50k 32542 34 Private 98283 Prof-school 15.0 Never-married Tech-support Not-in-family Asian-Pac-Islander Male 0 1564 40 India >=50k 32553 35 Self-emp-inc 135436 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States >=50k Lets loop through these rows and make predictions, printing out the predicted class, the probabilities and the actual class. In [34]: for i , r in sample_df . iterrows (): row , clas , probs = learn . predict ( r ) print ( f 'the predicted class is { clas } ' ) print ( f 'with a probability of { probs } ' ) print ( f 'the actual class was { r . salary } ' ) the predicted class is 0 with a probability of tensor([0.9911, 0.0089]) the actual class was <50k the predicted class is 0 with a probability of tensor([0.6258, 0.3742]) the actual class was >=50k the predicted class is 1 with a probability of tensor([0.0919, 0.9081]) the actual class was >=50k","title":"20200826 fastai lesson 1"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#lesson-1-deep-learning-for-coders","text":"This notebook will go over some of the practical material discussed in lesson 1 of the fastai 2020 course.","title":"Lesson 1: Deep Learning for Coders"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#example-1-computer-vision","text":"In [1]: from fastai.vision.all import * from pathlib import Path download one of the standard datasets provided by fasta, the Oxford-IIIT Pet Dataset which is a 37 category pet dataset with roughly 200 images for each class. In [2]: path = untar_data ( URLs . PETS ) / 'images' path Out[2]: Path('/storage/data/oxford-iiit-pet/images')","title":"Example 1: Computer Vision"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#create-an-imagedataloader","text":"Fastai needs to know where to get the image labels from. Normally these labels are part of the filenames or folder structure. In this case the filenames contain the animal breeds. american_bulldog_146.jpg and Siamese_56.jpg for example it so happens that cat breeds start with an uppercase letter. For this example, we will not classify all 37 breeds. We will instead classify whether the images are of dogs or cats. First define a function is_cat that checks whether the first letter in the image label is uppercase. is_cat returns a boolean value that will be used as the new image label. from_name_func applies the function to our data to create the labels we need. valid_pct=0.2 : hold 20% of the data aside for the validation set, 80% will be used for the training set item_tfms=Resize(224) : resize images to 224x224 fastai provides item transforms (applied to each image in this case) and batch transform which are applied to a batch of items at a time. In [12]: # check a few image names to confirm that # dog images start with lowercase filenames # cat images start with uppercase filenames files = get_image_files ( path ) files [ 0 ], files [ 6 ] Out[12]: (Path('/storage/data/oxford-iiit-pet/images/american_bulldog_146.jpg'), Path('/storage/data/oxford-iiit-pet/images/Siamese_56.jpg')) In [3]: def is_cat ( x ): return x [ 0 ] . isupper () dls = ImageDataLoaders . from_name_func ( path , get_image_files ( path ), valid_pct = 0.2 , seed = 42 , label_func = is_cat , item_tfms = Resize ( 224 )) In [4]: # take a look at some of the data dls . show_batch ( max_n = 6 ) In [30]: # check number of items in training and test datasets len ( dls . train_ds ), len ( dls . valid_ds ) Out[30]: (5912, 1478)","title":"Create an ImageDataLoader"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#create-a-cnn_learner","text":"using the resnet34 architecture resnet paper this is a pretrained learner, which means when we fit the model, we will not need to train from scratch, rather, we will only fine tune the model by default, freeze_epochs is set to 1 In [68]: learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) In [7]: learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.149828 0.034039 0.008796 00:45 epoch train_loss valid_loss error_rate time 0 0.078255 0.016130 0.006089 01:01 In [8]: learn . show_results ()","title":"Create a cnn_learner"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#testing-the-model","text":"Lets load in a picture of a cat and a dog to check the model In [41]: img_01 = Path . cwd () / 'img_1.PNG' img_02 = Path . cwd () / 'img_2.PNG' In [54]: im1 = PILImage . create ( img_01 ) im2 = PILImage . create ( img_02 ) im1 . to_thumb ( 192 ) Out[54]: In [55]: im2 . to_thumb ( 192 ) Out[55]: In [70]: images = [ im1 , im2 ] for i in images : is_cat , _ , probs = learn . predict ( i ) print ( f \"Is this a cat?: { is_cat } .\" ) print ( f \"Probability it's a cat: { probs [ 1 ] . item () : .5f } \" ) Is this a cat?: False. Probability it's a cat: 0.01819 Is this a cat?: True. Probability it's a cat: 0.88616","title":"Testing the model"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#example-2-tabular","text":"For this example we will use the Adults data set. Our goal is to predict if a person is earning above or below $50k per year using information such as age, working class, education and occupation. There are about 32K rows in the dataset. In [1]: from fastai.tabular.all import * path = untar_data ( URLs . ADULT_SAMPLE ) path Out[1]: Path('/storage/data/adult_sample') In [2]: df = pd . read_csv ( path / 'adult.csv' ) df . head () Out[2]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 49 Private 101320 Assoc-acdm 12.0 Married-civ-spouse NaN Wife White Female 0 1902 40 United-States >=50k 1 44 Private 236746 Masters 14.0 Divorced Exec-managerial Not-in-family White Male 10520 0 45 United-States >=50k 2 38 Private 96185 HS-grad NaN Divorced NaN Unmarried Black Female 0 0 32 United-States <50k 3 38 Self-emp-inc 112847 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband Asian-Pac-Islander Male 0 0 40 United-States >=50k 4 42 Self-emp-not-inc 82297 7th-8th NaN Married-civ-spouse Other-service Wife Black Female 0 0 50 United-States <50k In [3]: len ( df ) Out[3]: 32561","title":"Example 2: Tabular"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#create-an-tabulardataloader","text":"Again we create data loader using the path . We need to specify some information such as the y variable (the value we want to predict), and we also need to specify which columns contain categorical values and which contain continuous variables. Do this using cat_names and cont_names . Some data processing needs to occur.. we need to specify how to handle missing data. Info below from the docs FillMissing by default sets fill_strategy=median Normalize will normalize the continuous variables (substract the mean and divide by the std) Categorify transform the categorical variables to something similar to pd.Categorical This is another classification problem. Our goal is to predict whether a persons salary was below 50k (0) or above (1). In [4]: dls = TabularDataLoaders . from_csv ( path / 'adult.csv' , path = path , y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) I'm going to keep some of the data at the end of the set aside for testing. df[:32500] will select from row 0 to 32500, the remaining rows will not be seen by the model In [5]: splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df [: 32500 ])) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) In [6]: dls = to . dataloaders ( bs = 64 ) In [7]: dls . show_batch () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary 0 Private HS-grad Married-spouse-absent Other-service Unmarried White False 32.000000 128016.002920 9.0 <50k 1 Private 7th-8th Married-civ-spouse Exec-managerial Wife White False 52.000000 194259.000001 4.0 <50k 2 Private Some-college Widowed Exec-managerial Unmarried White False 31.000000 73796.004491 10.0 <50k 3 Private Some-college Separated Other-service Not-in-family White False 64.000001 114993.998143 10.0 <50k 4 Self-emp-not-inc Assoc-voc Married-civ-spouse Prof-specialty Husband White False 68.000000 116902.996854 11.0 <50k 5 Private Bachelors Married-civ-spouse Prof-specialty Husband White False 42.000000 190178.999991 13.0 >=50k 6 Self-emp-not-inc Prof-school Married-civ-spouse Prof-specialty Husband White False 66.000000 291362.001320 15.0 <50k 7 Self-emp-not-inc Bachelors Married-civ-spouse Sales Husband White False 63.000001 298249.000475 13.0 >=50k 8 Private Masters Divorced Tech-support Not-in-family White False 47.000000 606752.001736 14.0 <50k 9 State-gov Bachelors Married-civ-spouse Exec-managerial Husband White False 42.000000 345969.005416 13.0 >=50k We can see that our y values have been turned into the categories 0 and 1. In [8]: dls . y . value_counts () Out[8]: 0 19756 1 6244 Name: salary, dtype: int64 In [9]: learn = tabular_learner ( dls , metrics = accuracy ) In [10]: learn . fit_one_cycle ( 3 ) epoch train_loss valid_loss accuracy time 0 0.366288 0.354235 0.834769 00:06 1 0.367247 0.348617 0.839538 00:05 2 0.358275 0.345206 0.839077 00:06 In [11]: learn . show_results () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary salary_pred 0 5.0 11.0 3.0 11.0 1.0 5.0 1.0 1.494630 1.838917 2.322299 0.0 1.0 1 5.0 12.0 3.0 8.0 1.0 5.0 1.0 -0.558852 -0.690051 -0.421488 0.0 0.0 2 3.0 10.0 3.0 11.0 6.0 3.0 1.0 0.174535 0.000144 1.146390 1.0 1.0 3 5.0 10.0 3.0 5.0 1.0 5.0 1.0 0.467889 -1.014015 1.146390 1.0 1.0 4 5.0 16.0 5.0 9.0 4.0 5.0 1.0 -1.365576 4.387854 -0.029518 0.0 0.0 5 5.0 10.0 1.0 5.0 2.0 5.0 1.0 0.174535 0.616141 1.146390 0.0 0.0 6 5.0 10.0 3.0 2.0 6.0 5.0 1.0 1.494630 0.898075 1.146390 0.0 1.0 7 5.0 12.0 3.0 5.0 6.0 5.0 1.0 0.101196 -0.713219 -0.421488 1.0 1.0 8 7.0 2.0 3.0 4.0 1.0 5.0 1.0 -0.338836 0.932638 -1.205427 0.0 0.0","title":"Create an TabularDataLoader"},{"location":"fastai%202020%20course/20200826_fastai_lesson_1/#check-the-model-by-making-predictions-on-the-dataset","text":"using the data that was held aside which the model has not yet seen. In [22]: # pick some random rows of the df sample_df = df . iloc [[ 32513 , 32542 , 32553 ]] sample_df Out[22]: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 32513 23 Private 209955 HS-grad 9.0 Never-married Craft-repair Not-in-family White Male 0 0 40 United-States <50k 32542 34 Private 98283 Prof-school 15.0 Never-married Tech-support Not-in-family Asian-Pac-Islander Male 0 1564 40 India >=50k 32553 35 Self-emp-inc 135436 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States >=50k Lets loop through these rows and make predictions, printing out the predicted class, the probabilities and the actual class. In [34]: for i , r in sample_df . iterrows (): row , clas , probs = learn . predict ( r ) print ( f 'the predicted class is { clas } ' ) print ( f 'with a probability of { probs } ' ) print ( f 'the actual class was { r . salary } ' ) the predicted class is 0 with a probability of tensor([0.9911, 0.0089]) the actual class was <50k the predicted class is 0 with a probability of tensor([0.6258, 0.3742]) the actual class was >=50k the predicted class is 1 with a probability of tensor([0.0919, 0.9081]) the actual class was >=50k","title":"Check the model by making predictions on the dataset"},{"location":"notebooks/01_study_notes/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); fastai 2019: notes \u00b6 Data \u00b6 in fastai, the following applies (this is mainly how kaggle does it too) train training data with labels valid a validation set for assessing and testing. Valid DS has labels test a test data set which has NO labels Gradient Descent \u00b6 will take our prediction and try to make it better using the intercept and slope for each \"movement\", calculate the loss, if it is better we keep that we do this by calculating the derivative (ie calculating the gradient) Gradient Descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves towards a set of parameter values that minimies the function. This is done by taking steps in the negative direction of the function gradient Weight Decay \u00b6 is a type of regulariztion we throw away a certain % of weights for each layer of the model to stop it from learning the data it is training on. Weight decay is where we take the loss function and add the sum of squared of the parameters x by some number (wd) wd should be 0.1 according to J Howard. You should try using this when you train $w_t = w_{t-1} - lr \\times {{dl}\\over{dw}_{t-1}}$ update $w$ weight at epoch/time t This means, our $w_{t-1}$ weights at previous time minus learning rate $lr$ times the derivative of our $lr$ with respect to derivative of our weights $w$ Momentum \u00b6 0.9 is really common momentum is what affects our step size when we are exploring the weight space it is the exponentially weighted moving average of the gradient RMS Prop \u00b6 similar to momentum but is.. exponentially weighted moving average of the gradient squared Adam \u00b6 does both momentumn and weight decay keep track of exponentially weighted moving average of the gradient squared exponentially weighted moving average of my steps both divide by exponentially weighted moving average of the squared terms take .9 of a step in the same direct as last time Image regression \u00b6 different to classification in classification problems, we predict a discrete variable (ie category) in regression, we are trying to predict continuous variables look at the 'heads' data set lesson 3 it is predicting a set of coordinates on an image Loss \u00b6 is some function of our independent variables and our weights $L(x,w) = mse(\\hat{y}, y)$ we used MSE for eg between predictions y_hat and actuals y predictions come from running a model on those predictions, and the model contains some weights this creates a.graf then we add weight decay $L(x,w) = mse(\\hat{y}, y) + wd \\times \\sum{w}^2$ cross entropy loss \u00b6 this is the loss function you want when doing single label, multi-class classificaiton for classification, we need a loss function where predicting the right thing confidently should have low loss predicting the wrong thing confidently should have high loss sum of one hot encoded variables, times, all of your activations requires softmax which says e to the activations, div sum of e to the activations it requires that all activations sum to 1 all activations are > 0 all activations are less than 1 Loss functions to use \u00b6 Classification cross entropy loss Regression MSE data augmentation \u00b6 called by get_transforms in fastai particularly important to image data as the transforms are things like brightness, flipping, skewing/perspective warping, padding (zeros, border, reflection) \"reflection is nearly always better\" J Howard there are transformations that your preprocessing will add to you data. the purpose is \"teach\" the model that a cat is still a cat even if the image is too bright/dark/blury you should assess that data to see whether the data requires augmentation data augmentation creates more versions of each individual image, thereby increasing the size of your dataset, providing you with more training data everytime you grab something, fastai randomly transforms it so potentially every image will look a little bit different. You can see this by plotting something a few times (check 2019 lesson 7 video around 10:30) Convolutional Neural Networks \u00b6 like the neural nets we have seen before, so doing matrix multiplication but a slightly different type of matrix multiplication Convolutional Kernal \u00b6 explained well here as the kernal passes over the image, the resulting mat mul and addition is creating a negative image all a convolution can do is find edges and gradients each layer takes the results of the previous to create more complex shapes (see Zyler and Fergus visualizing layers of nets) each output is the result of a linear equation convolutions can be implemented with matrix multiplication but we generally don't do it because it is slow rank three tensor and kernals \u00b6 think of a cube think of a colour image as having 3 channels (R,G,B) - rank 3 tensor the kernal now becomes a rank 3 kernal (3x3x3 kernal) we now do an element wise mult of 27 things instead of 9 we then add all 27 together to end up with one number there need to be 3 kernels to create rank 3 tensor as an output however we get to choose how ever many kernels we need ofter 16 in the first layer these will create 16 channels representing how much left edge, top, edge, gradient, blue etc etc this is repeated many times we want to have more and more channels as we go deeper into the network this creates memory issues to avoid this we use a kernel that skips over pixels called STRIDE 2 convolution weight tying \u00b6 when you have multiple things with the same weight it's called weight tying kernel size in model \u00b6 generally speaking we start with a larger kernel first which then reduces stride size will reduce the image size ie image of 224 will become 112x112 with a stride 2 conv how to get to final output \u00b6 for every channel in the final output, we take an average, which will give us an vector of x length then we pop through a single matmul of vector of size x by the number of categories this is called average pooling Simple CNN \u00b6 sequential layers conv2d ResNet \u00b6 adds skip onnectionsd to sequential architecture DenseNet \u00b6 like resnet but instead of + x it concatenates see lesson 7. not too clear on this yet dens blocks get bigger and bigger but the original layer features are still there these nets are very memory intensive because of this though they do have fewer parameters they work really well for small data sets and for segmentation maybe for generation too?? U-Net \u00b6 - Tabular Data \u00b6 what architecture is this? It's not a CNN so maybe RNN or a linear model?? you need to specify you categorical and continuous variables if this is a regression problem, ie you dependent variable is continuous then you need to... this is discussed somewhere add notes here Normalization takes continuous variables, subtracts their mean and div by standard deviation (converts to 0, 1) whatever you do to training, you have to do to validation set (re: pre-processing) layers=[200,100] this is the embedding size of the last two layers?? Time Series Tabular Data generally don't use RNN for time series tabular add additional categorical variables dor date columns Collaborative filtering \u00b6 linear model it's basically a regression which means we only have one layer so no point with discriminative learning rates so for fit, just pass in one lr n_factors=50 this is the width of the embedding size factors is what they call the term in this collab filtering domain. min_score & max_score the min and max of the 'ratings' replaced by y_range these are the bounds where the sigmoid function will truncate we need to go a bit above the max \"rating\" number so that the actual number can be reached. ie if you have ratings from 1-5, you would pick a y_range=[0, 5.5] this is a way of improving the network by limiting the range. We want it to be as good as possible at predicting scores between 0-5 so no use allowing any numbers above ~5. embedding matrix these are vectors with a baised term added on the biased term is like the score for all \"movies/product\" the biased term is a way to say some products/movies are better than others so it's not surprising that they are liked more. an embedding means, look something up in an array this is the same as doing a matrix product by a one hot encoded matrix embedding is a memory efficient way of doing the multiplicaiton latent factors or features these are the hidden features that are revealed through training our model the bias term is a weight that basically give better items more weight, worse items less weight Classification \u00b6 loss functions, regularization and activations \u00b6 Reading \u00b6 neuralnetworks and deep learning chroma matrix products be familiar with the output of a matrix of size x * size y = size ? training notes \u00b6 lr of 3e-3 generally works well for the first round of training before unfreezing then for second round, for the first part of the slice use 10x lower for second part of slice, then whatever lr_finder found for the first part of the slice learn.fit_one_cycle(4, 3e-3) learn.unfreeze() learn.fit_one_cycle(4, slice(\"lr_finder number\",3e-4)) learn.recorder.plot_losses() will show you the loss plotted out. You want to see something that goes down, then increases a bit then goes down again. That is a good sign if it is ALWAYS going down, then you can bump your learning rate up a bit. if you are overfitting add more wd Learner \u00b6 we can pass in data , model , metrics , loss function it is a convenience function for us fit_one_cycle \u00b6 we use (in fastai) something like Adam by default fit one cycle implements discriminative learning rate and learning rate annealing increase the lr if you are doing well, then decrease after half way start slow when exploring the weight space, then increase towards the end as lr increases, momentum decreases, then towards the end, lr decreases, momentum increases over and underfitting \u00b6 training loss should always be lower than validation loss lr too high validation loss will be very high lower the lr lr too low error_rate will reduce but very very slowly increase lr a bit training loss will be higher than validation loss you never want this this means you are underftting num epochs too low or lr too low see 48:50 in lesson 2 video too few epochs - this looks dimilar to low `lr` - so try more epochs first - then if `lr` goes over the top, lower it too many epochs overfitting it is really hard to overfit how to tell error rate improves for a while, then gets worse again Terms \u00b6 Learning rate is the thing we mult gradient by to decide how much we update weights by Epoch one complete run through all data points Minibatch random bunch of points to update weights SGD stochastic gradient descent Model/Architecture function we are fitting the parameters to Parameters / coefficients / weights the numbers we are updating Afine function linear function multiply things together then add them up Loss Function how far away or how close you are to the correct answer ReLU is a \"filter\" where any number below 0 is cut off and set to 0 Activations numbers are the result of either a matrix multiply or an activation function such as(ReLU) sometimes called nonlinearities Parameters/Weights numbers inside the weights that we multipy that are stored to make a calculation this is what the model learns we use gradient descent on the parameters to update them parameters -= lr * parameters.grad Layers everything in the network that does a calculation every layer results in a set of activations Start layer input layer End layer output (final set of activations) Back Propagation the process of updating the parameters with gradient descent Fine Tuning Resnet34 was trained on imagenet so the final weight matrix is of len 1000 because you need to predict 1000 categories. We generally don't need to do that so that final set of weights is thrown away and replaced by 2 new weight matrices with a ReLU in between. these originally have random numbers in them this is what we train first while the start layers are frozen this ensures we don't back propagate the weights back into the initial layers this must be why when you unfreeze then re-train, you get worse before you get better. Makes sense! fastai by default splits the model into different sections and applied different learning rates to each part. this is because we don't need to train the early layers by much. So those weights won't be trained a lot. this is called using discriminative learning rates see Leslie Smith after unfreezing you can call fit(epochs=1, max_lr=1e-3) single lr throughout fit(epochs=1, max_lr=slice(1e-3)) evenly split lr between layers based on divisions of 3 (ie 1e-3/3) fit(epochs=1, max_lr=slice(1e-5,1e-3)) will apply 1e-5 to start group, then 1e-4 for middle group then 1e-3 for last layer group fit_one_cycle epoch and cyc_len are the same both represent the number of times you scan through your items Project notes \u00b6 Multilabel Classificaiton with Audio data \u00b6 What labels can you predict? using a spectrogram, you could classify key , scale , 'instrument , tambre` etc using vaoice it could be the tone of the voice hooty , squeezed , breathy , chest , head , etc Metrics use accuracy_thresh with a selected threshold check the video you need to update the accuracy metric Creating data \u00b6 you might want to use the actual matrices instead of images pytorch has a TensorDataset() function that will converts any 2 tensor into a dataset. You can then use DataBunch.create() to create a databunch iterator lesson 5 at 1:27 shows this Reading \u00b6 links \u00b6 musical freqs ISMIR librosa creators MIR THIS IS USEFUL Pitch Detection ### Papers Detecting Musical Key With Supervised Learning Deep residual learning for image recognition ResBlocks and resnets Visualising the loss landscape of neural nets In [ ]:","title":"01 study notes"},{"location":"notebooks/01_study_notes/#fastai-2019-notes","text":"","title":"fastai 2019: notes"},{"location":"notebooks/01_study_notes/#data","text":"in fastai, the following applies (this is mainly how kaggle does it too) train training data with labels valid a validation set for assessing and testing. Valid DS has labels test a test data set which has NO labels","title":"Data"},{"location":"notebooks/01_study_notes/#gradient-descent","text":"will take our prediction and try to make it better using the intercept and slope for each \"movement\", calculate the loss, if it is better we keep that we do this by calculating the derivative (ie calculating the gradient) Gradient Descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves towards a set of parameter values that minimies the function. This is done by taking steps in the negative direction of the function gradient","title":"Gradient Descent"},{"location":"notebooks/01_study_notes/#weight-decay","text":"is a type of regulariztion we throw away a certain % of weights for each layer of the model to stop it from learning the data it is training on. Weight decay is where we take the loss function and add the sum of squared of the parameters x by some number (wd) wd should be 0.1 according to J Howard. You should try using this when you train $w_t = w_{t-1} - lr \\times {{dl}\\over{dw}_{t-1}}$ update $w$ weight at epoch/time t This means, our $w_{t-1}$ weights at previous time minus learning rate $lr$ times the derivative of our $lr$ with respect to derivative of our weights $w$","title":"Weight Decay"},{"location":"notebooks/01_study_notes/#momentum","text":"0.9 is really common momentum is what affects our step size when we are exploring the weight space it is the exponentially weighted moving average of the gradient","title":"Momentum"},{"location":"notebooks/01_study_notes/#rms-prop","text":"similar to momentum but is.. exponentially weighted moving average of the gradient squared","title":"RMS Prop"},{"location":"notebooks/01_study_notes/#adam","text":"does both momentumn and weight decay keep track of exponentially weighted moving average of the gradient squared exponentially weighted moving average of my steps both divide by exponentially weighted moving average of the squared terms take .9 of a step in the same direct as last time","title":"Adam"},{"location":"notebooks/01_study_notes/#image-regression","text":"different to classification in classification problems, we predict a discrete variable (ie category) in regression, we are trying to predict continuous variables look at the 'heads' data set lesson 3 it is predicting a set of coordinates on an image","title":"Image regression"},{"location":"notebooks/01_study_notes/#loss","text":"is some function of our independent variables and our weights $L(x,w) = mse(\\hat{y}, y)$ we used MSE for eg between predictions y_hat and actuals y predictions come from running a model on those predictions, and the model contains some weights this creates a.graf then we add weight decay $L(x,w) = mse(\\hat{y}, y) + wd \\times \\sum{w}^2$","title":"Loss"},{"location":"notebooks/01_study_notes/#cross-entropy-loss","text":"this is the loss function you want when doing single label, multi-class classificaiton for classification, we need a loss function where predicting the right thing confidently should have low loss predicting the wrong thing confidently should have high loss sum of one hot encoded variables, times, all of your activations requires softmax which says e to the activations, div sum of e to the activations it requires that all activations sum to 1 all activations are > 0 all activations are less than 1","title":"cross entropy loss"},{"location":"notebooks/01_study_notes/#loss-functions-to-use","text":"Classification cross entropy loss Regression MSE","title":"Loss functions to use"},{"location":"notebooks/01_study_notes/#data-augmentation","text":"called by get_transforms in fastai particularly important to image data as the transforms are things like brightness, flipping, skewing/perspective warping, padding (zeros, border, reflection) \"reflection is nearly always better\" J Howard there are transformations that your preprocessing will add to you data. the purpose is \"teach\" the model that a cat is still a cat even if the image is too bright/dark/blury you should assess that data to see whether the data requires augmentation data augmentation creates more versions of each individual image, thereby increasing the size of your dataset, providing you with more training data everytime you grab something, fastai randomly transforms it so potentially every image will look a little bit different. You can see this by plotting something a few times (check 2019 lesson 7 video around 10:30)","title":"data augmentation"},{"location":"notebooks/01_study_notes/#convolutional-neural-networks","text":"like the neural nets we have seen before, so doing matrix multiplication but a slightly different type of matrix multiplication","title":"Convolutional Neural Networks"},{"location":"notebooks/01_study_notes/#convolutional-kernal","text":"explained well here as the kernal passes over the image, the resulting mat mul and addition is creating a negative image all a convolution can do is find edges and gradients each layer takes the results of the previous to create more complex shapes (see Zyler and Fergus visualizing layers of nets) each output is the result of a linear equation convolutions can be implemented with matrix multiplication but we generally don't do it because it is slow","title":"Convolutional Kernal"},{"location":"notebooks/01_study_notes/#rank-three-tensor-and-kernals","text":"think of a cube think of a colour image as having 3 channels (R,G,B) - rank 3 tensor the kernal now becomes a rank 3 kernal (3x3x3 kernal) we now do an element wise mult of 27 things instead of 9 we then add all 27 together to end up with one number there need to be 3 kernels to create rank 3 tensor as an output however we get to choose how ever many kernels we need ofter 16 in the first layer these will create 16 channels representing how much left edge, top, edge, gradient, blue etc etc this is repeated many times we want to have more and more channels as we go deeper into the network this creates memory issues to avoid this we use a kernel that skips over pixels called STRIDE 2 convolution","title":"rank three tensor and kernals"},{"location":"notebooks/01_study_notes/#weight-tying","text":"when you have multiple things with the same weight it's called weight tying","title":"weight tying"},{"location":"notebooks/01_study_notes/#kernel-size-in-model","text":"generally speaking we start with a larger kernel first which then reduces stride size will reduce the image size ie image of 224 will become 112x112 with a stride 2 conv","title":"kernel size in model"},{"location":"notebooks/01_study_notes/#how-to-get-to-final-output","text":"for every channel in the final output, we take an average, which will give us an vector of x length then we pop through a single matmul of vector of size x by the number of categories this is called average pooling","title":"how to get to final output"},{"location":"notebooks/01_study_notes/#simple-cnn","text":"sequential layers conv2d","title":"Simple CNN"},{"location":"notebooks/01_study_notes/#resnet","text":"adds skip onnectionsd to sequential architecture","title":"ResNet"},{"location":"notebooks/01_study_notes/#densenet","text":"like resnet but instead of + x it concatenates see lesson 7. not too clear on this yet dens blocks get bigger and bigger but the original layer features are still there these nets are very memory intensive because of this though they do have fewer parameters they work really well for small data sets and for segmentation maybe for generation too??","title":"DenseNet"},{"location":"notebooks/01_study_notes/#u-net","text":"-","title":"U-Net"},{"location":"notebooks/01_study_notes/#tabular-data","text":"what architecture is this? It's not a CNN so maybe RNN or a linear model?? you need to specify you categorical and continuous variables if this is a regression problem, ie you dependent variable is continuous then you need to... this is discussed somewhere add notes here Normalization takes continuous variables, subtracts their mean and div by standard deviation (converts to 0, 1) whatever you do to training, you have to do to validation set (re: pre-processing) layers=[200,100] this is the embedding size of the last two layers?? Time Series Tabular Data generally don't use RNN for time series tabular add additional categorical variables dor date columns","title":"Tabular Data"},{"location":"notebooks/01_study_notes/#collaborative-filtering","text":"linear model it's basically a regression which means we only have one layer so no point with discriminative learning rates so for fit, just pass in one lr n_factors=50 this is the width of the embedding size factors is what they call the term in this collab filtering domain. min_score & max_score the min and max of the 'ratings' replaced by y_range these are the bounds where the sigmoid function will truncate we need to go a bit above the max \"rating\" number so that the actual number can be reached. ie if you have ratings from 1-5, you would pick a y_range=[0, 5.5] this is a way of improving the network by limiting the range. We want it to be as good as possible at predicting scores between 0-5 so no use allowing any numbers above ~5. embedding matrix these are vectors with a baised term added on the biased term is like the score for all \"movies/product\" the biased term is a way to say some products/movies are better than others so it's not surprising that they are liked more. an embedding means, look something up in an array this is the same as doing a matrix product by a one hot encoded matrix embedding is a memory efficient way of doing the multiplicaiton latent factors or features these are the hidden features that are revealed through training our model the bias term is a weight that basically give better items more weight, worse items less weight","title":"Collaborative filtering"},{"location":"notebooks/01_study_notes/#classification","text":"","title":"Classification"},{"location":"notebooks/01_study_notes/#loss-functions-regularization-and-activations","text":"","title":"loss functions, regularization and activations"},{"location":"notebooks/01_study_notes/#reading","text":"neuralnetworks and deep learning chroma matrix products be familiar with the output of a matrix of size x * size y = size ?","title":"Reading"},{"location":"notebooks/01_study_notes/#training-notes","text":"lr of 3e-3 generally works well for the first round of training before unfreezing then for second round, for the first part of the slice use 10x lower for second part of slice, then whatever lr_finder found for the first part of the slice learn.fit_one_cycle(4, 3e-3) learn.unfreeze() learn.fit_one_cycle(4, slice(\"lr_finder number\",3e-4)) learn.recorder.plot_losses() will show you the loss plotted out. You want to see something that goes down, then increases a bit then goes down again. That is a good sign if it is ALWAYS going down, then you can bump your learning rate up a bit. if you are overfitting add more wd","title":"training notes"},{"location":"notebooks/01_study_notes/#learner","text":"we can pass in data , model , metrics , loss function it is a convenience function for us","title":"Learner"},{"location":"notebooks/01_study_notes/#fit_one_cycle","text":"we use (in fastai) something like Adam by default fit one cycle implements discriminative learning rate and learning rate annealing increase the lr if you are doing well, then decrease after half way start slow when exploring the weight space, then increase towards the end as lr increases, momentum decreases, then towards the end, lr decreases, momentum increases","title":"fit_one_cycle"},{"location":"notebooks/01_study_notes/#over-and-underfitting","text":"training loss should always be lower than validation loss lr too high validation loss will be very high lower the lr lr too low error_rate will reduce but very very slowly increase lr a bit training loss will be higher than validation loss you never want this this means you are underftting num epochs too low or lr too low see 48:50 in lesson 2 video too few epochs - this looks dimilar to low `lr` - so try more epochs first - then if `lr` goes over the top, lower it too many epochs overfitting it is really hard to overfit how to tell error rate improves for a while, then gets worse again","title":"over and underfitting"},{"location":"notebooks/01_study_notes/#terms","text":"Learning rate is the thing we mult gradient by to decide how much we update weights by Epoch one complete run through all data points Minibatch random bunch of points to update weights SGD stochastic gradient descent Model/Architecture function we are fitting the parameters to Parameters / coefficients / weights the numbers we are updating Afine function linear function multiply things together then add them up Loss Function how far away or how close you are to the correct answer ReLU is a \"filter\" where any number below 0 is cut off and set to 0 Activations numbers are the result of either a matrix multiply or an activation function such as(ReLU) sometimes called nonlinearities Parameters/Weights numbers inside the weights that we multipy that are stored to make a calculation this is what the model learns we use gradient descent on the parameters to update them parameters -= lr * parameters.grad Layers everything in the network that does a calculation every layer results in a set of activations Start layer input layer End layer output (final set of activations) Back Propagation the process of updating the parameters with gradient descent Fine Tuning Resnet34 was trained on imagenet so the final weight matrix is of len 1000 because you need to predict 1000 categories. We generally don't need to do that so that final set of weights is thrown away and replaced by 2 new weight matrices with a ReLU in between. these originally have random numbers in them this is what we train first while the start layers are frozen this ensures we don't back propagate the weights back into the initial layers this must be why when you unfreeze then re-train, you get worse before you get better. Makes sense! fastai by default splits the model into different sections and applied different learning rates to each part. this is because we don't need to train the early layers by much. So those weights won't be trained a lot. this is called using discriminative learning rates see Leslie Smith after unfreezing you can call fit(epochs=1, max_lr=1e-3) single lr throughout fit(epochs=1, max_lr=slice(1e-3)) evenly split lr between layers based on divisions of 3 (ie 1e-3/3) fit(epochs=1, max_lr=slice(1e-5,1e-3)) will apply 1e-5 to start group, then 1e-4 for middle group then 1e-3 for last layer group fit_one_cycle epoch and cyc_len are the same both represent the number of times you scan through your items","title":"Terms"},{"location":"notebooks/01_study_notes/#project-notes","text":"","title":"Project notes"},{"location":"notebooks/01_study_notes/#multilabel-classificaiton-with-audio-data","text":"What labels can you predict? using a spectrogram, you could classify key , scale , 'instrument , tambre` etc using vaoice it could be the tone of the voice hooty , squeezed , breathy , chest , head , etc Metrics use accuracy_thresh with a selected threshold check the video you need to update the accuracy metric","title":"Multilabel Classificaiton with Audio data"},{"location":"notebooks/01_study_notes/#creating-data","text":"you might want to use the actual matrices instead of images pytorch has a TensorDataset() function that will converts any 2 tensor into a dataset. You can then use DataBunch.create() to create a databunch iterator lesson 5 at 1:27 shows this","title":"Creating data"},{"location":"notebooks/01_study_notes/#reading","text":"","title":"Reading"},{"location":"notebooks/01_study_notes/#links","text":"musical freqs ISMIR librosa creators MIR THIS IS USEFUL Pitch Detection ### Papers Detecting Musical Key With Supervised Learning Deep residual learning for image recognition ResBlocks and resnets Visualising the loss landscape of neural nets In [ ]:","title":"links"},{"location":"notebooks/lesson_3_multilabel/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); lesson 6: regularization, convolutions \u00b6 In [1]: from fastai import * from fastai.vision import * % matplotlib inline /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError. warnings.warn(msg) In [2]: path = untar_data ( URLs . PETS ) / 'images' Downloading https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz In [7]: tfms = get_transforms ( max_rotate = 20 , max_zoom = 1.3 , max_lighting = 0.4 , max_warp = 0.4 , p_affine = 1. , p_lighting = 1. ) In [8]: src = ImageList . from_folder ( path ) . split_by_rand_pct ( 0.2 , seed = 2 ) In [9]: def get_data ( size , bs , padding_mode = 'reflection' ): return ( src . label_from_re ( r '([^/]+)_\\d+.jpg$' ) . transform ( tfms , size = size , padding_mode = padding_mode ) . databunch ( bs = bs ) . normalize ( imagenet_stats )) In [10]: bs = 16 data = get_data ( 224 , bs ) /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \" In [12]: learn = cnn_learner ( data , models . resnet34 , metrics = error_rate , bn_final = True ) Manual convolutions \u00b6 In [21]: idx = 29 x , y = data . valid_ds [ idx ] x . show () data . valid_ds . y [ idx ] Out[21]: Category 8 In [41]: k = tensor ([ [ 0. , - 5 / 3 , 1 ], [ - 5 / 3 , - 5 / 3 , 1 ], [ 1. , 1 , 1 ], ]) . expand ( 1 , 3 , 3 , 3 ) / 6 # expand basically copies the kernal in a memory efficient way In [42]: k Out[42]: tensor([[[[ 0.0000, -0.2778, 0.1667], [-0.2778, -0.2778, 0.1667], [ 0.1667, 0.1667, 0.1667]], [[ 0.0000, -0.2778, 0.1667], [-0.2778, -0.2778, 0.1667], [ 0.1667, 0.1667, 0.1667]], [[ 0.0000, -0.2778, 0.1667], [-0.2778, -0.2778, 0.1667], [ 0.1667, 0.1667, 0.1667]]]]) In [23]: k . shape # check the shape # it is a 3 x 3 kernel # there are 3 of them # 1 kernal (unit axis) Out[23]: torch.Size([1, 3, 3, 3]) In [35]: t = data . valid_ds [ 29 ][ 0 ] . data ; t . shape Out[35]: torch.Size([3, 224, 224]) pytorch expects minibatches so we need to create a minibatch of size 1 ie rank 4 tensor of size 1 indexing into an array with [None] creates a new unit axis like below In [36]: t [ None ] . shape Out[36]: torch.Size([1, 3, 224, 224]) In [37]: edge = F . conv2d ( t [ None ], k ) In [38]: show_image ( edge [ 0 ], figsize = ( 5 , 5 )); In [40]: learn . model Out[40]: Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=37, bias=True) (9): BatchNorm1d(37, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True) ) ) In [39]: learn . summary () /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \" Out[39]: Sequential ====================================================================== Layer (type) Output Shape Param # Trainable ====================================================================== Conv2d [64, 112, 112] 9,408 False ______________________________________________________________________ BatchNorm2d [64, 112, 112] 128 True ______________________________________________________________________ ReLU [64, 112, 112] 0 False ______________________________________________________________________ MaxPool2d [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ ReLU [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ ReLU [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ ReLU [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ Conv2d [128, 28, 28] 73,728 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 8,192 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [256, 14, 14] 294,912 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 32,768 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [512, 7, 7] 1,179,648 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ ReLU [512, 7, 7] 0 False ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ Conv2d [512, 7, 7] 131,072 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ ReLU [512, 7, 7] 0 False ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ ReLU [512, 7, 7] 0 False ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ AdaptiveAvgPool2d [512, 1, 1] 0 False ______________________________________________________________________ AdaptiveMaxPool2d [512, 1, 1] 0 False ______________________________________________________________________ Flatten [1024] 0 False ______________________________________________________________________ BatchNorm1d [1024] 2,048 True ______________________________________________________________________ Dropout [1024] 0 False ______________________________________________________________________ Linear [512] 524,800 True ______________________________________________________________________ ReLU [512] 0 False ______________________________________________________________________ BatchNorm1d [512] 1,024 True ______________________________________________________________________ Dropout [512] 0 False ______________________________________________________________________ Linear [37] 18,981 True ______________________________________________________________________ BatchNorm1d [37] 74 True ______________________________________________________________________ Total params: 21,831,599 Total trainable params: 563,951 Total non-trainable params: 21,267,648 Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99) Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ Loss function : FlattenedLoss ====================================================================== Callbacks functions applied Heatmap \u00b6 In [46]: m = learn . model . eval (); In [60]: m [ 0 ] ## this is the convolutional part of the model the resnet 34 Out[60]: Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) In [47]: xb , _ = data . one_item ( x ) xb_im = Image ( data . denorm ( xb )[ 0 ]) #xb = xb.cuda() In [43]: from fastai.callbacks.hooks import * In [44]: # grab m[0] and hook it's output def hooked_backward ( cat = y ): with hook_output ( m [ 0 ]) as hook_a : with hook_output ( m [ 0 ], grad = True ) as hook_g : preds = m ( xb ) preds [ 0 , int ( cat )] . backward () return hook_a , hook_g In [48]: # activation hook # gradient hook hook_a , hook_g = hooked_backward () In [58]: acts = hook_a . stored [ 0 ] . cpu () acts . shape Out[58]: torch.Size([512, 7, 7]) In [50]: avg_acts = acts . mean ( 0 ) # take the mean across dimension 0 avg_acts . shape Out[50]: torch.Size([7, 7]) In [55]: def show_heatmap ( hm ): _ , ax = plt . subplots () xb_im . show ( ax ) ax . imshow ( hm , alpha = 0.6 , extent = ( 0 , 224 , 224 , 0 ), interpolation = 'bilinear' , cmap = 'magma' ); In [56]: show_heatmap ( avg_acts ) In [ ]:","title":"Lesson 3 multilabel"},{"location":"notebooks/lesson_3_multilabel/#lesson-6-regularization-convolutions","text":"In [1]: from fastai import * from fastai.vision import * % matplotlib inline /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError. warnings.warn(msg) In [2]: path = untar_data ( URLs . PETS ) / 'images' Downloading https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz In [7]: tfms = get_transforms ( max_rotate = 20 , max_zoom = 1.3 , max_lighting = 0.4 , max_warp = 0.4 , p_affine = 1. , p_lighting = 1. ) In [8]: src = ImageList . from_folder ( path ) . split_by_rand_pct ( 0.2 , seed = 2 ) In [9]: def get_data ( size , bs , padding_mode = 'reflection' ): return ( src . label_from_re ( r '([^/]+)_\\d+.jpg$' ) . transform ( tfms , size = size , padding_mode = padding_mode ) . databunch ( bs = bs ) . normalize ( imagenet_stats )) In [10]: bs = 16 data = get_data ( 224 , bs ) /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \" In [12]: learn = cnn_learner ( data , models . resnet34 , metrics = error_rate , bn_final = True )","title":"lesson 6: regularization, convolutions"},{"location":"notebooks/lesson_3_multilabel/#manual-convolutions","text":"In [21]: idx = 29 x , y = data . valid_ds [ idx ] x . show () data . valid_ds . y [ idx ] Out[21]: Category 8 In [41]: k = tensor ([ [ 0. , - 5 / 3 , 1 ], [ - 5 / 3 , - 5 / 3 , 1 ], [ 1. , 1 , 1 ], ]) . expand ( 1 , 3 , 3 , 3 ) / 6 # expand basically copies the kernal in a memory efficient way In [42]: k Out[42]: tensor([[[[ 0.0000, -0.2778, 0.1667], [-0.2778, -0.2778, 0.1667], [ 0.1667, 0.1667, 0.1667]], [[ 0.0000, -0.2778, 0.1667], [-0.2778, -0.2778, 0.1667], [ 0.1667, 0.1667, 0.1667]], [[ 0.0000, -0.2778, 0.1667], [-0.2778, -0.2778, 0.1667], [ 0.1667, 0.1667, 0.1667]]]]) In [23]: k . shape # check the shape # it is a 3 x 3 kernel # there are 3 of them # 1 kernal (unit axis) Out[23]: torch.Size([1, 3, 3, 3]) In [35]: t = data . valid_ds [ 29 ][ 0 ] . data ; t . shape Out[35]: torch.Size([3, 224, 224]) pytorch expects minibatches so we need to create a minibatch of size 1 ie rank 4 tensor of size 1 indexing into an array with [None] creates a new unit axis like below In [36]: t [ None ] . shape Out[36]: torch.Size([1, 3, 224, 224]) In [37]: edge = F . conv2d ( t [ None ], k ) In [38]: show_image ( edge [ 0 ], figsize = ( 5 , 5 )); In [40]: learn . model Out[40]: Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=True) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=37, bias=True) (9): BatchNorm1d(37, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True) ) ) In [39]: learn . summary () /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/torch/nn/functional.py:2854: UserWarning: The default behavior for interpolate/upsample with float scale_factor will change in 1.6.0 to align with other frameworks/libraries, and use scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. warnings.warn(\"The default behavior for interpolate/upsample with float scale_factor will change \" Out[39]: Sequential ====================================================================== Layer (type) Output Shape Param # Trainable ====================================================================== Conv2d [64, 112, 112] 9,408 False ______________________________________________________________________ BatchNorm2d [64, 112, 112] 128 True ______________________________________________________________________ ReLU [64, 112, 112] 0 False ______________________________________________________________________ MaxPool2d [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ ReLU [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ ReLU [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ ReLU [64, 56, 56] 0 False ______________________________________________________________________ Conv2d [64, 56, 56] 36,864 False ______________________________________________________________________ BatchNorm2d [64, 56, 56] 128 True ______________________________________________________________________ Conv2d [128, 28, 28] 73,728 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 8,192 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ ReLU [128, 28, 28] 0 False ______________________________________________________________________ Conv2d [128, 28, 28] 147,456 False ______________________________________________________________________ BatchNorm2d [128, 28, 28] 256 True ______________________________________________________________________ Conv2d [256, 14, 14] 294,912 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 32,768 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ ReLU [256, 14, 14] 0 False ______________________________________________________________________ Conv2d [256, 14, 14] 589,824 False ______________________________________________________________________ BatchNorm2d [256, 14, 14] 512 True ______________________________________________________________________ Conv2d [512, 7, 7] 1,179,648 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ ReLU [512, 7, 7] 0 False ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ Conv2d [512, 7, 7] 131,072 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ ReLU [512, 7, 7] 0 False ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ ReLU [512, 7, 7] 0 False ______________________________________________________________________ Conv2d [512, 7, 7] 2,359,296 False ______________________________________________________________________ BatchNorm2d [512, 7, 7] 1,024 True ______________________________________________________________________ AdaptiveAvgPool2d [512, 1, 1] 0 False ______________________________________________________________________ AdaptiveMaxPool2d [512, 1, 1] 0 False ______________________________________________________________________ Flatten [1024] 0 False ______________________________________________________________________ BatchNorm1d [1024] 2,048 True ______________________________________________________________________ Dropout [1024] 0 False ______________________________________________________________________ Linear [512] 524,800 True ______________________________________________________________________ ReLU [512] 0 False ______________________________________________________________________ BatchNorm1d [512] 1,024 True ______________________________________________________________________ Dropout [512] 0 False ______________________________________________________________________ Linear [37] 18,981 True ______________________________________________________________________ BatchNorm1d [37] 74 True ______________________________________________________________________ Total params: 21,831,599 Total trainable params: 563,951 Total non-trainable params: 21,267,648 Optimized with 'torch.optim.adam.Adam', betas=(0.9, 0.99) Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ Loss function : FlattenedLoss ====================================================================== Callbacks functions applied","title":"Manual convolutions"},{"location":"notebooks/lesson_3_multilabel/#heatmap","text":"In [46]: m = learn . model . eval (); In [60]: m [ 0 ] ## this is the convolutional part of the model the resnet 34 Out[60]: Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) In [47]: xb , _ = data . one_item ( x ) xb_im = Image ( data . denorm ( xb )[ 0 ]) #xb = xb.cuda() In [43]: from fastai.callbacks.hooks import * In [44]: # grab m[0] and hook it's output def hooked_backward ( cat = y ): with hook_output ( m [ 0 ]) as hook_a : with hook_output ( m [ 0 ], grad = True ) as hook_g : preds = m ( xb ) preds [ 0 , int ( cat )] . backward () return hook_a , hook_g In [48]: # activation hook # gradient hook hook_a , hook_g = hooked_backward () In [58]: acts = hook_a . stored [ 0 ] . cpu () acts . shape Out[58]: torch.Size([512, 7, 7]) In [50]: avg_acts = acts . mean ( 0 ) # take the mean across dimension 0 avg_acts . shape Out[50]: torch.Size([7, 7]) In [55]: def show_heatmap ( hm ): _ , ax = plt . subplots () xb_im . show ( ax ) ax . imshow ( hm , alpha = 0.6 , extent = ( 0 , 224 , 224 , 0 ), interpolation = 'bilinear' , cmap = 'magma' ); In [56]: show_heatmap ( avg_acts ) In [ ]:","title":"Heatmap"},{"location":"notebooks/spectrogram_data/","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI escape sequences */ /* The color values are a mix of http://www.xcolors.net/dl/baskerville-ivorylight and http://www.xcolors.net/dl/euphrasia */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-default-inverse-fg { color: #FFFFFF; } .ansi-default-inverse-bg { background-color: #000000; } .ansi-bold { font-weight: bold; } .ansi-underline { text-decoration: underline; } /* The following styles are deprecated an will be removed in a future version */ .ansibold { font-weight: bold; } .ansi-inverse { outline: 0.5px dotted; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; position: relative; overflow: visible; } div.cell:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: transparent; } div.cell.jupyter-soft-selected { border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: #ababab; } div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #42A5F5; } @media print { div.cell.selected, div.cell.selected.jupyter-soft-selected { border-color: transparent; } } .edit_mode div.cell.selected { border-color: #66BB6A; } .edit_mode div.cell.selected:before { position: absolute; display: block; top: -1px; left: -1px; width: 5px; height: calc(100% + 2px); content: ''; background: #66BB6A; } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ /* Note that this should set vertical padding only, since CodeMirror assumes that horizontal padding will be set on CodeMirror pre */ padding: 0.4em 0; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. This sets horizontal padding only, use .CodeMirror-lines for vertical */ padding: 0 0.4em; border: 0; border-radius: 0; } .CodeMirror-cursor { border-left: 1.4px solid black; } @media screen and (min-width: 2138px) and (max-width: 4319px) { .CodeMirror-cursor { border-left: 2px solid black; } } @media screen and (min-width: 4320px) { .CodeMirror-cursor { border-left: 4px solid black; } } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } div.output_area .mglyph > img { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 1px 0 1px 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html ul:not(.list-inline), .rendered_html ol:not(.list-inline) { padding-left: 2em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html tbody tr:nth-child(odd) { background: #f5f5f5; } .rendered_html tbody tr:hover { background: rgba(66, 165, 245, 0.2); } .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, .rendered_html * + .alert { margin-top: 1em; } [dir=\"rtl\"] div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.rendered .rendered_html tr, .text_cell.rendered .rendered_html th, .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .text_cell .dropzone .input_area { border: 2px dashed #bababa; margin: -1px; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight-ipynb .hll { background-color: #ffffcc } .highlight-ipynb { background: #f8f8f8; } .highlight-ipynb .c { color: #408080; font-style: italic } /* Comment */ .highlight-ipynb .err { border: 1px solid #FF0000 } /* Error */ .highlight-ipynb .k { color: #008000; font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: #666666 } /* Operator */ .highlight-ipynb .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight-ipynb .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight-ipynb .gd { color: #A00000 } /* Generic.Deleted */ .highlight-ipynb .ge { font-style: italic } /* Generic.Emph */ .highlight-ipynb .gr { color: #FF0000 } /* Generic.Error */ .highlight-ipynb .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight-ipynb .gi { color: #00A000 } /* Generic.Inserted */ .highlight-ipynb .go { color: #888888 } /* Generic.Output */ .highlight-ipynb .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight-ipynb .gs { font-weight: bold } /* Generic.Strong */ .highlight-ipynb .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight-ipynb .gt { color: #0044DD } /* Generic.Traceback */ .highlight-ipynb .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: #008000 } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: #B00040 } /* Keyword.Type */ .highlight-ipynb .m { color: #666666 } /* Literal.Number */ .highlight-ipynb .s { color: #BA2121 } /* Literal.String */ .highlight-ipynb .na { color: #7D9029 } /* Name.Attribute */ .highlight-ipynb .nb { color: #008000 } /* Name.Builtin */ .highlight-ipynb .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight-ipynb .no { color: #880000 } /* Name.Constant */ .highlight-ipynb .nd { color: #AA22FF } /* Name.Decorator */ .highlight-ipynb .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight-ipynb .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight-ipynb .nf { color: #0000FF } /* Name.Function */ .highlight-ipynb .nl { color: #A0A000 } /* Name.Label */ .highlight-ipynb .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight-ipynb .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight-ipynb .nv { color: #19177C } /* Name.Variable */ .highlight-ipynb .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: #bbbbbb } /* Text.Whitespace */ .highlight-ipynb .mb { color: #666666 } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: #666666 } /* Literal.Number.Float */ .highlight-ipynb .mh { color: #666666 } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: #666666 } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: #666666 } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight-ipynb .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: #BA2121 } /* Literal.String.Char */ .highlight-ipynb .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight-ipynb .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight-ipynb .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: #008000 } /* Literal.String.Other */ .highlight-ipynb .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight-ipynb .ss { color: #19177C } /* Literal.String.Symbol */ .highlight-ipynb .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight-ipynb .fm { color: #0000FF } /* Name.Function.Magic */ .highlight-ipynb .vc { color: #19177C } /* Name.Variable.Class */ .highlight-ipynb .vg { color: #19177C } /* Name.Variable.Global */ .highlight-ipynb .vi { color: #19177C } /* Name.Variable.Instance */ .highlight-ipynb .vm { color: #19177C } /* Name.Variable.Magic */ .highlight-ipynb .il { color: #666666 } /* Literal.Number.Integer.Long */ .rendered_html a{text-decoration:inherit !important}.rendered_html :link{text-decoration:inherit !important}.rendered_html :visited{text-decoration:inherit !important}pre code{background-color:inherit !important}.highlight{color:#000000}.highlight code{color:#000000}.highlight .n{color:#333333}.highlight .p{color:#000000}.text_cell .prompt{display:none !important}div.input_prompt{padding:0.2em 0.4em}div.output_prompt{padding:0.4em}.text_cell{margin:0 !important;padding:0 !important;border:none !important}.text_cell_render{margin:0 !important;padding:0 !important;border:none !important}.rendered_html *+p{margin-top:inherit !important}.anchor-link{display:none !important}.code_cell{margin:0 !important;padding:5px 0 !important;border:none !important}.celltoolbar{border:thin solid #CFCFCF;border-bottom:none;background:#EEE;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;box-pack:end;-webkit-box-pack:start;-ms-flex-pack:start;justify-content:flex-start;display:-webkit-flex}.celltoolbar .tags_button_container{display:-webkit-box;display:-ms-flexbox;display:flex}.celltoolbar .tags_button_container .tag-container{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;overflow:hidden;position:relative}.celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;-webkit-box-shadow:none;box-shadow:none;width:inherit;font-size:13px;font-family:\"Helvetica Neue\", Helvetica, Arial, sans-serif;height:22px;line-height:22px;display:inline-block}div.input_area>div.highlight{margin:0.25em 0.4em !important}.code_cell pre{font-size:12px !important}.output_html table.dataframe{font-family:Arial, sans-serif;font-size:13px;line-height:20px}.output_html table.dataframe th,td{padding:4px;text-align:left}.bk-plot-wrapper tbody tr{background:none !important}.bk-plot-wrapper tbody tr:hover{background:none !important} /*# sourceMappingURL=jupyter-fixes.min.css.map */ MathJax.Hub.Config({ tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, // Center justify equations in code and markdown cells. Elsewhere // we use CSS to left justify single line equations in code cells. displayAlign: 'center', \"HTML-CSS\": { styles: {'.MathJax_Display': {\"margin\": 0}}, linebreaks: { automatic: true } } }); spectrogram data \u00b6 Assessing different types of plots \u00b6 I need the pitch component to be clear. It is not obvious to me that it is in the above plots info \u00b6 sr = sampling rate in (Hz) frame = short audio clip n_fft = samples per frame hop_length = # samples between frames Frequent Furrier Transform \u00b6 use a log y_axis instead of linear. Why? Because all of the information is at the bottom of the plot, the log version brings this out see this Constant Q transform \u00b6 doing log spaced freq representation here one vertical move is one shift in semitone Reading \u00b6 MIR this will be very useful WIKI paper medium music genre classification git uses the numpy arrays not the images! getting to know mel spec Things to try \u00b6 train with smaller image sizes, then create a new data bunch and you can actually load that data back into the model, then change the size to something bigger and train for longer. J.Howard says he hasn't gotten much better results with smaller than 64, but could be worth a try after first round of training, do lr_find() again, check the learning rate and pick a new one, using a slice lr of 3e-3 generally works well for the first round of training before unfreezing then for second round, for the first part of the slice use 10x lower for second part of slice, then whatever lr_finder found for the first part of the slice learn.fit_one_cycle(4, 3e-3) learn.unfreeze() learn.fit_one_cycle(4, slice(\"lr_finder number\",3e-4)) In [1]: import matplotlib.pyplot as plt import IPython.display as ipd import librosa import librosa.display from pathlib import Path import numpy as np % matplotlib inline /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location. Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0. from numba.decorators import jit as optional_jit /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location. Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0. from numba.decorators import jit as optional_jit In [2]: path = Path . cwd () / 'music_data' path Out[2]: PosixPath('/Users/devindearaujo/Desktop/fastai/music_data') Mel Spec loop \u00b6 In [56]: # this script will compute STFT specto grams # not the best for pitch folders = [ 'C3' , 'D3' , 'E3' , 'F3' , 'G3' , 'A3' , 'B3' ] output = path . mkdir ( 'output' ) for f in folders : p = path / f for child in p . iterdir (): if child . stem != '.DS_Store' : # load audio wavs sig , fs = librosa . load ( child ) # setup plot plt . figure ( figsize = ( 10 , 10 )) # process spectrogram s = librosa . feature . melspectrogram ( y = sig , sr = fs , power = 2 ) librosa . display . specshow ( librosa . power_to_db ( s , ref = np . max )) fname = child . stem + '.jpg' plt . savefig ( fname = output / fname , format = 'jpg' ) plt . close ( 'all' ); Chroma Spec loop \u00b6 chroma is good for note classificaiton, not octave classification In [54]: # this will convert to folders = [ 'Cs3' , 'Ds3' , 'Fs3' , 'Gs3' , 'As3' ] #folders = ['C3', 'D3', 'E3', 'F3', 'G3', 'A3', 'B3'] for f in folders : p = path / f for child in p . iterdir (): if child . stem != '.DS_Store' : # load audio wavs y , sr = librosa . load ( child ) # setup plot plt . figure ( figsize = ( 8 , 8 )) # process spectrogram C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C ); #y_axis='cqt_note' axis label is hidden fname = child . stem + '.jpg' plt . savefig ( fname = p / fname , format = 'jpg' ) plt . close ( 'all' ); Pitch or frequency detection \u00b6 the estimate_pitch function returns a pretty close apporximation of f0 checked against this In [92]: y , sr = librosa . load ( path / 'E3/E3_21.wav' , mono = True ) librosa . core . hz_to_note ( 440 ) librosa . core . note_to_hz ( 'C3' ) # Estimate the tuning of an audio time series or spectrogram input. librosa . core . pitch . estimate_tuning ( y , sr ) pitches , magnitudes = librosa . piptrack ( y = y , sr = sr , threshold = 1 , ref = np . mean ) pitches . shape , magnitudes . shape Out[92]: ((1025, 87), (1025, 87)) In [94]: def estimate_pitch ( segment , sr , fmin = 50.0 , fmax = 2000.0 ): # Compute autocorrelation of input segment. r = librosa . autocorrelate ( segment ) # Define lower and upper limits for the autocorrelation argmax. i_min = sr / fmax i_max = sr / fmin r [: int ( i_min )] = 0 r [ int ( i_max ):] = 0 # Find the location of the maximum autocorrelation. i = r . argmax () f0 = float ( sr ) / i return f0 estimate_pitch ( y , sr ) Out[94]: 165.78947368421052 Spectrogram investigation \u00b6 In [154]: y , sr = librosa . load ( path / 'G3/G3_21.wav' , mono = True ) ipd . Audio ( y , rate = sr ) Out[154]: Your browser does not support the audio element. In [53]: ## Log y_axis y , sr = librosa . load ( path / 'E3/E3_25.wav' ) #plt.figure(figsize=(4,4)) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) #librosa.display.specshow(D, y_axis='log'); y . shape , D . shape Out[53]: ((44100,), (1025, 87)) In [40]: ## Log y_axis y , sr = librosa . load ( path / 'E3/E3_25.wav' ) plt . figure ( figsize = ( 4 , 4 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' ); In [41]: ## Log y_axis STFT with different hop_lengths y , sr = librosa . load ( path / 'F3/F3_2.wav' ) plt . figure ( figsize = ( 4 , 4 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y , hop_length = 10000 )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' ); In [118]: y , sr = librosa . load ( path / 'F3/F3_25.wav' ) plt . figure ( figsize = ( 6 , 6 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C ); #y_axis='cqt_note' Loop through \u00b6 In [107]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 6 , 6 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C , y_axis = 'chroma' ); #plt.close() In [132]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 6 , 6 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y , hop_length = 15000 )), ref = np . max ) plt . title ( w . stem ) librosa . display . specshow ( D , y_axis = 'log' ); In [150]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 6 , 6 )) C = librosa . cqt ( y = y , sr = sr , n_bins = 72 ) plt . title ( w . stem ) # plot the log #logC = librosa.amplitude_to_db(np.abs(C)) librosa . display . specshow ( np . abs ( C ), y_axis = 'chroma' ); Check Octave UP \u00b6 check docs there is an fmin minimum frequency to analyze in the CQT. Default: \u2018C1\u2019 ~= 32.7 Hz see if librosa has feature to extract min f there is also n_octaves -Number of octaves to analyze above fmin In [35]: 261.63 / 2 Out[35]: 130.815 In [38]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 4 , 4 )) plt . title ( w . stem ) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) #C = librosa.feature.chroma_stft(y=y, sr=sr) librosa . display . specshow ( C , y_axis = 'chroma' ); #plt.close()","title":"Spectrogram data"},{"location":"notebooks/spectrogram_data/#spectrogram-data","text":"","title":"spectrogram data"},{"location":"notebooks/spectrogram_data/#assessing-different-types-of-plots","text":"I need the pitch component to be clear. It is not obvious to me that it is in the above plots","title":"Assessing different types of plots"},{"location":"notebooks/spectrogram_data/#info","text":"sr = sampling rate in (Hz) frame = short audio clip n_fft = samples per frame hop_length = # samples between frames","title":"info"},{"location":"notebooks/spectrogram_data/#frequent-furrier-transform","text":"use a log y_axis instead of linear. Why? Because all of the information is at the bottom of the plot, the log version brings this out see this","title":"Frequent Furrier Transform"},{"location":"notebooks/spectrogram_data/#constant-q-transform","text":"doing log spaced freq representation here one vertical move is one shift in semitone","title":"Constant Q transform"},{"location":"notebooks/spectrogram_data/#reading","text":"MIR this will be very useful WIKI paper medium music genre classification git uses the numpy arrays not the images! getting to know mel spec","title":"Reading"},{"location":"notebooks/spectrogram_data/#things-to-try","text":"train with smaller image sizes, then create a new data bunch and you can actually load that data back into the model, then change the size to something bigger and train for longer. J.Howard says he hasn't gotten much better results with smaller than 64, but could be worth a try after first round of training, do lr_find() again, check the learning rate and pick a new one, using a slice lr of 3e-3 generally works well for the first round of training before unfreezing then for second round, for the first part of the slice use 10x lower for second part of slice, then whatever lr_finder found for the first part of the slice learn.fit_one_cycle(4, 3e-3) learn.unfreeze() learn.fit_one_cycle(4, slice(\"lr_finder number\",3e-4)) In [1]: import matplotlib.pyplot as plt import IPython.display as ipd import librosa import librosa.display from pathlib import Path import numpy as np % matplotlib inline /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location. Import requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0. from numba.decorators import jit as optional_jit /Users/devindearaujo/.pyenv/versions/3.7.7/lib/python3.7/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location. Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0. from numba.decorators import jit as optional_jit In [2]: path = Path . cwd () / 'music_data' path Out[2]: PosixPath('/Users/devindearaujo/Desktop/fastai/music_data')","title":"Things to try"},{"location":"notebooks/spectrogram_data/#mel-spec-loop","text":"In [56]: # this script will compute STFT specto grams # not the best for pitch folders = [ 'C3' , 'D3' , 'E3' , 'F3' , 'G3' , 'A3' , 'B3' ] output = path . mkdir ( 'output' ) for f in folders : p = path / f for child in p . iterdir (): if child . stem != '.DS_Store' : # load audio wavs sig , fs = librosa . load ( child ) # setup plot plt . figure ( figsize = ( 10 , 10 )) # process spectrogram s = librosa . feature . melspectrogram ( y = sig , sr = fs , power = 2 ) librosa . display . specshow ( librosa . power_to_db ( s , ref = np . max )) fname = child . stem + '.jpg' plt . savefig ( fname = output / fname , format = 'jpg' ) plt . close ( 'all' );","title":"Mel Spec loop"},{"location":"notebooks/spectrogram_data/#chroma-spec-loop","text":"chroma is good for note classificaiton, not octave classification In [54]: # this will convert to folders = [ 'Cs3' , 'Ds3' , 'Fs3' , 'Gs3' , 'As3' ] #folders = ['C3', 'D3', 'E3', 'F3', 'G3', 'A3', 'B3'] for f in folders : p = path / f for child in p . iterdir (): if child . stem != '.DS_Store' : # load audio wavs y , sr = librosa . load ( child ) # setup plot plt . figure ( figsize = ( 8 , 8 )) # process spectrogram C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C ); #y_axis='cqt_note' axis label is hidden fname = child . stem + '.jpg' plt . savefig ( fname = p / fname , format = 'jpg' ) plt . close ( 'all' );","title":"Chroma Spec loop"},{"location":"notebooks/spectrogram_data/#pitch-or-frequency-detection","text":"the estimate_pitch function returns a pretty close apporximation of f0 checked against this In [92]: y , sr = librosa . load ( path / 'E3/E3_21.wav' , mono = True ) librosa . core . hz_to_note ( 440 ) librosa . core . note_to_hz ( 'C3' ) # Estimate the tuning of an audio time series or spectrogram input. librosa . core . pitch . estimate_tuning ( y , sr ) pitches , magnitudes = librosa . piptrack ( y = y , sr = sr , threshold = 1 , ref = np . mean ) pitches . shape , magnitudes . shape Out[92]: ((1025, 87), (1025, 87)) In [94]: def estimate_pitch ( segment , sr , fmin = 50.0 , fmax = 2000.0 ): # Compute autocorrelation of input segment. r = librosa . autocorrelate ( segment ) # Define lower and upper limits for the autocorrelation argmax. i_min = sr / fmax i_max = sr / fmin r [: int ( i_min )] = 0 r [ int ( i_max ):] = 0 # Find the location of the maximum autocorrelation. i = r . argmax () f0 = float ( sr ) / i return f0 estimate_pitch ( y , sr ) Out[94]: 165.78947368421052","title":"Pitch or frequency detection"},{"location":"notebooks/spectrogram_data/#spectrogram-investigation","text":"In [154]: y , sr = librosa . load ( path / 'G3/G3_21.wav' , mono = True ) ipd . Audio ( y , rate = sr ) Out[154]: Your browser does not support the audio element. In [53]: ## Log y_axis y , sr = librosa . load ( path / 'E3/E3_25.wav' ) #plt.figure(figsize=(4,4)) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) #librosa.display.specshow(D, y_axis='log'); y . shape , D . shape Out[53]: ((44100,), (1025, 87)) In [40]: ## Log y_axis y , sr = librosa . load ( path / 'E3/E3_25.wav' ) plt . figure ( figsize = ( 4 , 4 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' ); In [41]: ## Log y_axis STFT with different hop_lengths y , sr = librosa . load ( path / 'F3/F3_2.wav' ) plt . figure ( figsize = ( 4 , 4 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y , hop_length = 10000 )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' ); In [118]: y , sr = librosa . load ( path / 'F3/F3_25.wav' ) plt . figure ( figsize = ( 6 , 6 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C ); #y_axis='cqt_note'","title":"Spectrogram investigation"},{"location":"notebooks/spectrogram_data/#loop-through","text":"In [107]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 6 , 6 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C , y_axis = 'chroma' ); #plt.close() In [132]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 6 , 6 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y , hop_length = 15000 )), ref = np . max ) plt . title ( w . stem ) librosa . display . specshow ( D , y_axis = 'log' ); In [150]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 6 , 6 )) C = librosa . cqt ( y = y , sr = sr , n_bins = 72 ) plt . title ( w . stem ) # plot the log #logC = librosa.amplitude_to_db(np.abs(C)) librosa . display . specshow ( np . abs ( C ), y_axis = 'chroma' );","title":"Loop through"},{"location":"notebooks/spectrogram_data/#check-octave-up","text":"check docs there is an fmin minimum frequency to analyze in the CQT. Default: \u2018C1\u2019 ~= 32.7 Hz see if librosa has feature to extract min f there is also n_octaves -Number of octaves to analyze above fmin In [35]: 261.63 / 2 Out[35]: 130.815 In [38]: p = path / 'test' for w in p . iterdir (): if w . stem != '.DS_Store' : y , sr = librosa . load ( w ) plt . figure ( figsize = ( 4 , 4 )) plt . title ( w . stem ) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) #C = librosa.feature.chroma_stft(y=y, sr=sr) librosa . display . specshow ( C , y_axis = 'chroma' ); #plt.close()","title":"Check Octave UP"}]}