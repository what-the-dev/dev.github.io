{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Data Scientist working at SEEK, specialising in Online and Tech. I work closely with product teams to identify opportunities to optimise product and provide value to candidates through data driven insights and modelling. I am a proactive, creative person with a passion for learning. I am constantly striving to improving my skill-set. I do this by immersing myself in new challenges. I am passionate about local music, python, data, the arts and travel. Skillset Machine Learning Deep Learning Python SQL Adobe Analytics Power BI Web Analytics Data Visualisation","title":"About"},{"location":"#about","text":"Data Scientist working at SEEK, specialising in Online and Tech. I work closely with product teams to identify opportunities to optimise product and provide value to candidates through data driven insights and modelling. I am a proactive, creative person with a passion for learning. I am constantly striving to improving my skill-set. I do this by immersing myself in new challenges. I am passionate about local music, python, data, the arts and travel.","title":"About"},{"location":"#skillset","text":"Machine Learning Deep Learning Python SQL Adobe Analytics Power BI Web Analytics Data Visualisation","title":"Skillset"},{"location":"computer%20vision/01_image_segmentation_floodnet/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Image Segmentation Image segmentation refers to the process of dividing an image into meaningful and distinct regions or objects at the pixel level. It involves assigning a label or class to each pixel in an image to identify different objects, boundaries, or areas of interest. The goal of image segmentation is to separate and distinguish different objects or regions within an image, enabling a computer or an algorithm to understand and analyze the image at a more detailed level. Segmentation has benefits to downstream tasks such as object recognition and tracking, scene understanding, medical image analysis and robotics to name a few. U-Net The U-Net is a convolutional neural net (CNN) that was originally developed in 2015 at the Computer Science Department of the University of Freiburg for the task of biomedical image segmentation. U-Net introduced an encoder-decoder architecture with skip connections. The contracting path captured context and abstract features, while the expansive path recovered spatial resolution using skip connections. U-Net's design made it highly effective for biomedical image segmentation and subsequently gained popularity in other domains. from fastai.vision.all import * from fastai.data.all import * FloodNet The below description is from the FloodNet GitHub FloodNet provides high-resolution UAV (Unmanned Aerial Vehicle) imageries with detailed semantic annotation regarding the damages. To advance the damage assessment process for post-disaster scenarios, we present a unique challenge considering classification, semantic segmentation, visual question answering highlighting the UAS imagery-based FloodNet dataset. Track 1 In this track, participants are required to complete two semi-supervised tasks. The first task is image classification, and the second task is semantic segmentation. 1. Semi-Supervised Classification: Classification for FloodNet dataset requires classifying the images into \u2018Flooded\u2019 and \u2018Non-Flooded\u2019 classes. Only a few of the training images have their labels available, while most of the training images are unlabeled. Semi-Supervised Semantic Segmentation: The semantic segmentation labels include: 1) Background, 2) Building Flooded, 3) Building Non-Flooded, 4) Road Flooded, 5) Road Non-Flooded, 6) Water, 7)Tree, 8) Vehicle, 9) Pool, 10) Grass. Only a small portion of the training images have their corresponding masks available. Links FloodNet paper data download import numpy as np import pandas as pd from pathlib import Path Segmentaion datasets usually consist of image files, mask files and codes which are the segmenttion pixel labels. path = Path . cwd () / 'floodnet_data' path Path('/Users/ddearaujo/Desktop/dl/vision/image_segmentation/floodnet_data') # get loabels / codes col_map = { 'Class Index.1' : 'class_id' , 'Class Name.1' : 'label' } df_codes = pd . read_csv ( path / 'class_mapping.csv' , header = 2 ) . iloc [:, - 2 :] . rename ( columns = col_map ) codes = df_codes . label . values df_codes . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } class_id label 0 0 Background 1 1 Building-flooded 2 2 Building-non-flooded 3 3 Road-flooded 4 4 Road-non-flooded # Get all the files in path with optional extensions # mask files are PNG so we can exclude these by specifying the extensions fnames = get_files ( path / \"train\" , extensions = '.jpg' ) fnames [ 0 ] Path('/Users/ddearaujo/Desktop/dl/vision/image_segmentation/floodnet_data/train/not_flooded/image/7078.jpg') def label_func ( fn ): p = path / 'train' / fn . parts [ - 3 ] / 'mask' / f ' { fn . stem } _lab.png' return p dls = SegmentationDataLoaders . from_label_func ( path , bs = 8 , fnames = fnames , label_func = label_func , codes = codes , item_tfms = Resize ( 128 ) ) segmentation dls . show_batch ( max_n = 4 ) Model: U-Net Traditional convolutional neural networks (CNNs) are effective for various computer vision tasks, such as image classification, object detection, and localization. However, they have limitations when it comes to image segmentation. Reasons for this include... Resolution Loss: CNNs typically downsample the input image as they progress through the network to capture higher-level features. This downsampling reduces the resolution of the feature maps, making it challenging to accurately localize and segment small objects or fine details in the image. Contextual Information: Segmentation tasks often require capturing contextual information to distinguish between objects with similar appearances or to handle complex object boundaries. Traditional CNNs, with their hierarchical feature extraction, may struggle to capture long-range dependencies and global context, which are crucial for accurate segmentation. Limited Localization Accuracy: CNNs designed for classification or localization tasks focus on identifying the presence of objects within an image but do not provide precise information about their boundaries. Segmenting an image requires pixel-level localization accuracy, which is not emphasized in traditional CNNs. The U-Net is specifically designed for semantic segmentation and addresses the above limitations. It employs a U-shaped architecture, consisting of a contracting path (encoder) and an expansive path (decoder), with skip connections between corresponding encoder and decoder layers. Advantages of using a UNet include... - U-shaped Architecture: U-Net's U-shaped design enables the preservation of high-resolution feature maps through skip connections, which helps in localizing objects accurately. - Context Aggregation: Skip connections in UNet allow the decoder to receive feature maps from different resolutions, incorporating both local and global contextual information. This aids in better segmentation by capturing fine details and understanding the overall context. - Dense Feature Propagation: U-Net uses upsampling and concatenation operations during the decoding phase, which helps in recovering the lost spatial resolution. This dense feature propagation aids in precise segmentation by retaining spatial information. model = unet_learner ( dls , resnet34 ) model . fine_tune ( 10 ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } epoch train_loss valid_loss time 0 1.452725 1.594936 02:43 /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } epoch train_loss valid_loss time 0 1.108694 1.171069 02:42 1 1.014180 1.138221 02:50 2 0.988032 1.126446 02:40 3 0.967991 1.056948 02:40 4 0.885660 0.986956 02:38 5 0.820628 0.929066 02:39 6 0.770862 0.848385 02:38 7 0.713452 0.886977 02:36 8 0.667192 0.956540 02:37 9 0.625676 0.927906 02:37 model . show_results ( max_n = 4 , figsize = ( 7 , 10 )) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } interp = SegmentationInterpretation . from_learner ( model ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } top_losses = interp . top_losses ( 4 , largest = True )[ 1 ] interp . show_results ( top_losses . data ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } The interpreter shows the model makes some reasonable predictions, but there is still room for improvement! Bibliography body { font-family: Arial, sans-serif; } .citation { margin-bottom: 10px; } .title { font-weight: bold; } .authors { font-style: italic; } .journal { font-weight: bold; } .year { font-weight: bold; } FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, Robin Murphy arXiv preprint arXiv:2012.02951 2020","title":"01 image segmentation floodnet"},{"location":"computer%20vision/01_image_segmentation_floodnet/#image-segmentation","text":"Image segmentation refers to the process of dividing an image into meaningful and distinct regions or objects at the pixel level. It involves assigning a label or class to each pixel in an image to identify different objects, boundaries, or areas of interest. The goal of image segmentation is to separate and distinguish different objects or regions within an image, enabling a computer or an algorithm to understand and analyze the image at a more detailed level. Segmentation has benefits to downstream tasks such as object recognition and tracking, scene understanding, medical image analysis and robotics to name a few.","title":"Image Segmentation"},{"location":"computer%20vision/01_image_segmentation_floodnet/#u-net","text":"The U-Net is a convolutional neural net (CNN) that was originally developed in 2015 at the Computer Science Department of the University of Freiburg for the task of biomedical image segmentation. U-Net introduced an encoder-decoder architecture with skip connections. The contracting path captured context and abstract features, while the expansive path recovered spatial resolution using skip connections. U-Net's design made it highly effective for biomedical image segmentation and subsequently gained popularity in other domains. from fastai.vision.all import * from fastai.data.all import *","title":"U-Net"},{"location":"computer%20vision/01_image_segmentation_floodnet/#floodnet","text":"The below description is from the FloodNet GitHub FloodNet provides high-resolution UAV (Unmanned Aerial Vehicle) imageries with detailed semantic annotation regarding the damages. To advance the damage assessment process for post-disaster scenarios, we present a unique challenge considering classification, semantic segmentation, visual question answering highlighting the UAS imagery-based FloodNet dataset.","title":"FloodNet"},{"location":"computer%20vision/01_image_segmentation_floodnet/#track-1","text":"In this track, participants are required to complete two semi-supervised tasks. The first task is image classification, and the second task is semantic segmentation. 1. Semi-Supervised Classification: Classification for FloodNet dataset requires classifying the images into \u2018Flooded\u2019 and \u2018Non-Flooded\u2019 classes. Only a few of the training images have their labels available, while most of the training images are unlabeled. Semi-Supervised Semantic Segmentation: The semantic segmentation labels include: 1) Background, 2) Building Flooded, 3) Building Non-Flooded, 4) Road Flooded, 5) Road Non-Flooded, 6) Water, 7)Tree, 8) Vehicle, 9) Pool, 10) Grass. Only a small portion of the training images have their corresponding masks available.","title":"Track 1"},{"location":"computer%20vision/01_image_segmentation_floodnet/#links","text":"FloodNet paper data download import numpy as np import pandas as pd from pathlib import Path Segmentaion datasets usually consist of image files, mask files and codes which are the segmenttion pixel labels. path = Path . cwd () / 'floodnet_data' path Path('/Users/ddearaujo/Desktop/dl/vision/image_segmentation/floodnet_data') # get loabels / codes col_map = { 'Class Index.1' : 'class_id' , 'Class Name.1' : 'label' } df_codes = pd . read_csv ( path / 'class_mapping.csv' , header = 2 ) . iloc [:, - 2 :] . rename ( columns = col_map ) codes = df_codes . label . values df_codes . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } class_id label 0 0 Background 1 1 Building-flooded 2 2 Building-non-flooded 3 3 Road-flooded 4 4 Road-non-flooded # Get all the files in path with optional extensions # mask files are PNG so we can exclude these by specifying the extensions fnames = get_files ( path / \"train\" , extensions = '.jpg' ) fnames [ 0 ] Path('/Users/ddearaujo/Desktop/dl/vision/image_segmentation/floodnet_data/train/not_flooded/image/7078.jpg') def label_func ( fn ): p = path / 'train' / fn . parts [ - 3 ] / 'mask' / f ' { fn . stem } _lab.png' return p dls = SegmentationDataLoaders . from_label_func ( path , bs = 8 , fnames = fnames , label_func = label_func , codes = codes , item_tfms = Resize ( 128 ) )","title":"Links"},{"location":"computer%20vision/01_image_segmentation_floodnet/#segmentation","text":"dls . show_batch ( max_n = 4 )","title":"segmentation"},{"location":"computer%20vision/01_image_segmentation_floodnet/#model-u-net","text":"Traditional convolutional neural networks (CNNs) are effective for various computer vision tasks, such as image classification, object detection, and localization. However, they have limitations when it comes to image segmentation. Reasons for this include... Resolution Loss: CNNs typically downsample the input image as they progress through the network to capture higher-level features. This downsampling reduces the resolution of the feature maps, making it challenging to accurately localize and segment small objects or fine details in the image. Contextual Information: Segmentation tasks often require capturing contextual information to distinguish between objects with similar appearances or to handle complex object boundaries. Traditional CNNs, with their hierarchical feature extraction, may struggle to capture long-range dependencies and global context, which are crucial for accurate segmentation. Limited Localization Accuracy: CNNs designed for classification or localization tasks focus on identifying the presence of objects within an image but do not provide precise information about their boundaries. Segmenting an image requires pixel-level localization accuracy, which is not emphasized in traditional CNNs. The U-Net is specifically designed for semantic segmentation and addresses the above limitations. It employs a U-shaped architecture, consisting of a contracting path (encoder) and an expansive path (decoder), with skip connections between corresponding encoder and decoder layers. Advantages of using a UNet include... - U-shaped Architecture: U-Net's U-shaped design enables the preservation of high-resolution feature maps through skip connections, which helps in localizing objects accurately. - Context Aggregation: Skip connections in UNet allow the decoder to receive feature maps from different resolutions, incorporating both local and global contextual information. This aids in better segmentation by capturing fine details and understanding the overall context. - Dense Feature Propagation: U-Net uses upsampling and concatenation operations during the decoding phase, which helps in recovering the lost spatial resolution. This dense feature propagation aids in precise segmentation by retaining spatial information. model = unet_learner ( dls , resnet34 ) model . fine_tune ( 10 ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } epoch train_loss valid_loss time 0 1.452725 1.594936 02:43 /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } epoch train_loss valid_loss time 0 1.108694 1.171069 02:42 1 1.014180 1.138221 02:50 2 0.988032 1.126446 02:40 3 0.967991 1.056948 02:40 4 0.885660 0.986956 02:38 5 0.820628 0.929066 02:39 6 0.770862 0.848385 02:38 7 0.713452 0.886977 02:36 8 0.667192 0.956540 02:37 9 0.625676 0.927906 02:37 model . show_results ( max_n = 4 , figsize = ( 7 , 10 )) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } interp = SegmentationInterpretation . from_learner ( model ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } top_losses = interp . top_losses ( 4 , largest = True )[ 1 ] interp . show_results ( top_losses . data ) /* Turns off some styling */ progress { /* gets rid of default border in Firefox and Opera. */ border: none; /* Needs to be in here for Safari polyfill so background images work as expected. */ background-size: auto; } progress:not([value]), progress:not([value])::-webkit-progress-bar { background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px); } .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar { background: #F44336; } The interpreter shows the model makes some reasonable predictions, but there is still room for improvement!","title":"Model: U-Net"},{"location":"computer%20vision/01_image_segmentation_floodnet/#bibliography","text":"body { font-family: Arial, sans-serif; } .citation { margin-bottom: 10px; } .title { font-weight: bold; } .authors { font-style: italic; } .journal { font-weight: bold; } .year { font-weight: bold; } FloodNet: A High Resolution Aerial Imagery Dataset for Post Flood Scene Understanding Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, Robin Murphy arXiv preprint arXiv:2012.02951 2020","title":"Bibliography"},{"location":"computer%20vision/02_YOLO_running_biomechanics/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Human Pose Estimation: YoloV8 Classifying Running Biomechanics In this notebook I aim to explore using YOLOv8 pose data to estimate running biomechanics. Specifically, I am going to limit the scope of the task to classifying what is called the \"initial contact\". The moment the gait cycle begins is when one foot comes in contact with the ground. The cycle lasts until the same foot again comes in contact with the ground. These moments of impact are referred to as intial contact. Goal The goal then is to use yolov8 pose estimation data to build and train a classifier that will detect the point of initial contact. Method Extract pose data for runners using YOLOv8 Using a single runner, clean data and identify initial contact Train a classifier on the data from itertools import cycle import numpy as np import pandas as pd from numpy.linalg import lstsq from ultralytics import YOLO from ultralytics.yolo.utils.plotting import * import cv2 from PIL import Image from IPython.display import Video import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' Pose Detection with YOLOv8 # use pretrained model VERSION = 'yolov8s-pose.pt' # load pretrained model yolo = YOLO ( 'models/' + VERSION ) save_path = '/Users/devindearaujo/Desktop/deep_learning/04_vision/' results = yolo . predict ( source_video , save = True , name = save_path , stream = True , boxes = False , verbose = False , # do not output to terminal ) # loop through results # get keypoint data stream_data = [] for i , frame in enumerate ( results ): orig_img = frame . orig_img kpts = frame . keypoints . data # Keypoints data = { 'frame' : i , 'orig_img' : orig_img , 'kpts' : kpts } stream_data . append ( data ) Results saved to /Users/devindearaujo/Desktop/deep_learning/04_vision3 print ( f 'there are { len ( stream_data ) } frames in video' ) there are 447 frames in video Identifying Initial Contact I could go through the video frame by frame and flag the frames where a runner's foot has touched thr ground, but this seems tedious and if the dataset was any bigger it would quickly become infeasible to do this by hand. Instead, I'm going to build some tooling to assist with this. The idea is simple, using a single runner (maybe the chap on the right hand side: RHS), draw a line across the frame where the treadmill serface is located. Then find or estimate the point where the feet coordinates intersect or collide with this line. The distance between the line and the point of intersection can be measured by calculating the error between the two. Where the error is small, the foot is likely close to the line, we can assume the minima is the point of contact. To do this, we will need to get the coordinates of each runner and store them separately. Sidebar: the point of intersection between a point and a line If I can identify where foot hits the ground, then I can then calssify the onset or initial contact. Once I have this I could probably identify other statistics like the stride or gait! But for now, the initial contact will do. Below is a simple example, building the mechanics that we will use in our real problem. Basically, I have a line, given by two sets of coordinates (x,y pairs) and I have a point that approaches that line. I need to find the point on the line where the distance between the line and points is the smallest (ie lowest error). There are plenty of libraries that can help solve this as well as more rigorous mathematics but the below is good enough for a rough POC. def solve_line ( x , y ): \"\"\" solve line equation by least squares ref: https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html arguments: x,y: two sets of coordinates (x,y pairs) \"\"\" A = np . vstack ([ x , np . ones ( len ( x ))]) . T m , c = np . linalg . lstsq ( A , y , rcond = None )[ 0 ] return m , c # fitting a line through # two sets of coordinates x = np . array ([ 3.9210 , 9.3849 ]) y = np . array ([ 4.1213 , 5.2848 ]) m , c , = solve_line ( x , y ) print ( \"Line Solution is y = {m} x + {c} \" . format ( m = m , c = c )) Line Solution is y = 0.2129431358553416x + 3.2863499643112064 # plot line with random point to illustrate problem # random point point = np . array ([ 6.0 , 4.8 ]) plt . plot ( x , y , 'o' , label = 'Original data' , markersize = 10 ) plt . plot ( x , m * x + c , 'r' , label = 'Fitted line' ) plt . scatter ( * point , c = 'purple' ) plt . legend () plt . show (); The plot above describes the problem visually, we have a line, and a point heading towards it. Where will that point intersect with the line? We need a function that will estimate this given the points current position and a function that will measure the error which will help us select the closest point in our frames. def RMSE ( x , y ): \"\"\" root mean squared error\"\"\" return np . sqrt ( np . sum (( x - y ) ** 2 )) def calc_intersection_point ( m , c , point_coords : np . array ): \"\"\" calculates the expected point of contact between a line given by y=mx+c and a point given as a set of coordinates arguments: point_coords: x and y coords for point m: slope of line c: intercept of line \"\"\" point_x , point_y = point_coords # x-coordinate of the intersection point intersection_x = ( point_y - c ) / m # y-coordinate of the intersection point intersection_y = m * intersection_x + c return intersection_x , intersection_y mock example I've mocked up some data that describes the point (ie y coordinates are decreasing) as it moves closer and closer to the line. We want to find the point on the line where the distance or error is smallest. The line that they are approaching is the line plotted above. descending_coords = np . array ( [[ 6.0 , 5.0 ], [ 6.0 , 4.8 ], [ 6.0 , 4.58 ], # very close to line [ 6.0 , 4.2 ] # passed through line ] ) for points in descending_coords : inter = calc_intersection_point ( m , c , points ) print ( RMSE ( points , inter )) 2.047453743016753 1.1082358659217801 0.0750962011173133 1.7094177653631322 as we can see, the coordinates [6.0, 4.58] are the ones that minimise the error between the line. Back to the main problem Now that we have a method for estimating where on a line a point will collide, we can establish a baseline (e.g. the \"floor\") in the frame. Since the camera here is stationary, it is simple to do this visually using the axis tick marks as a guide. To make this work, we willl need to identify at minimum the x,y coords of the left and right feet. We might want to identify the shin and even thigh. My thinking here is that we can use these coordinates in our classification model down the line. How will we identify these data points? If you look inside the Annotator class in the yolov8 codebase you will see a skeleton attribute, these are the indices of the coordinates that connect the various coordinates together. We can use these to find limbs. to do plot a frame, identify the \"floor\" using a frame where a leg is at it's the lowest. we are using the runner on the RHS for this. draw a baseline along these coordinates and check that the foot intersects with it. we won't need all coords for this, only the feet and maybe shins. plot the \"limb\" ie left or right shin to help visually inspect the data. we can use the skeleton coordinates for this. # plot a frame with keypoints colours = cycle ([ '#e02d86' , '#ff9d00' , '#01718e' ]) skeleton = np . array ([ [ 16 , 14 ], [ 14 , 12 ], [ 15 , 13 ], [ 12 , 13 ], [ 6 , 12 ], [ 6 , 8 ], [ 7 , 9 ], [ 8 , 10 ], [ 2 , 3 ], [ 1 , 2 ], [ 1 , 3 ], [ 2 , 4 ], [ 3 , 5 ], [ 4 , 6 ], [ 5 , 7 ]] ) limb_labels = { 'left_shin' : 2 , 'left_thigh' : 3 , 'left_bicep' : 14 , 'left_forearm' : 6 , 'right_shin' : 0 , 'right_thigh' : 1 , 'right_bicep' : 5 , 'right_forearm' : 6 , } def get_points ( kpts ): \"\"\" return x,y coords from keypoints \"\"\" x = kpts [:, 0 ] y = kpts [:, 1 ] return x , y def plot_keypoints ( frame , plot_ticks = True ): \"\"\" frame: a dict containing keypoints and other data \"\"\" img = frame [ 'orig_img' ] img = cv2 . cvtColor ( img , code = cv2 . COLOR_BGR2RGB ) fig , ax = plt . subplots () ax . imshow ( img ) # each object is a person! for obj in frame [ 'kpts' ]: c = next ( colours ) x , y = get_points ( obj ) ax . scatter ( x , y , s = 2.5 , c = c ) if plot_ticks == False : plt . xticks ([]) plt . yticks ([]) plt . tight_layout (); class Limbs (): def __init__ ( self ): self . skeleton = skeleton self . limb_labels = limb_labels def get_limb_by_name ( self , name : str ): \"return the indices of a limb\" idx = self . limb_labels [ name ] return self . skeleton [ idx ] # Looks like RHS runner touches down on # frame 101 frame = stream_data [ 101 ] # plot keyboints on base image plot_keypoints ( frame ) # plot the \"floor\" # get the line coords x = np . array ([ 900 , 625 ]) y = np . array ([ 650 , 625 ]) m , c , = solve_line ( x , y ) # plot floor points & line plt . plot ( x , y , 'o' , c = 'r' , label = 'Original data' , markersize = 5 ) plt . plot ( x , m * x + c , 'r' , label = 'Fitted line' ) # limb limbs = Limbs () r_shin = frame [ 'kpts' ][ 1 ][ limbs . get_limb_by_name ( 'right_shin' )] plt . plot ( r_shin [:, 0 ], r_shin [:, 1 ], 'ro--' , label = 'limb' , markersize = 5 , linewidth = 1.5 ); Turns out there is a an issue with the model... While the YOLO is able to predict keypoints and bounding boxes, there doesn't seem to be anything that guarantees these keypoints will align with the same person across frames.. Below, in frame 106 the keypoints at index 0 align with the RHS runner, but in frame 107 the keypoints align with the LHS runner.... def plot_image_pairs ( fr1_id , fr2_id , stream_data , runner_id = 1 , figsize = ( 14 , 7 ), s = 4.5 , c = 'r' ): \"\"\" plot two images side by side with keypoints for a single runner \"\"\" f1 , f2 = stream_data [ fr1_id ], stream_data [ fr2_id ] # get keypoint coords x1 , y1 = f1 [ 'kpts' ][ runner_id ][:, 0 ], f1 [ 'kpts' ][ runner_id ][:, 1 ] x2 , y2 = f2 [ 'kpts' ][ runner_id ][:, 0 ], f2 [ 'kpts' ][ runner_id ][:, 1 ] # plot fig , axs = plt . subplots ( 1 , 2 , figsize = figsize ) axs [ 0 ] . imshow ( f1 [ 'orig_img' ]) # don't care about colour conversion axs [ 1 ] . imshow ( f2 [ 'orig_img' ]) axs [ 0 ] . scatter ( x1 , y1 , s = s , c = c ) axs [ 1 ] . scatter ( x2 , y2 , s = s , c = c ) axs [ 0 ] . set_title ( f 'frame { fr1_id } ' ) axs [ 1 ] . set_title ( f 'frame { fr2_id } ' ); plot_image_pairs ( 106 , 107 , stream_data , runner_id = 0 ) Sidebar: points inside polygon... One way to solve this problem would be to... - check each frame, assert whether the keypoints are inside a bounding box that covers the runner of interest - identify the frames where this is not true - for these frames, simply switch the arrays containing the keypoints The nature of the video makes this task somewhat easier than it would be if the camera were not fixed! # dummy example poly = plt . Rectangle ( xy = ( 570 , 10 ), width = 1000 - 570 , height = 700 , fill = False ) points = np . array ([[ 800 , 600 ], [ 800 , 400 ]]) poly . contains_points ( points ) array([ True, True]) # plot bounding box fig , ax = plt . subplots () bbox = plt . Rectangle ( xy = ( 570 , 10 ), width = 1000 - 570 , height = 700 , fill = False , ec = \"#d91ec0\" ) ax . imshow ( f_106 [ 'orig_img' ], cmap = 'gray' ) ax . scatter ( x_106 , y_106 , s = 5 , c = 'r' ) ax . add_patch ( bbox ) ax . set_title ( 'frame 106' ); # check for a single frame points_106 = f_106 [ 'kpts' ][ 0 ][:,: 2 ] . numpy () points_107 = f_107 [ 'kpts' ][ 0 ][:,: 2 ] . numpy () bbox = plt . Rectangle ( xy = ( 570 , 10 ), width = 1000 - 570 , height = 700 , fill = False ) # 106 is inside, 107 is outside. bbox . contains_points ( points_106 ), bbox . contains_points ( points_107 ) (array([ True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]), array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])) # box is empty! np . sum ( bbox . contains_points ( points_107 )) 0 # how often is this happening? res = [] for f in stream_data : f_id = f [ 'frame' ] # get points from obj points_0 = f [ 'kpts' ][ 0 ][:,: 2 ] . numpy () points_1 = f [ 'kpts' ][ 1 ][:,: 2 ] . numpy () contains_points_0 = np . where ( np . sum ( bbox . contains_points ( points_0 )) > 0 , True , False ) . item () contains_points_1 = np . where ( np . sum ( bbox . contains_points ( points_1 )) > 0 , True , False ) . item () res . append ({ 'frame' : f_id , 'contains_points_0' : contains_points_0 , 'contains_points_1' : contains_points_1 }) df = pd . DataFrame ( res ) # only ~30% of the frames in # contains_points_0 column belong to runner on RHS df . contains_points_0 . mean (), df . contains_points_1 . mean () (0.32662192393736017, 0.6733780760626398) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frame contains_points_0 contains_points_1 0 0 False True 1 1 True False 2 2 True False 3 3 False True 4 4 False True swapping problematic points # put it all together # loop through stream data # if frame id is in the list # of problematic frames # swap data stream_data_fix = [] frames_to_swap = df [ df . contains_points_1 == False ] . frame . values for f in stream_data : # get id, image & keypoints f_id = f [ 'frame' ] orig_img = f [ 'orig_img' ] kpts = f [ 'kpts' ] # get object points (ie person 0 and 1) points_0 = kpts [ 0 ] points_1 = kpts [ 1 ] if f_id in frames_to_swap : # swap points new_kpts = torch . tensor ( np . array ([ points_1 . numpy (), points_0 . numpy ()])) # dims should match assert new_kpts . shape == torch . Size ([ 2 , 17 , 3 ]) data = { 'frame' : f_id , 'orig_img' : orig_img , 'kpts' : new_kpts } else : data = { 'frame' : f_id , 'orig_img' : orig_img , 'kpts' : kpts } stream_data_fix . append ( data ) # sort so frame ids match stream_data_fix = sorted ( stream_data_fix , key = lambda x : x [ 'frame' ]) type ( stream_data ), len ( stream_data ), type ( stream_data_fix ), len ( stream_data_fix ) (list, 447, list, 447) plot_image_pairs ( 106 , 107 , stream_data_fix , 1 ) After the swap, all points belonging to the RHS runner are now indexed with id=1 Back to the main problem Extracting leg data. Specifically the foot and shin coordinatses. Calculating error between foot and floor building a table with the data we care about def calc_leg_floor_error ( shin ): \"\"\" return the index and error of the limb closest to the floor \"\"\" out = [] for i , points in enumerate ( shin ): inter = calc_intersection_point ( m , c , points ) e = RMSE ( points , inter ) #out.append({i:e}) out . append ({ 'idx' : i , 'error' : e }) return min ( out , key = lambda x : list ( x . values ())[ 0 ]) shin = stream_data_fix [ 0 ][ 'kpts' ][ 0 ][ limbs . get_limb_by_name ( 'left_shin' )][:,: 2 ] . numpy () calc_leg_floor_error ( shin ) {'idx': 0, 'error': 237.58837890625261} RHS_runner_id = 1 limbs = Limbs () res = [] for f in stream_data_fix : # data f_id = f [ 'frame' ] orig_img = f [ 'orig_img' ][[ RHS_runner_id ]] kpts = f [ 'kpts' ][ RHS_runner_id ] # get leg coordinates l_shin = kpts [ limbs . get_limb_by_name ( 'left_shin' )][:,: 2 ] . numpy () r_shin = kpts [ limbs . get_limb_by_name ( 'right_shin' )][:,: 2 ] . numpy () # get error for closest val l_shin_err = calc_leg_floor_error ( l_shin ) r_shin_err = calc_leg_floor_error ( r_shin ) data = { 'frame' : f_id , 'orig_img' : orig_img , 'kpts' : kpts , 'l_shin' : l_shin , 'r_shin' : r_shin , 'l_shin_err_id' : l_shin_err [ 'idx' ], 'r_shin_err_id' : r_shin_err [ 'idx' ], 'l_shin_err' : l_shin_err [ 'error' ], 'r_shin_err' : r_shin_err [ 'error' ], } res . append ( data ) df_legs = pd . DataFrame ( res ) df_legs . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frame orig_img kpts l_shin r_shin l_shin_err_id r_shin_err_id l_shin_err r_shin_err 0 0 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(721.7922), tensor(148.8591), tensor(0... [[940.49304, 509.91675], [804.28107, 435.70178]] [[736.6238, 616.66504], [762.37585, 461.6278]] 0 0 1581.408813 203.308350 1 1 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(726.7953), tensor(159.0453), tensor(0... [[933.1233, 482.19775], [783.6169, 438.49356]] [[743.5001, 630.4986], [760.86804, 479.3217]] 0 0 1878.947998 58.015564 Error distribution looking at the below plots, we can see that the lowest error values occur below the 25% quartile. We can use this to filter the data and roughly categorise the initial contact. This is just a starting point. Some manual curation afterwards will be done to clean up the data properly. Given this is only a small dataset (<500 rows) this is feasible, but for large production tables I would refine this method and implement something more robust! fig , axs = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) axs [ 0 ] . hist ( df_legs [ 'l_shin_err' ], bins = 40 ) axs [ 1 ] . hist ( df_legs [ 'r_shin_err' ], bins = 40 ) axs [ 0 ] . set_title ( f \"error distribution (left), Q(25,50)\" ) axs [ 1 ] . set_title ( f \"error distribution (right), Q(25,50)\" ) quantiles = [ .25 , .50 ] colors = [ 'orange' , 'red' , 'orange' ] for q , c in zip ( quantiles , colors ): lq = df_legs [ 'l_shin_err' ] . quantile ( q ) rq = df_legs [ 'r_shin_err' ] . quantile ( q ) axs [ 0 ] . axvline ( lq , 0 , 1 , color = c , ls = '--' ) axs [ 0 ] . text ( lq + 30 , 40 , f ' { lq : .1f } ' , fontsize = 'x-small' ) axs [ 1 ] . axvline ( rq , 0 , 1 , color = c , ls = '--' ) axs [ 1 ] . text ( rq + 30 , 40 , f ' { rq : .1f } ' , fontsize = 'x-small' ); Manually counting the initial contact points for the RHS runner gives 23 for the right leg and 22 for the left. This is is a good enough guide for how many points to expect in this small dtaset. I have taken a wider percentile (10th) that includes more than 23 points, because it is likely that two points close together will have very similar errors. lq = df_legs [ 'l_shin_err' ] . quantile ( .10 ) rq = df_legs [ 'r_shin_err' ] . quantile ( .10 ) df_legs [ 'l_initial_contact' ] = np . where ( df_legs [ 'l_shin_err' ] < lq , True , False ) df_legs [ 'r_initial_contact' ] = np . where ( df_legs [ 'r_shin_err' ] < rq , True , False ) # right leg initial contact 23 times # left leg initial contact 22 times df_legs [ 'l_initial_contact' ] . value_counts (), df_legs [ 'r_initial_contact' ] . value_counts () (l_initial_contact False 402 True 45 Name: count, dtype: int64, r_initial_contact False 402 True 45 Name: count, dtype: int64) # check a few frames plot_image_pairs ( 16 , 34 , stream_data_fix , 1 ) plot_image_pairs ( 359 , 399 , stream_data_fix , 1 ) Not bad at all! As expected, if you look at the data you will see that there examples where two sequential frames have been flagged as initial contact - this is because these frames are close enough that both have low error. plot_image_pairs ( 438 , 437 , stream_data_fix , 1 ) The data was exported and some manual cleaning was done. Specifically, ensuring only a single point of contact was flagged for each leg as it hits the treadmill. df_leg_adj = pd . read_csv ( 'df_leg_adjusted.csv' ) df_leg_adj = df_legs [[ 'frame' , 'orig_img' , 'kpts' , 'l_shin' , 'r_shin' , 'l_shin_err_id' , 'r_shin_err_id' ]] . merge ( df_leg_adj , how = 'left' , on = 'frame' ) df_leg_adj . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frame orig_img kpts l_shin r_shin l_shin_err_id r_shin_err_id l_initial_contact r_initial_contact l_shin_err r_shin_err 0 0 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(721.7922), tensor(148.8591), tensor(0... [[940.49304, 509.91675], [804.28107, 435.70178]] [[736.6238, 616.66504], [762.37585, 461.6278]] 0 0 False False 1581 203 1 1 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(726.7953), tensor(159.0453), tensor(0... [[933.1233, 482.19775], [783.6169, 438.49356]] [[743.5001, 630.4986], [760.86804, 479.3217]] 0 0 False False 1879 58 Building a classifer def get_shin_coords ( x , col = 0 ): return x [ 0 ][ col ] X_df = df_leg_adj shins = [ 'l_shin' , 'r_shin' ] X_data = [] for s in shins : x_coord = X_df [ s ] . apply ( get_shin_coords , col = 0 ) #.values y_coord = X_df [ s ] . apply ( get_shin_coords , col = 1 ) #.values X_data . append ( x_coord ) X_data . append ( y_coord ) # X , y X = np . array ( X_data ) . T #y_l = np.where(df_leg_adj['l_initial_contact'] == True, 1,0) #y_r = np.where(df_leg_adj['r_initial_contact'] == True, 1,0) # let's predict both left and right leg # to improve data sparsity and because # we know which leg is which based on skeleton y = np . sum ( df_leg_adj [[ 'l_initial_contact' , 'r_initial_contact' ]], axis = 1 ) . values RandomForestClassifier w GridSearchCV GridSearch is a process of performing hyperparameter tuning in order to find optimal values for a machine learing model. Generally speaking, there is no way to know in advance the best parameters for a model in a given problem setting. GridSearch helps by providing an automated way to search through a defined parameter space, and returns the optimal values. The problem I am facing with this data is that the event I are interested in, happens very infrequently. The majority class in this data would be every frame where the RHS runner's foot is not making contact with the treadmill. Basically, the class labels in this dataset are very imbalanced. There are many strategies to help deal with this. The method that improved model fit in this example was a balanced class weighting strategy. Here, the values of y are used to automatically adjust weights inversely proportional to the class frequencies. The balanced_subsample argument means this is performed for each bootstrap sample for every tree grown. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV # training spit X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 123 ) param_grid = [ { 'n_estimators' : [ 5 , 10 , 20 ], 'max_depth' :[ 2 , 3 , 6 ], 'class_weight' :[ 'balanced' , 'balanced_subsample' ] }] grid = GridSearchCV ( estimator = RandomForestClassifier (), param_grid = param_grid , refit = True , n_jobs =- 1 , verbose = 1 , ) grid . fit ( X_train , y_train ) # print best parameter after tuning print ( grid . best_params_ , ' \\n ' ) # TEST test_accuracy = grid . score ( X_test , y_test ) print ( 'Accuracy of the best parameters using the inner CV of' ) print ( f 'the random search: { grid . best_score_ : .3f } ' ) print ( f 'Accuracy on test set: { test_accuracy : .3f } ' ) Fitting 5 folds for each of 18 candidates, totalling 90 fits {'class_weight': 'balanced', 'max_depth': 6, 'n_estimators': 5} Accuracy of the best parameters using the inner CV of the random search: 0.916 Accuracy on test set: 0.900 fit the model using the best params #take best params clf = RandomForestClassifier ( ** grid . best_params_ , random_state = 123 ) clf . fit ( X_train , y_train ) clf . score ( X_train , y_train ), clf . score ( X_test , y_test ) (0.9607843137254902, 0.9) clf . predict ( X_test ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]) y_test array([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]) MLP out of interest, see how an MLP does. The random forrest did a pretty good job. It certainly provides a much more efficient method for identifyign the inital contact points than the functional methods I built above to help create the data set in the first place. import torch import torch.nn as nn # convert to tensors X_train = torch . from_numpy ( X_train ) X_test = torch . from_numpy ( X_test ) y_train = torch . from_numpy ( y_train ) y_test = torch . from_numpy ( y_test ) def train_mlp_classifier ( model , optim , criterion , X_train , y_train , X_test , y_test , num_epochs = 100 ): for epoch in range ( num_epochs ): # zero grad model . train () optim . zero_grad () # forward, loss # ------------- # preds -> (n examples, n classes) (357, 2) preds = model ( X_train ) loss = criterion ( preds , y_train ) # backprop, step loss . backward () optim . step () # validate acc = eval_mlp_classifier ( model , X_test , y_test ) log = f 'Epoch: { epoch : 03d } , Train Loss: { loss : .3f } , Val Acc: { acc : .3f } ' if epoch % 100 == 0 : print ( log ) def eval_mlp_classifier ( model , X_test , y_test ): model . eval () out = model ( X_test ) . argmax ( dim = 1 ) # get preds correct = ( out == y_test ) . sum () acc = int ( correct ) / len ( y_test ) return acc class MLP ( nn . Module ): \"\"\" MLP classifier -------------- n_input: number input features n_output: number of classes n_hidden: number of hidden layers \"\"\" def __init__ ( self , n_input = 4 , n_output = 2 , n_hidden = 64 ): super () . __init__ () self . model = nn . Sequential ( nn . Linear ( n_input , n_hidden * 8 ), nn . ReLU (), nn . Dropout ( 0.2 ), nn . Linear ( n_hidden * 8 , n_hidden * 4 ), nn . ReLU (), #nn.Dropout(0.2), nn . Linear ( n_hidden * 4 , n_output ), ) def forward ( self , x ): return self . model ( x ) DEVICE = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # init mlp = MLP () . to ( DEVICE ) # optimiser and loss optim = torch . optim . Adam ( mlp . parameters (), lr = 0.001 , weight_decay = .05 ) criterion = nn . CrossEntropyLoss ( weight = torch . tensor ([ .2 , 1. ])) train_mlp_classifier ( mlp , optim , criterion , X_train , y_train , X_test , y_test , num_epochs = 300 ) Epoch: 000, Train Loss: 21.817, Val Acc: 0.133 Epoch: 100, Train Loss: 1.545, Val Acc: 0.878 Epoch: 200, Train Loss: 0.540, Val Acc: 0.889 mlp ( X_test ) . argmax ( 1 ) tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]) y_test tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]) Not bad, maybe more consistent that the random forrest but still misses a few points of contact. Summary What a journey! This notebook was an exploration into using pose data from YOLOv8. Off the shelf, YOLO is a very impressive model but all you ever see online are videos of object detection on street scenes, people dancing, excersising or holding cups and pens up to their webcams. I wanted to understand how useable the data from YOLO actaully is and how it could use to estimate something specific. Classifying a runners gait is nothing new, but the idea here was to illustrate how much thought, preparation and work there is in exploring, cleaning and preparing the pose data for a downstream task. Where to next? There are a few things that I would have liked to try but this notebook got a bit big to fit it all in! I would have liked to see how the model performs on the second runner in the frame, this would help in understanding whether the model is able to generalise. I suspect that the model wouldn't perform well on the LHS runner because the magnitude of the X coordinates would be smaller that for the RHS runner due to their position on screen. A potential workaround would be to perform some simple data augmentation on the RHS coordinates, this would hopefully help the model to learn to be invariant to the scale. Another idea could be to include all keypoints for the RHS runner during training or even represent the data as a graph and apply graph representation learning techniques to the data, specifically, this could mean that we can encode in the feet or leg data information propagated from the other keypoints in the body. Maybe this would encode more useful information and assist better in the task. I'd also like to try these models on some more dynamic video content, like someone running outside or around a track or being followed with a handheld camera. references physio-pedia running biomechanics pexels woman and man on treadmill","title":"02 YOLO running biomechanics"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#human-pose-estimation-yolov8","text":"","title":"Human Pose Estimation: YoloV8"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#classifying-running-biomechanics","text":"In this notebook I aim to explore using YOLOv8 pose data to estimate running biomechanics. Specifically, I am going to limit the scope of the task to classifying what is called the \"initial contact\". The moment the gait cycle begins is when one foot comes in contact with the ground. The cycle lasts until the same foot again comes in contact with the ground. These moments of impact are referred to as intial contact.","title":"Classifying Running Biomechanics"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#goal","text":"The goal then is to use yolov8 pose estimation data to build and train a classifier that will detect the point of initial contact.","title":"Goal"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#method","text":"Extract pose data for runners using YOLOv8 Using a single runner, clean data and identify initial contact Train a classifier on the data from itertools import cycle import numpy as np import pandas as pd from numpy.linalg import lstsq from ultralytics import YOLO from ultralytics.yolo.utils.plotting import * import cv2 from PIL import Image from IPython.display import Video import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina'","title":"Method"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#pose-detection-with-yolov8","text":"# use pretrained model VERSION = 'yolov8s-pose.pt' # load pretrained model yolo = YOLO ( 'models/' + VERSION ) save_path = '/Users/devindearaujo/Desktop/deep_learning/04_vision/' results = yolo . predict ( source_video , save = True , name = save_path , stream = True , boxes = False , verbose = False , # do not output to terminal ) # loop through results # get keypoint data stream_data = [] for i , frame in enumerate ( results ): orig_img = frame . orig_img kpts = frame . keypoints . data # Keypoints data = { 'frame' : i , 'orig_img' : orig_img , 'kpts' : kpts } stream_data . append ( data ) Results saved to /Users/devindearaujo/Desktop/deep_learning/04_vision3 print ( f 'there are { len ( stream_data ) } frames in video' ) there are 447 frames in video","title":"Pose Detection with YOLOv8"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#identifying-initial-contact","text":"I could go through the video frame by frame and flag the frames where a runner's foot has touched thr ground, but this seems tedious and if the dataset was any bigger it would quickly become infeasible to do this by hand. Instead, I'm going to build some tooling to assist with this. The idea is simple, using a single runner (maybe the chap on the right hand side: RHS), draw a line across the frame where the treadmill serface is located. Then find or estimate the point where the feet coordinates intersect or collide with this line. The distance between the line and the point of intersection can be measured by calculating the error between the two. Where the error is small, the foot is likely close to the line, we can assume the minima is the point of contact. To do this, we will need to get the coordinates of each runner and store them separately.","title":"Identifying Initial Contact"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#sidebar-the-point-of-intersection-between-a-point-and-a-line","text":"If I can identify where foot hits the ground, then I can then calssify the onset or initial contact. Once I have this I could probably identify other statistics like the stride or gait! But for now, the initial contact will do. Below is a simple example, building the mechanics that we will use in our real problem. Basically, I have a line, given by two sets of coordinates (x,y pairs) and I have a point that approaches that line. I need to find the point on the line where the distance between the line and points is the smallest (ie lowest error). There are plenty of libraries that can help solve this as well as more rigorous mathematics but the below is good enough for a rough POC. def solve_line ( x , y ): \"\"\" solve line equation by least squares ref: https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html arguments: x,y: two sets of coordinates (x,y pairs) \"\"\" A = np . vstack ([ x , np . ones ( len ( x ))]) . T m , c = np . linalg . lstsq ( A , y , rcond = None )[ 0 ] return m , c # fitting a line through # two sets of coordinates x = np . array ([ 3.9210 , 9.3849 ]) y = np . array ([ 4.1213 , 5.2848 ]) m , c , = solve_line ( x , y ) print ( \"Line Solution is y = {m} x + {c} \" . format ( m = m , c = c )) Line Solution is y = 0.2129431358553416x + 3.2863499643112064 # plot line with random point to illustrate problem # random point point = np . array ([ 6.0 , 4.8 ]) plt . plot ( x , y , 'o' , label = 'Original data' , markersize = 10 ) plt . plot ( x , m * x + c , 'r' , label = 'Fitted line' ) plt . scatter ( * point , c = 'purple' ) plt . legend () plt . show (); The plot above describes the problem visually, we have a line, and a point heading towards it. Where will that point intersect with the line? We need a function that will estimate this given the points current position and a function that will measure the error which will help us select the closest point in our frames. def RMSE ( x , y ): \"\"\" root mean squared error\"\"\" return np . sqrt ( np . sum (( x - y ) ** 2 )) def calc_intersection_point ( m , c , point_coords : np . array ): \"\"\" calculates the expected point of contact between a line given by y=mx+c and a point given as a set of coordinates arguments: point_coords: x and y coords for point m: slope of line c: intercept of line \"\"\" point_x , point_y = point_coords # x-coordinate of the intersection point intersection_x = ( point_y - c ) / m # y-coordinate of the intersection point intersection_y = m * intersection_x + c return intersection_x , intersection_y mock example I've mocked up some data that describes the point (ie y coordinates are decreasing) as it moves closer and closer to the line. We want to find the point on the line where the distance or error is smallest. The line that they are approaching is the line plotted above. descending_coords = np . array ( [[ 6.0 , 5.0 ], [ 6.0 , 4.8 ], [ 6.0 , 4.58 ], # very close to line [ 6.0 , 4.2 ] # passed through line ] ) for points in descending_coords : inter = calc_intersection_point ( m , c , points ) print ( RMSE ( points , inter )) 2.047453743016753 1.1082358659217801 0.0750962011173133 1.7094177653631322 as we can see, the coordinates [6.0, 4.58] are the ones that minimise the error between the line.","title":"Sidebar: the point of intersection between a point and a line"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#back-to-the-main-problem","text":"Now that we have a method for estimating where on a line a point will collide, we can establish a baseline (e.g. the \"floor\") in the frame. Since the camera here is stationary, it is simple to do this visually using the axis tick marks as a guide. To make this work, we willl need to identify at minimum the x,y coords of the left and right feet. We might want to identify the shin and even thigh. My thinking here is that we can use these coordinates in our classification model down the line. How will we identify these data points? If you look inside the Annotator class in the yolov8 codebase you will see a skeleton attribute, these are the indices of the coordinates that connect the various coordinates together. We can use these to find limbs.","title":"Back to the main problem"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#to-do","text":"plot a frame, identify the \"floor\" using a frame where a leg is at it's the lowest. we are using the runner on the RHS for this. draw a baseline along these coordinates and check that the foot intersects with it. we won't need all coords for this, only the feet and maybe shins. plot the \"limb\" ie left or right shin to help visually inspect the data. we can use the skeleton coordinates for this. # plot a frame with keypoints colours = cycle ([ '#e02d86' , '#ff9d00' , '#01718e' ]) skeleton = np . array ([ [ 16 , 14 ], [ 14 , 12 ], [ 15 , 13 ], [ 12 , 13 ], [ 6 , 12 ], [ 6 , 8 ], [ 7 , 9 ], [ 8 , 10 ], [ 2 , 3 ], [ 1 , 2 ], [ 1 , 3 ], [ 2 , 4 ], [ 3 , 5 ], [ 4 , 6 ], [ 5 , 7 ]] ) limb_labels = { 'left_shin' : 2 , 'left_thigh' : 3 , 'left_bicep' : 14 , 'left_forearm' : 6 , 'right_shin' : 0 , 'right_thigh' : 1 , 'right_bicep' : 5 , 'right_forearm' : 6 , } def get_points ( kpts ): \"\"\" return x,y coords from keypoints \"\"\" x = kpts [:, 0 ] y = kpts [:, 1 ] return x , y def plot_keypoints ( frame , plot_ticks = True ): \"\"\" frame: a dict containing keypoints and other data \"\"\" img = frame [ 'orig_img' ] img = cv2 . cvtColor ( img , code = cv2 . COLOR_BGR2RGB ) fig , ax = plt . subplots () ax . imshow ( img ) # each object is a person! for obj in frame [ 'kpts' ]: c = next ( colours ) x , y = get_points ( obj ) ax . scatter ( x , y , s = 2.5 , c = c ) if plot_ticks == False : plt . xticks ([]) plt . yticks ([]) plt . tight_layout (); class Limbs (): def __init__ ( self ): self . skeleton = skeleton self . limb_labels = limb_labels def get_limb_by_name ( self , name : str ): \"return the indices of a limb\" idx = self . limb_labels [ name ] return self . skeleton [ idx ] # Looks like RHS runner touches down on # frame 101 frame = stream_data [ 101 ] # plot keyboints on base image plot_keypoints ( frame ) # plot the \"floor\" # get the line coords x = np . array ([ 900 , 625 ]) y = np . array ([ 650 , 625 ]) m , c , = solve_line ( x , y ) # plot floor points & line plt . plot ( x , y , 'o' , c = 'r' , label = 'Original data' , markersize = 5 ) plt . plot ( x , m * x + c , 'r' , label = 'Fitted line' ) # limb limbs = Limbs () r_shin = frame [ 'kpts' ][ 1 ][ limbs . get_limb_by_name ( 'right_shin' )] plt . plot ( r_shin [:, 0 ], r_shin [:, 1 ], 'ro--' , label = 'limb' , markersize = 5 , linewidth = 1.5 ); Turns out there is a an issue with the model... While the YOLO is able to predict keypoints and bounding boxes, there doesn't seem to be anything that guarantees these keypoints will align with the same person across frames.. Below, in frame 106 the keypoints at index 0 align with the RHS runner, but in frame 107 the keypoints align with the LHS runner.... def plot_image_pairs ( fr1_id , fr2_id , stream_data , runner_id = 1 , figsize = ( 14 , 7 ), s = 4.5 , c = 'r' ): \"\"\" plot two images side by side with keypoints for a single runner \"\"\" f1 , f2 = stream_data [ fr1_id ], stream_data [ fr2_id ] # get keypoint coords x1 , y1 = f1 [ 'kpts' ][ runner_id ][:, 0 ], f1 [ 'kpts' ][ runner_id ][:, 1 ] x2 , y2 = f2 [ 'kpts' ][ runner_id ][:, 0 ], f2 [ 'kpts' ][ runner_id ][:, 1 ] # plot fig , axs = plt . subplots ( 1 , 2 , figsize = figsize ) axs [ 0 ] . imshow ( f1 [ 'orig_img' ]) # don't care about colour conversion axs [ 1 ] . imshow ( f2 [ 'orig_img' ]) axs [ 0 ] . scatter ( x1 , y1 , s = s , c = c ) axs [ 1 ] . scatter ( x2 , y2 , s = s , c = c ) axs [ 0 ] . set_title ( f 'frame { fr1_id } ' ) axs [ 1 ] . set_title ( f 'frame { fr2_id } ' ); plot_image_pairs ( 106 , 107 , stream_data , runner_id = 0 )","title":"to do"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#sidebar-points-inside-polygon","text":"One way to solve this problem would be to... - check each frame, assert whether the keypoints are inside a bounding box that covers the runner of interest - identify the frames where this is not true - for these frames, simply switch the arrays containing the keypoints The nature of the video makes this task somewhat easier than it would be if the camera were not fixed! # dummy example poly = plt . Rectangle ( xy = ( 570 , 10 ), width = 1000 - 570 , height = 700 , fill = False ) points = np . array ([[ 800 , 600 ], [ 800 , 400 ]]) poly . contains_points ( points ) array([ True, True]) # plot bounding box fig , ax = plt . subplots () bbox = plt . Rectangle ( xy = ( 570 , 10 ), width = 1000 - 570 , height = 700 , fill = False , ec = \"#d91ec0\" ) ax . imshow ( f_106 [ 'orig_img' ], cmap = 'gray' ) ax . scatter ( x_106 , y_106 , s = 5 , c = 'r' ) ax . add_patch ( bbox ) ax . set_title ( 'frame 106' ); # check for a single frame points_106 = f_106 [ 'kpts' ][ 0 ][:,: 2 ] . numpy () points_107 = f_107 [ 'kpts' ][ 0 ][:,: 2 ] . numpy () bbox = plt . Rectangle ( xy = ( 570 , 10 ), width = 1000 - 570 , height = 700 , fill = False ) # 106 is inside, 107 is outside. bbox . contains_points ( points_106 ), bbox . contains_points ( points_107 ) (array([ True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]), array([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])) # box is empty! np . sum ( bbox . contains_points ( points_107 )) 0 # how often is this happening? res = [] for f in stream_data : f_id = f [ 'frame' ] # get points from obj points_0 = f [ 'kpts' ][ 0 ][:,: 2 ] . numpy () points_1 = f [ 'kpts' ][ 1 ][:,: 2 ] . numpy () contains_points_0 = np . where ( np . sum ( bbox . contains_points ( points_0 )) > 0 , True , False ) . item () contains_points_1 = np . where ( np . sum ( bbox . contains_points ( points_1 )) > 0 , True , False ) . item () res . append ({ 'frame' : f_id , 'contains_points_0' : contains_points_0 , 'contains_points_1' : contains_points_1 }) df = pd . DataFrame ( res ) # only ~30% of the frames in # contains_points_0 column belong to runner on RHS df . contains_points_0 . mean (), df . contains_points_1 . mean () (0.32662192393736017, 0.6733780760626398) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frame contains_points_0 contains_points_1 0 0 False True 1 1 True False 2 2 True False 3 3 False True 4 4 False True","title":"Sidebar: points inside polygon..."},{"location":"computer%20vision/02_YOLO_running_biomechanics/#swapping-problematic-points","text":"# put it all together # loop through stream data # if frame id is in the list # of problematic frames # swap data stream_data_fix = [] frames_to_swap = df [ df . contains_points_1 == False ] . frame . values for f in stream_data : # get id, image & keypoints f_id = f [ 'frame' ] orig_img = f [ 'orig_img' ] kpts = f [ 'kpts' ] # get object points (ie person 0 and 1) points_0 = kpts [ 0 ] points_1 = kpts [ 1 ] if f_id in frames_to_swap : # swap points new_kpts = torch . tensor ( np . array ([ points_1 . numpy (), points_0 . numpy ()])) # dims should match assert new_kpts . shape == torch . Size ([ 2 , 17 , 3 ]) data = { 'frame' : f_id , 'orig_img' : orig_img , 'kpts' : new_kpts } else : data = { 'frame' : f_id , 'orig_img' : orig_img , 'kpts' : kpts } stream_data_fix . append ( data ) # sort so frame ids match stream_data_fix = sorted ( stream_data_fix , key = lambda x : x [ 'frame' ]) type ( stream_data ), len ( stream_data ), type ( stream_data_fix ), len ( stream_data_fix ) (list, 447, list, 447) plot_image_pairs ( 106 , 107 , stream_data_fix , 1 ) After the swap, all points belonging to the RHS runner are now indexed with id=1","title":"swapping problematic points"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#back-to-the-main-problem_1","text":"Extracting leg data. Specifically the foot and shin coordinatses. Calculating error between foot and floor building a table with the data we care about def calc_leg_floor_error ( shin ): \"\"\" return the index and error of the limb closest to the floor \"\"\" out = [] for i , points in enumerate ( shin ): inter = calc_intersection_point ( m , c , points ) e = RMSE ( points , inter ) #out.append({i:e}) out . append ({ 'idx' : i , 'error' : e }) return min ( out , key = lambda x : list ( x . values ())[ 0 ]) shin = stream_data_fix [ 0 ][ 'kpts' ][ 0 ][ limbs . get_limb_by_name ( 'left_shin' )][:,: 2 ] . numpy () calc_leg_floor_error ( shin ) {'idx': 0, 'error': 237.58837890625261} RHS_runner_id = 1 limbs = Limbs () res = [] for f in stream_data_fix : # data f_id = f [ 'frame' ] orig_img = f [ 'orig_img' ][[ RHS_runner_id ]] kpts = f [ 'kpts' ][ RHS_runner_id ] # get leg coordinates l_shin = kpts [ limbs . get_limb_by_name ( 'left_shin' )][:,: 2 ] . numpy () r_shin = kpts [ limbs . get_limb_by_name ( 'right_shin' )][:,: 2 ] . numpy () # get error for closest val l_shin_err = calc_leg_floor_error ( l_shin ) r_shin_err = calc_leg_floor_error ( r_shin ) data = { 'frame' : f_id , 'orig_img' : orig_img , 'kpts' : kpts , 'l_shin' : l_shin , 'r_shin' : r_shin , 'l_shin_err_id' : l_shin_err [ 'idx' ], 'r_shin_err_id' : r_shin_err [ 'idx' ], 'l_shin_err' : l_shin_err [ 'error' ], 'r_shin_err' : r_shin_err [ 'error' ], } res . append ( data ) df_legs = pd . DataFrame ( res ) df_legs . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frame orig_img kpts l_shin r_shin l_shin_err_id r_shin_err_id l_shin_err r_shin_err 0 0 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(721.7922), tensor(148.8591), tensor(0... [[940.49304, 509.91675], [804.28107, 435.70178]] [[736.6238, 616.66504], [762.37585, 461.6278]] 0 0 1581.408813 203.308350 1 1 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(726.7953), tensor(159.0453), tensor(0... [[933.1233, 482.19775], [783.6169, 438.49356]] [[743.5001, 630.4986], [760.86804, 479.3217]] 0 0 1878.947998 58.015564","title":"Back to the main problem"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#error-distribution","text":"looking at the below plots, we can see that the lowest error values occur below the 25% quartile. We can use this to filter the data and roughly categorise the initial contact. This is just a starting point. Some manual curation afterwards will be done to clean up the data properly. Given this is only a small dataset (<500 rows) this is feasible, but for large production tables I would refine this method and implement something more robust! fig , axs = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) axs [ 0 ] . hist ( df_legs [ 'l_shin_err' ], bins = 40 ) axs [ 1 ] . hist ( df_legs [ 'r_shin_err' ], bins = 40 ) axs [ 0 ] . set_title ( f \"error distribution (left), Q(25,50)\" ) axs [ 1 ] . set_title ( f \"error distribution (right), Q(25,50)\" ) quantiles = [ .25 , .50 ] colors = [ 'orange' , 'red' , 'orange' ] for q , c in zip ( quantiles , colors ): lq = df_legs [ 'l_shin_err' ] . quantile ( q ) rq = df_legs [ 'r_shin_err' ] . quantile ( q ) axs [ 0 ] . axvline ( lq , 0 , 1 , color = c , ls = '--' ) axs [ 0 ] . text ( lq + 30 , 40 , f ' { lq : .1f } ' , fontsize = 'x-small' ) axs [ 1 ] . axvline ( rq , 0 , 1 , color = c , ls = '--' ) axs [ 1 ] . text ( rq + 30 , 40 , f ' { rq : .1f } ' , fontsize = 'x-small' ); Manually counting the initial contact points for the RHS runner gives 23 for the right leg and 22 for the left. This is is a good enough guide for how many points to expect in this small dtaset. I have taken a wider percentile (10th) that includes more than 23 points, because it is likely that two points close together will have very similar errors. lq = df_legs [ 'l_shin_err' ] . quantile ( .10 ) rq = df_legs [ 'r_shin_err' ] . quantile ( .10 ) df_legs [ 'l_initial_contact' ] = np . where ( df_legs [ 'l_shin_err' ] < lq , True , False ) df_legs [ 'r_initial_contact' ] = np . where ( df_legs [ 'r_shin_err' ] < rq , True , False ) # right leg initial contact 23 times # left leg initial contact 22 times df_legs [ 'l_initial_contact' ] . value_counts (), df_legs [ 'r_initial_contact' ] . value_counts () (l_initial_contact False 402 True 45 Name: count, dtype: int64, r_initial_contact False 402 True 45 Name: count, dtype: int64) # check a few frames plot_image_pairs ( 16 , 34 , stream_data_fix , 1 ) plot_image_pairs ( 359 , 399 , stream_data_fix , 1 ) Not bad at all! As expected, if you look at the data you will see that there examples where two sequential frames have been flagged as initial contact - this is because these frames are close enough that both have low error. plot_image_pairs ( 438 , 437 , stream_data_fix , 1 ) The data was exported and some manual cleaning was done. Specifically, ensuring only a single point of contact was flagged for each leg as it hits the treadmill. df_leg_adj = pd . read_csv ( 'df_leg_adjusted.csv' ) df_leg_adj = df_legs [[ 'frame' , 'orig_img' , 'kpts' , 'l_shin' , 'r_shin' , 'l_shin_err_id' , 'r_shin_err_id' ]] . merge ( df_leg_adj , how = 'left' , on = 'frame' ) df_leg_adj . head ( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } frame orig_img kpts l_shin r_shin l_shin_err_id r_shin_err_id l_initial_contact r_initial_contact l_shin_err r_shin_err 0 0 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(721.7922), tensor(148.8591), tensor(0... [[940.49304, 509.91675], [804.28107, 435.70178]] [[736.6238, 616.66504], [762.37585, 461.6278]] 0 0 False False 1581 203 1 1 [[[73, 45, 37], [73, 45, 37], [73, 45, 37], [7... [[tensor(726.7953), tensor(159.0453), tensor(0... [[933.1233, 482.19775], [783.6169, 438.49356]] [[743.5001, 630.4986], [760.86804, 479.3217]] 0 0 False False 1879 58","title":"Error distribution"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#building-a-classifer","text":"def get_shin_coords ( x , col = 0 ): return x [ 0 ][ col ] X_df = df_leg_adj shins = [ 'l_shin' , 'r_shin' ] X_data = [] for s in shins : x_coord = X_df [ s ] . apply ( get_shin_coords , col = 0 ) #.values y_coord = X_df [ s ] . apply ( get_shin_coords , col = 1 ) #.values X_data . append ( x_coord ) X_data . append ( y_coord ) # X , y X = np . array ( X_data ) . T #y_l = np.where(df_leg_adj['l_initial_contact'] == True, 1,0) #y_r = np.where(df_leg_adj['r_initial_contact'] == True, 1,0) # let's predict both left and right leg # to improve data sparsity and because # we know which leg is which based on skeleton y = np . sum ( df_leg_adj [[ 'l_initial_contact' , 'r_initial_contact' ]], axis = 1 ) . values","title":"Building a classifer"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#randomforestclassifier-w-gridsearchcv","text":"GridSearch is a process of performing hyperparameter tuning in order to find optimal values for a machine learing model. Generally speaking, there is no way to know in advance the best parameters for a model in a given problem setting. GridSearch helps by providing an automated way to search through a defined parameter space, and returns the optimal values. The problem I am facing with this data is that the event I are interested in, happens very infrequently. The majority class in this data would be every frame where the RHS runner's foot is not making contact with the treadmill. Basically, the class labels in this dataset are very imbalanced. There are many strategies to help deal with this. The method that improved model fit in this example was a balanced class weighting strategy. Here, the values of y are used to automatically adjust weights inversely proportional to the class frequencies. The balanced_subsample argument means this is performed for each bootstrap sample for every tree grown. from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.model_selection import GridSearchCV # training spit X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 123 ) param_grid = [ { 'n_estimators' : [ 5 , 10 , 20 ], 'max_depth' :[ 2 , 3 , 6 ], 'class_weight' :[ 'balanced' , 'balanced_subsample' ] }] grid = GridSearchCV ( estimator = RandomForestClassifier (), param_grid = param_grid , refit = True , n_jobs =- 1 , verbose = 1 , ) grid . fit ( X_train , y_train ) # print best parameter after tuning print ( grid . best_params_ , ' \\n ' ) # TEST test_accuracy = grid . score ( X_test , y_test ) print ( 'Accuracy of the best parameters using the inner CV of' ) print ( f 'the random search: { grid . best_score_ : .3f } ' ) print ( f 'Accuracy on test set: { test_accuracy : .3f } ' ) Fitting 5 folds for each of 18 candidates, totalling 90 fits {'class_weight': 'balanced', 'max_depth': 6, 'n_estimators': 5} Accuracy of the best parameters using the inner CV of the random search: 0.916 Accuracy on test set: 0.900 fit the model using the best params #take best params clf = RandomForestClassifier ( ** grid . best_params_ , random_state = 123 ) clf . fit ( X_train , y_train ) clf . score ( X_train , y_train ), clf . score ( X_test , y_test ) (0.9607843137254902, 0.9) clf . predict ( X_test ) array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]) y_test array([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])","title":"RandomForestClassifier w GridSearchCV"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#mlp","text":"out of interest, see how an MLP does. The random forrest did a pretty good job. It certainly provides a much more efficient method for identifyign the inital contact points than the functional methods I built above to help create the data set in the first place. import torch import torch.nn as nn # convert to tensors X_train = torch . from_numpy ( X_train ) X_test = torch . from_numpy ( X_test ) y_train = torch . from_numpy ( y_train ) y_test = torch . from_numpy ( y_test ) def train_mlp_classifier ( model , optim , criterion , X_train , y_train , X_test , y_test , num_epochs = 100 ): for epoch in range ( num_epochs ): # zero grad model . train () optim . zero_grad () # forward, loss # ------------- # preds -> (n examples, n classes) (357, 2) preds = model ( X_train ) loss = criterion ( preds , y_train ) # backprop, step loss . backward () optim . step () # validate acc = eval_mlp_classifier ( model , X_test , y_test ) log = f 'Epoch: { epoch : 03d } , Train Loss: { loss : .3f } , Val Acc: { acc : .3f } ' if epoch % 100 == 0 : print ( log ) def eval_mlp_classifier ( model , X_test , y_test ): model . eval () out = model ( X_test ) . argmax ( dim = 1 ) # get preds correct = ( out == y_test ) . sum () acc = int ( correct ) / len ( y_test ) return acc class MLP ( nn . Module ): \"\"\" MLP classifier -------------- n_input: number input features n_output: number of classes n_hidden: number of hidden layers \"\"\" def __init__ ( self , n_input = 4 , n_output = 2 , n_hidden = 64 ): super () . __init__ () self . model = nn . Sequential ( nn . Linear ( n_input , n_hidden * 8 ), nn . ReLU (), nn . Dropout ( 0.2 ), nn . Linear ( n_hidden * 8 , n_hidden * 4 ), nn . ReLU (), #nn.Dropout(0.2), nn . Linear ( n_hidden * 4 , n_output ), ) def forward ( self , x ): return self . model ( x ) DEVICE = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # init mlp = MLP () . to ( DEVICE ) # optimiser and loss optim = torch . optim . Adam ( mlp . parameters (), lr = 0.001 , weight_decay = .05 ) criterion = nn . CrossEntropyLoss ( weight = torch . tensor ([ .2 , 1. ])) train_mlp_classifier ( mlp , optim , criterion , X_train , y_train , X_test , y_test , num_epochs = 300 ) Epoch: 000, Train Loss: 21.817, Val Acc: 0.133 Epoch: 100, Train Loss: 1.545, Val Acc: 0.878 Epoch: 200, Train Loss: 0.540, Val Acc: 0.889 mlp ( X_test ) . argmax ( 1 ) tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]) y_test tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]) Not bad, maybe more consistent that the random forrest but still misses a few points of contact.","title":"MLP"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#summary","text":"What a journey! This notebook was an exploration into using pose data from YOLOv8. Off the shelf, YOLO is a very impressive model but all you ever see online are videos of object detection on street scenes, people dancing, excersising or holding cups and pens up to their webcams. I wanted to understand how useable the data from YOLO actaully is and how it could use to estimate something specific. Classifying a runners gait is nothing new, but the idea here was to illustrate how much thought, preparation and work there is in exploring, cleaning and preparing the pose data for a downstream task.","title":"Summary"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#where-to-next","text":"There are a few things that I would have liked to try but this notebook got a bit big to fit it all in! I would have liked to see how the model performs on the second runner in the frame, this would help in understanding whether the model is able to generalise. I suspect that the model wouldn't perform well on the LHS runner because the magnitude of the X coordinates would be smaller that for the RHS runner due to their position on screen. A potential workaround would be to perform some simple data augmentation on the RHS coordinates, this would hopefully help the model to learn to be invariant to the scale. Another idea could be to include all keypoints for the RHS runner during training or even represent the data as a graph and apply graph representation learning techniques to the data, specifically, this could mean that we can encode in the feet or leg data information propagated from the other keypoints in the body. Maybe this would encode more useful information and assist better in the task. I'd also like to try these models on some more dynamic video content, like someone running outside or around a track or being followed with a handheld camera.","title":"Where to next?"},{"location":"computer%20vision/02_YOLO_running_biomechanics/#references","text":"physio-pedia running biomechanics pexels woman and man on treadmill","title":"references"},{"location":"fastai%20deep%20learning%202020/","text":"About fastai: Deep Learning for Coders 2020 course Notes from the 2020 version of the course. Note, the lesson numbers do not correspond to the fastai lesson numbers. This is because multiple notebooks are sometimes covered in one lecture, where it makes sense, I have kept these as part of a single notebook rather than separate.","title":"About"},{"location":"fastai%20deep%20learning%202020/#about","text":"","title":"About"},{"location":"fastai%20deep%20learning%202020/#fastai-deep-learning-for-coders-2020-course","text":"Notes from the 2020 version of the course. Note, the lesson numbers do not correspond to the fastai lesson numbers. This is because multiple notebooks are sometimes covered in one lecture, where it makes sense, I have kept these as part of a single notebook rather than separate.","title":"fastai: Deep Learning for Coders 2020 course"},{"location":"fastai%20deep%20learning%202020/lesson%2001/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 1: Deep Learning for Coders 06-09-2020 This notebook will go over some of the practical material discussed in lesson 1 of the fastai 2020 course. I am going to cover 2 examples here - classification from image data and tabular data Example 1: Computer Vision from fastai.vision.all import * from pathlib import Path download one of the standard datasets provided by fasta, the Oxford-IIIT Pet Dataset which is a 37 category pet dataset with roughly 200 images for each class. path = untar_data ( URLs . PETS ) / 'images' path Path('/storage/data/oxford-iiit-pet/images') Create an ImageDataLoader Fastai needs to know where to get the image labels from. Normally these labels are part of the filenames or folder structure. In this case the filenames contain the animal breeds. american_bulldog_146.jpg and Siamese_56.jpg for example it so happens that cat breeds start with an uppercase letter. For this example, we will not classify all 37 breeds. We will instead classify whether the images are of dogs or cats. First define a function is_cat that checks whether the first letter in the image label is uppercase. is_cat returns a boolean value that will be used as the new image label. - from_name_func applies the function to our data to create the labels we need. valid_pct=0.2 : hold 20% of the data aside for the validation set, 80% will be used for the training set item_tfms=Resize(224) : resize images to 224x224 fastai provides item transforms (applied to each image in this case) and batch transform which are applied to a batch of items at a time. # check a few image names to confirm that # dog images start with lowercase filenames # cat images start with uppercase filenames files = get_image_files ( path ) files [ 0 ], files [ 6 ] (Path('/storage/data/oxford-iiit-pet/images/american_bulldog_146.jpg'), Path('/storage/data/oxford-iiit-pet/images/Siamese_56.jpg')) def is_cat ( x ): return x [ 0 ] . isupper () dls = ImageDataLoaders . from_name_func ( path , get_image_files ( path ), valid_pct = 0.2 , seed = 42 , label_func = is_cat , item_tfms = Resize ( 224 )) # check our function works! is_cat ( files [ 0 ] . name ), is_cat ( files [ 6 ] . name ) (False, True) # take a look at some of the data dls . show_batch ( max_n = 6 ) # check number of items in training and test datasets len ( dls . train_ds ), len ( dls . valid_ds ) (5912, 1478) Create a cnn_learner using the resnet34 architecture resnet paper this is a pretrained learner, which means when we fit the model, we will not need to train from scratch, rather, we will only fine tune the model by default, freeze_epochs is set to 1 learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.158638 0.023677 0.008119 00:45 epoch train_loss valid_loss error_rate time 0 0.061309 0.013070 0.004736 01:01 learn . show_results () Testing the model Lets load in a picture of a cat and a dog to check the model img_01 = Path . cwd () / 'lesson1_assets/img_1.PNG' img_02 = Path . cwd () / 'lesson1_assets/img_2.PNG' im1 = PILImage . create ( img_01 ) im2 = PILImage . create ( img_02 ) im1 . to_thumb ( 192 ) im2 . to_thumb ( 192 ) learn.predict() returns 3 things, the label ( True / False in our case), the class that scored highest (1 or 0) and then the probabilities of each class. As a reminder, let's use learn.dls.vocab.o2i to check how the classes are mapped to our labels # show how our labels map to our vocab learn . dls . vocab . o2i {False: 0, True: 1} is_cat , clas , probs = learn . predict ( im1 ) is_cat , clas , probs ('True', tensor(1), tensor([2.7169e-10, 1.0000e+00])) Let's check both images... images = [ im1 , im2 ] for i in images : is_cat , _ , probs = learn . predict ( i ) print ( f \"Is this a cat?: { is_cat } .\" ) print ( f \"Probability it's a cat: { probs [ 1 ] . item () : .5f } \" ) Is this a cat?: True. Probability it's a cat: 1.00000 Is this a cat?: False. Probability it's a cat: 0.00000 Example 2: Tabular For this example we will use the Adults data set. Our goal is to predict if a person is earning above or below $50k per year using information such as age, working class, education and occupation. There are about 32K rows in the dataset. from fastai.tabular.all import * path = untar_data ( URLs . ADULT_SAMPLE ) path Path('/storage/data/adult_sample') df = pd . read_csv ( path / 'adult.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 49 Private 101320 Assoc-acdm 12.0 Married-civ-spouse NaN Wife White Female 0 1902 40 United-States >=50k 1 44 Private 236746 Masters 14.0 Divorced Exec-managerial Not-in-family White Male 10520 0 45 United-States >=50k 2 38 Private 96185 HS-grad NaN Divorced NaN Unmarried Black Female 0 0 32 United-States <50k 3 38 Self-emp-inc 112847 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband Asian-Pac-Islander Male 0 0 40 United-States >=50k 4 42 Self-emp-not-inc 82297 7th-8th NaN Married-civ-spouse Other-service Wife Black Female 0 0 50 United-States <50k len ( df ) 32561 Create an TabularDataLoader Again we create data loader using the path . We need to specify some information such as the y variable (the value we want to predict), and we also need to specify which columns contain categorical values and which contain continuous variables. Do this using cat_names and cont_names . Some data processing needs to occur.. - we need to specify how to handle missing data. Info below from the docs - FillMissing by default sets fill_strategy=median - Normalize will normalize the continuous variables (substract the mean and divide by the std) - Categorify transform the categorical variables to something similar to pd.Categorical This is another classification problem. Our goal is to predict whether a persons salary was below 50k (0) or above (1). dls = TabularDataLoaders . from_csv ( path / 'adult.csv' , path = path , y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) I'm going to keep some of the data at the end of the set aside for testing. df[:32500] will select from row 0 to 32500, the remaining rows will not be seen by the model splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df [: 32500 ])) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) dls = to . dataloaders ( bs = 64 ) dls . show_batch () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary 0 Private HS-grad Married-spouse-absent Other-service Unmarried White False 32.000000 128016.002920 9.0 <50k 1 Private 7th-8th Married-civ-spouse Exec-managerial Wife White False 52.000000 194259.000001 4.0 <50k 2 Private Some-college Widowed Exec-managerial Unmarried White False 31.000000 73796.004491 10.0 <50k 3 Private Some-college Separated Other-service Not-in-family White False 64.000001 114993.998143 10.0 <50k 4 Self-emp-not-inc Assoc-voc Married-civ-spouse Prof-specialty Husband White False 68.000000 116902.996854 11.0 <50k 5 Private Bachelors Married-civ-spouse Prof-specialty Husband White False 42.000000 190178.999991 13.0 >=50k 6 Self-emp-not-inc Prof-school Married-civ-spouse Prof-specialty Husband White False 66.000000 291362.001320 15.0 <50k 7 Self-emp-not-inc Bachelors Married-civ-spouse Sales Husband White False 63.000001 298249.000475 13.0 >=50k 8 Private Masters Divorced Tech-support Not-in-family White False 47.000000 606752.001736 14.0 <50k 9 State-gov Bachelors Married-civ-spouse Exec-managerial Husband White False 42.000000 345969.005416 13.0 >=50k We can see that our y values have been turned into the categories 0 and 1. dls . y . value_counts () 0 19756 1 6244 Name: salary, dtype: int64 learn = tabular_learner ( dls , metrics = accuracy ) learn . fit_one_cycle ( 3 ) epoch train_loss valid_loss accuracy time 0 0.366288 0.354235 0.834769 00:06 1 0.367247 0.348617 0.839538 00:05 2 0.358275 0.345206 0.839077 00:06 learn . show_results () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary salary_pred 0 5.0 11.0 3.0 11.0 1.0 5.0 1.0 1.494630 1.838917 2.322299 0.0 1.0 1 5.0 12.0 3.0 8.0 1.0 5.0 1.0 -0.558852 -0.690051 -0.421488 0.0 0.0 2 3.0 10.0 3.0 11.0 6.0 3.0 1.0 0.174535 0.000144 1.146390 1.0 1.0 3 5.0 10.0 3.0 5.0 1.0 5.0 1.0 0.467889 -1.014015 1.146390 1.0 1.0 4 5.0 16.0 5.0 9.0 4.0 5.0 1.0 -1.365576 4.387854 -0.029518 0.0 0.0 5 5.0 10.0 1.0 5.0 2.0 5.0 1.0 0.174535 0.616141 1.146390 0.0 0.0 6 5.0 10.0 3.0 2.0 6.0 5.0 1.0 1.494630 0.898075 1.146390 0.0 1.0 7 5.0 12.0 3.0 5.0 6.0 5.0 1.0 0.101196 -0.713219 -0.421488 1.0 1.0 8 7.0 2.0 3.0 4.0 1.0 5.0 1.0 -0.338836 0.932638 -1.205427 0.0 0.0 Check the model by making predictions on the dataset using the data that was held aside which the model has not yet seen. # pick some random rows of the df sample_df = df . iloc [[ 32513 , 32542 , 32553 ]] sample_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 32513 23 Private 209955 HS-grad 9.0 Never-married Craft-repair Not-in-family White Male 0 0 40 United-States <50k 32542 34 Private 98283 Prof-school 15.0 Never-married Tech-support Not-in-family Asian-Pac-Islander Male 0 1564 40 India >=50k 32553 35 Self-emp-inc 135436 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States >=50k Lets loop through these rows and make predictions, printing out the predicted class, the probabilities and the actual class. for i , r in sample_df . iterrows (): row , clas , probs = learn . predict ( r ) print ( f 'the predicted class is { clas } ' ) print ( f 'with a probability of { probs } ' ) print ( f 'the actual class was { r . salary } ' ) the predicted class is 0 with a probability of tensor([0.9911, 0.0089]) the actual class was <50k the predicted class is 0 with a probability of tensor([0.6258, 0.3742]) the actual class was >=50k the predicted class is 1 with a probability of tensor([0.0919, 0.9081]) the actual class was >=50k","title":"Lesson 01"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#lesson-1-deep-learning-for-coders","text":"06-09-2020 This notebook will go over some of the practical material discussed in lesson 1 of the fastai 2020 course. I am going to cover 2 examples here - classification from image data and tabular data","title":"Lesson 1: Deep Learning for Coders"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#example-1-computer-vision","text":"from fastai.vision.all import * from pathlib import Path download one of the standard datasets provided by fasta, the Oxford-IIIT Pet Dataset which is a 37 category pet dataset with roughly 200 images for each class. path = untar_data ( URLs . PETS ) / 'images' path Path('/storage/data/oxford-iiit-pet/images')","title":"Example 1: Computer Vision"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#create-an-imagedataloader","text":"Fastai needs to know where to get the image labels from. Normally these labels are part of the filenames or folder structure. In this case the filenames contain the animal breeds. american_bulldog_146.jpg and Siamese_56.jpg for example it so happens that cat breeds start with an uppercase letter. For this example, we will not classify all 37 breeds. We will instead classify whether the images are of dogs or cats. First define a function is_cat that checks whether the first letter in the image label is uppercase. is_cat returns a boolean value that will be used as the new image label. - from_name_func applies the function to our data to create the labels we need. valid_pct=0.2 : hold 20% of the data aside for the validation set, 80% will be used for the training set item_tfms=Resize(224) : resize images to 224x224 fastai provides item transforms (applied to each image in this case) and batch transform which are applied to a batch of items at a time. # check a few image names to confirm that # dog images start with lowercase filenames # cat images start with uppercase filenames files = get_image_files ( path ) files [ 0 ], files [ 6 ] (Path('/storage/data/oxford-iiit-pet/images/american_bulldog_146.jpg'), Path('/storage/data/oxford-iiit-pet/images/Siamese_56.jpg')) def is_cat ( x ): return x [ 0 ] . isupper () dls = ImageDataLoaders . from_name_func ( path , get_image_files ( path ), valid_pct = 0.2 , seed = 42 , label_func = is_cat , item_tfms = Resize ( 224 )) # check our function works! is_cat ( files [ 0 ] . name ), is_cat ( files [ 6 ] . name ) (False, True) # take a look at some of the data dls . show_batch ( max_n = 6 ) # check number of items in training and test datasets len ( dls . train_ds ), len ( dls . valid_ds ) (5912, 1478)","title":"Create an ImageDataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#create-a-cnn_learner","text":"using the resnet34 architecture resnet paper this is a pretrained learner, which means when we fit the model, we will not need to train from scratch, rather, we will only fine tune the model by default, freeze_epochs is set to 1 learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.158638 0.023677 0.008119 00:45 epoch train_loss valid_loss error_rate time 0 0.061309 0.013070 0.004736 01:01 learn . show_results ()","title":"Create a cnn_learner"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#testing-the-model","text":"Lets load in a picture of a cat and a dog to check the model img_01 = Path . cwd () / 'lesson1_assets/img_1.PNG' img_02 = Path . cwd () / 'lesson1_assets/img_2.PNG' im1 = PILImage . create ( img_01 ) im2 = PILImage . create ( img_02 ) im1 . to_thumb ( 192 ) im2 . to_thumb ( 192 ) learn.predict() returns 3 things, the label ( True / False in our case), the class that scored highest (1 or 0) and then the probabilities of each class. As a reminder, let's use learn.dls.vocab.o2i to check how the classes are mapped to our labels # show how our labels map to our vocab learn . dls . vocab . o2i {False: 0, True: 1} is_cat , clas , probs = learn . predict ( im1 ) is_cat , clas , probs ('True', tensor(1), tensor([2.7169e-10, 1.0000e+00])) Let's check both images... images = [ im1 , im2 ] for i in images : is_cat , _ , probs = learn . predict ( i ) print ( f \"Is this a cat?: { is_cat } .\" ) print ( f \"Probability it's a cat: { probs [ 1 ] . item () : .5f } \" ) Is this a cat?: True. Probability it's a cat: 1.00000 Is this a cat?: False. Probability it's a cat: 0.00000","title":"Testing the model"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#example-2-tabular","text":"For this example we will use the Adults data set. Our goal is to predict if a person is earning above or below $50k per year using information such as age, working class, education and occupation. There are about 32K rows in the dataset. from fastai.tabular.all import * path = untar_data ( URLs . ADULT_SAMPLE ) path Path('/storage/data/adult_sample') df = pd . read_csv ( path / 'adult.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 0 49 Private 101320 Assoc-acdm 12.0 Married-civ-spouse NaN Wife White Female 0 1902 40 United-States >=50k 1 44 Private 236746 Masters 14.0 Divorced Exec-managerial Not-in-family White Male 10520 0 45 United-States >=50k 2 38 Private 96185 HS-grad NaN Divorced NaN Unmarried Black Female 0 0 32 United-States <50k 3 38 Self-emp-inc 112847 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband Asian-Pac-Islander Male 0 0 40 United-States >=50k 4 42 Self-emp-not-inc 82297 7th-8th NaN Married-civ-spouse Other-service Wife Black Female 0 0 50 United-States <50k len ( df ) 32561","title":"Example 2: Tabular"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#create-an-tabulardataloader","text":"Again we create data loader using the path . We need to specify some information such as the y variable (the value we want to predict), and we also need to specify which columns contain categorical values and which contain continuous variables. Do this using cat_names and cont_names . Some data processing needs to occur.. - we need to specify how to handle missing data. Info below from the docs - FillMissing by default sets fill_strategy=median - Normalize will normalize the continuous variables (substract the mean and divide by the std) - Categorify transform the categorical variables to something similar to pd.Categorical This is another classification problem. Our goal is to predict whether a persons salary was below 50k (0) or above (1). dls = TabularDataLoaders . from_csv ( path / 'adult.csv' , path = path , y_names = \"salary\" , cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], procs = [ Categorify , FillMissing , Normalize ]) I'm going to keep some of the data at the end of the set aside for testing. df[:32500] will select from row 0 to 32500, the remaining rows will not be seen by the model splits = RandomSplitter ( valid_pct = 0.2 )( range_of ( df [: 32500 ])) to = TabularPandas ( df , procs = [ Categorify , FillMissing , Normalize ], cat_names = [ 'workclass' , 'education' , 'marital-status' , 'occupation' , 'relationship' , 'race' ], cont_names = [ 'age' , 'fnlwgt' , 'education-num' ], y_names = 'salary' , splits = splits ) dls = to . dataloaders ( bs = 64 ) dls . show_batch () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary 0 Private HS-grad Married-spouse-absent Other-service Unmarried White False 32.000000 128016.002920 9.0 <50k 1 Private 7th-8th Married-civ-spouse Exec-managerial Wife White False 52.000000 194259.000001 4.0 <50k 2 Private Some-college Widowed Exec-managerial Unmarried White False 31.000000 73796.004491 10.0 <50k 3 Private Some-college Separated Other-service Not-in-family White False 64.000001 114993.998143 10.0 <50k 4 Self-emp-not-inc Assoc-voc Married-civ-spouse Prof-specialty Husband White False 68.000000 116902.996854 11.0 <50k 5 Private Bachelors Married-civ-spouse Prof-specialty Husband White False 42.000000 190178.999991 13.0 >=50k 6 Self-emp-not-inc Prof-school Married-civ-spouse Prof-specialty Husband White False 66.000000 291362.001320 15.0 <50k 7 Self-emp-not-inc Bachelors Married-civ-spouse Sales Husband White False 63.000001 298249.000475 13.0 >=50k 8 Private Masters Divorced Tech-support Not-in-family White False 47.000000 606752.001736 14.0 <50k 9 State-gov Bachelors Married-civ-spouse Exec-managerial Husband White False 42.000000 345969.005416 13.0 >=50k We can see that our y values have been turned into the categories 0 and 1. dls . y . value_counts () 0 19756 1 6244 Name: salary, dtype: int64 learn = tabular_learner ( dls , metrics = accuracy ) learn . fit_one_cycle ( 3 ) epoch train_loss valid_loss accuracy time 0 0.366288 0.354235 0.834769 00:06 1 0.367247 0.348617 0.839538 00:05 2 0.358275 0.345206 0.839077 00:06 learn . show_results () workclass education marital-status occupation relationship race education-num_na age fnlwgt education-num salary salary_pred 0 5.0 11.0 3.0 11.0 1.0 5.0 1.0 1.494630 1.838917 2.322299 0.0 1.0 1 5.0 12.0 3.0 8.0 1.0 5.0 1.0 -0.558852 -0.690051 -0.421488 0.0 0.0 2 3.0 10.0 3.0 11.0 6.0 3.0 1.0 0.174535 0.000144 1.146390 1.0 1.0 3 5.0 10.0 3.0 5.0 1.0 5.0 1.0 0.467889 -1.014015 1.146390 1.0 1.0 4 5.0 16.0 5.0 9.0 4.0 5.0 1.0 -1.365576 4.387854 -0.029518 0.0 0.0 5 5.0 10.0 1.0 5.0 2.0 5.0 1.0 0.174535 0.616141 1.146390 0.0 0.0 6 5.0 10.0 3.0 2.0 6.0 5.0 1.0 1.494630 0.898075 1.146390 0.0 1.0 7 5.0 12.0 3.0 5.0 6.0 5.0 1.0 0.101196 -0.713219 -0.421488 1.0 1.0 8 7.0 2.0 3.0 4.0 1.0 5.0 1.0 -0.338836 0.932638 -1.205427 0.0 0.0","title":"Create an TabularDataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2001/#check-the-model-by-making-predictions-on-the-dataset","text":"using the data that was held aside which the model has not yet seen. # pick some random rows of the df sample_df = df . iloc [[ 32513 , 32542 , 32553 ]] sample_df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } age workclass fnlwgt education education-num marital-status occupation relationship race sex capital-gain capital-loss hours-per-week native-country salary 32513 23 Private 209955 HS-grad 9.0 Never-married Craft-repair Not-in-family White Male 0 0 40 United-States <50k 32542 34 Private 98283 Prof-school 15.0 Never-married Tech-support Not-in-family Asian-Pac-Islander Male 0 1564 40 India >=50k 32553 35 Self-emp-inc 135436 Prof-school 15.0 Married-civ-spouse Prof-specialty Husband White Male 0 0 50 United-States >=50k Lets loop through these rows and make predictions, printing out the predicted class, the probabilities and the actual class. for i , r in sample_df . iterrows (): row , clas , probs = learn . predict ( r ) print ( f 'the predicted class is { clas } ' ) print ( f 'with a probability of { probs } ' ) print ( f 'the actual class was { r . salary } ' ) the predicted class is 0 with a probability of tensor([0.9911, 0.0089]) the actual class was <50k the predicted class is 0 with a probability of tensor([0.6258, 0.3742]) the actual class was >=50k the predicted class is 1 with a probability of tensor([0.0919, 0.9081]) the actual class was >=50k","title":"Check the model by making predictions on the dataset"},{"location":"fastai%20deep%20learning%202020/lesson%2002/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 2: Deep Learning for Coders 12-09-2020 Lesson 2 goes a little deeper into computer vision and the fastai library by going a little deeper into the DataBlocks and DataLoaders . The course notebook uses Bing Images to download image data, the idea being that we curate our own data set for this exercise. Fastai provides some methods and instructions for doing this, you can see details in the notebook I have taken a different route to gathering data. My goal for this notebook is to build a model that is able to classify musical pitches. Audio Data generate audio samples using MIDI. I will not be worrying about sharps/flats simply to reduce complexity use librosa to process audio signals and generate chromagrams using the Constant Q Transform The Constant Q does a good job at isolating pitch but is not sensitive to octaves, thus, all audio samples are in the same octave. stackexchange # !conda install -c conda-forge librosa -y from fastai.vision.all import * from fastai.vision.data import * import matplotlib.pyplot as plt # hi-res plots % config InlineBackend . figure_format = 'retina' from pathlib import Path # sound library & widget to play audio import librosa import librosa.display import IPython.display as ipd Load in Data path = Path . cwd () data_path = path / 'lesson2_assets/cqt_data' audio_file = path / 'lesson2_assets/A3_1.wav' # take a look at the filenames data_path . ls ()[ 1 ] Path('/notebooks/lesson2_assets/cqt_data/A3_1.jpg') sidebar.. generating a chromagram Below is a sample note (A3 on the piano) followed by a demonstration of how to generate a Constant-Q chromagram. The Y axis is displaying the note name for convenience. These were removed to create the training data set. y , sr = librosa . load ( audio_file , mono = True ) ipd . Audio ( y , rate = sr ) Your browser does not support the audio element. doc ( librosa . feature . chroma_cqt ) plt . figure ( figsize = ( 5 , 5 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C , y_axis = 'chroma' ); For comparison, here is the same note visualised using a spectrogram... \"A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.\" wiki There is a lot more information within this plot (such as the fundamental frequency and harmonics above it), however using these images would make our classification task much harder. plt . figure ( figsize = ( 5 , 5 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' ); Image Data The filenames contain the classes that we are trying to predict. We need to define a function that will grab the first 2 characters of each file name to use as labels. This will look familiar to lesson one, excet we have more classes to predict fnames = get_image_files ( data_path ) def label_func ( fname ): return fname . name [: 2 ] label_func ( fnames [ 37 ]) # verify the funciton works 'B3' From Data to DataLoader What is a DataBlock? - \"The data block API takes its name from the way it's designed: every bit needed to build the DataLoaders object (type of inputs, targets, how to label, split...) is encapsulated in a block, and you can mix and match those blocks\" - docs Breaking down the Block - the tutorial in the docs does a good job of stepping through building a block from scratch.. Steps Start with an empty DataBlock . dblock = DataBlock() Tell the block how you want to assemble your items using a get_items function. we will use get_image_files as we did in lesson 1. Let the block know how/where to get our labels from in get_y . the lesson notebook uses parent_label which inherits the label from the parent folder. We need to use the label_func we created for this task. Specify the types of our data (images and labels). ImageBlock and CategoryBlock . blocks=(ImageBlock, CategoryBlock) . Decide how we want to split our data into training and valid datasets. we will randomly split (80% training, 20% validation). Specify any item transforms or batch transforms. audio = DataBlock ( blocks = ( ImageBlock , CategoryBlock ), get_items = get_image_files , splitter = RandomSplitter ( valid_pct = 0.2 , seed = 42 ), get_y = label_func , item_tfms = Resize ( 128 ) ) dls = audio . dataloaders ( data_path , bs = 32 ) dls . show_batch () Sidebar: Data Augmentation and Transforms fastai provides a number of transforms that can be applied to data. In the case of computer vision, augmentation is useful for introducing variety into the dataset. Consider facial recognition, in production, you may not always be dealing with descent portraits; camera angle, lighting, perspective and lighting conditions may vary. Augmentation introduces some of these concepts into our traing and validation set. In the context of the data I am working with, not all transformations may be useful. I would not expect these images to suffer from perspective warping or rotation, however, mirroring the image on the vertical could be useful, as could increasing and decreasing brightness and contrast. Here is a quick example of how to apply some transforms to a batch of images at a time using aug_transforms I am not going to apply any of these for training in this notebook. # no transformation applied dls . valid . show_batch ( max_n = 4 , nrows = 1 ) Here is a list of available transforms.. aug_transforms ( mult = 1.0 , do_flip = True , flip_vert = False , max_rotate = 10.0 , min_zoom = 1.0 , max_zoom = 1.1 , max_lighting = 0.2 , max_warp = 0.2 , p_affine = 0.75 , p_lighting = 0.75 , xtra_tfms = None , size = None , mode = 'bilinear' , pad_mode = 'reflection' , align_corners = True , batch = False , min_scale = 1.0 , ) aug_tfms = aug_transforms ( max_lighting = 0.8 , do_flip = True , flip_vert = True , max_rotate = 0 ) audio = audio . new ( item_tfms = Resize ( 128 ), batch_tfms = aug_tfms ) dls = audio . dataloaders ( data_path ) dls . train . show_batch ( max_n = 4 , nrows = 1 ) Create a cnn_learner and Train the model learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) Before training, let's use learn.lr_find to help find a good learning rate. Two values are returned by running lr_find one tenth of the minimum before the divergence when the slope is the steepest lr_min , lr_steep = learn . lr_find () # plot the values returned by lr_find learn . recorder . plot_lr_find () plt . axvline ( x = lr_min , color = 'red' ) plt . axvline ( x = 3e-3 , color = 'green' ) plt . axvline ( x = lr_steep , color = 'red' ); I'm going to pick a value inbetween the suggested lr's for training. lr_max = 3e-3 learn . fit_one_cycle ( n_epoch = 5 , lr_max = lr_max ) epoch train_loss valid_loss error_rate time 0 2.412440 1.861958 0.642857 00:02 1 1.202116 0.856408 0.261905 00:01 2 0.763907 0.548905 0.190476 00:01 3 0.543375 0.206846 0.095238 00:01 4 0.412221 0.038176 0.023810 00:01 learn . recorder . plot_loss () Interpretation the model is performing quite well, only one note was incorrectly predicted (G3 predicted for F3 actual) interp . plot_top_losses ( k = 4 , figsize = ( 6 , 6 )) interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix () learn . save ( 'base_cqt_model' ) Path('models/base_cqt_model.pth') Fine tune to try improve accuracy.. learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.000892 0.002096 0.000000 00:01 epoch train_loss valid_loss error_rate time 0 0.000499 0.000041 0.000000 00:02 interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix () Summary This wasn't the hardest problem in the world. The Constant Q transform really simplifies pitch detection. I think this is an interesting problem space because there are opportunities to progress these examples; I'd like to try classify all 12 notes (by adding sharps/flats), then try multi-label classification using a phrase of notes, and hopefully then addressing the issue of identifying notes across octaves.","title":"Lesson 02"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#lesson-2-deep-learning-for-coders","text":"12-09-2020 Lesson 2 goes a little deeper into computer vision and the fastai library by going a little deeper into the DataBlocks and DataLoaders . The course notebook uses Bing Images to download image data, the idea being that we curate our own data set for this exercise. Fastai provides some methods and instructions for doing this, you can see details in the notebook I have taken a different route to gathering data. My goal for this notebook is to build a model that is able to classify musical pitches.","title":"Lesson 2: Deep Learning for Coders"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#audio-data","text":"generate audio samples using MIDI. I will not be worrying about sharps/flats simply to reduce complexity use librosa to process audio signals and generate chromagrams using the Constant Q Transform The Constant Q does a good job at isolating pitch but is not sensitive to octaves, thus, all audio samples are in the same octave. stackexchange # !conda install -c conda-forge librosa -y from fastai.vision.all import * from fastai.vision.data import * import matplotlib.pyplot as plt # hi-res plots % config InlineBackend . figure_format = 'retina' from pathlib import Path # sound library & widget to play audio import librosa import librosa.display import IPython.display as ipd","title":"Audio Data"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#load-in-data","text":"path = Path . cwd () data_path = path / 'lesson2_assets/cqt_data' audio_file = path / 'lesson2_assets/A3_1.wav' # take a look at the filenames data_path . ls ()[ 1 ] Path('/notebooks/lesson2_assets/cqt_data/A3_1.jpg')","title":"Load in Data"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#sidebar-generating-a-chromagram","text":"Below is a sample note (A3 on the piano) followed by a demonstration of how to generate a Constant-Q chromagram. The Y axis is displaying the note name for convenience. These were removed to create the training data set. y , sr = librosa . load ( audio_file , mono = True ) ipd . Audio ( y , rate = sr ) Your browser does not support the audio element. doc ( librosa . feature . chroma_cqt ) plt . figure ( figsize = ( 5 , 5 )) C = librosa . feature . chroma_cqt ( y = y , sr = sr ) librosa . display . specshow ( C , y_axis = 'chroma' ); For comparison, here is the same note visualised using a spectrogram... \"A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.\" wiki There is a lot more information within this plot (such as the fundamental frequency and harmonics above it), however using these images would make our classification task much harder. plt . figure ( figsize = ( 5 , 5 )) D = librosa . amplitude_to_db ( np . abs ( librosa . stft ( y )), ref = np . max ) librosa . display . specshow ( D , y_axis = 'log' );","title":"sidebar.. generating a chromagram"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#image-data","text":"The filenames contain the classes that we are trying to predict. We need to define a function that will grab the first 2 characters of each file name to use as labels. This will look familiar to lesson one, excet we have more classes to predict fnames = get_image_files ( data_path ) def label_func ( fname ): return fname . name [: 2 ] label_func ( fnames [ 37 ]) # verify the funciton works 'B3'","title":"Image Data"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#from-data-to-dataloader","text":"What is a DataBlock? - \"The data block API takes its name from the way it's designed: every bit needed to build the DataLoaders object (type of inputs, targets, how to label, split...) is encapsulated in a block, and you can mix and match those blocks\" - docs Breaking down the Block - the tutorial in the docs does a good job of stepping through building a block from scratch..","title":"From Data to DataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#steps","text":"Start with an empty DataBlock . dblock = DataBlock() Tell the block how you want to assemble your items using a get_items function. we will use get_image_files as we did in lesson 1. Let the block know how/where to get our labels from in get_y . the lesson notebook uses parent_label which inherits the label from the parent folder. We need to use the label_func we created for this task. Specify the types of our data (images and labels). ImageBlock and CategoryBlock . blocks=(ImageBlock, CategoryBlock) . Decide how we want to split our data into training and valid datasets. we will randomly split (80% training, 20% validation). Specify any item transforms or batch transforms. audio = DataBlock ( blocks = ( ImageBlock , CategoryBlock ), get_items = get_image_files , splitter = RandomSplitter ( valid_pct = 0.2 , seed = 42 ), get_y = label_func , item_tfms = Resize ( 128 ) ) dls = audio . dataloaders ( data_path , bs = 32 ) dls . show_batch ()","title":"Steps"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#sidebar-data-augmentation-and-transforms","text":"fastai provides a number of transforms that can be applied to data. In the case of computer vision, augmentation is useful for introducing variety into the dataset. Consider facial recognition, in production, you may not always be dealing with descent portraits; camera angle, lighting, perspective and lighting conditions may vary. Augmentation introduces some of these concepts into our traing and validation set. In the context of the data I am working with, not all transformations may be useful. I would not expect these images to suffer from perspective warping or rotation, however, mirroring the image on the vertical could be useful, as could increasing and decreasing brightness and contrast. Here is a quick example of how to apply some transforms to a batch of images at a time using aug_transforms I am not going to apply any of these for training in this notebook. # no transformation applied dls . valid . show_batch ( max_n = 4 , nrows = 1 ) Here is a list of available transforms.. aug_transforms ( mult = 1.0 , do_flip = True , flip_vert = False , max_rotate = 10.0 , min_zoom = 1.0 , max_zoom = 1.1 , max_lighting = 0.2 , max_warp = 0.2 , p_affine = 0.75 , p_lighting = 0.75 , xtra_tfms = None , size = None , mode = 'bilinear' , pad_mode = 'reflection' , align_corners = True , batch = False , min_scale = 1.0 , ) aug_tfms = aug_transforms ( max_lighting = 0.8 , do_flip = True , flip_vert = True , max_rotate = 0 ) audio = audio . new ( item_tfms = Resize ( 128 ), batch_tfms = aug_tfms ) dls = audio . dataloaders ( data_path ) dls . train . show_batch ( max_n = 4 , nrows = 1 )","title":"Sidebar: Data Augmentation and Transforms"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#create-a-cnn_learner-and-train-the-model","text":"learn = cnn_learner ( dls , resnet34 , metrics = error_rate ) Before training, let's use learn.lr_find to help find a good learning rate. Two values are returned by running lr_find one tenth of the minimum before the divergence when the slope is the steepest lr_min , lr_steep = learn . lr_find () # plot the values returned by lr_find learn . recorder . plot_lr_find () plt . axvline ( x = lr_min , color = 'red' ) plt . axvline ( x = 3e-3 , color = 'green' ) plt . axvline ( x = lr_steep , color = 'red' ); I'm going to pick a value inbetween the suggested lr's for training. lr_max = 3e-3 learn . fit_one_cycle ( n_epoch = 5 , lr_max = lr_max ) epoch train_loss valid_loss error_rate time 0 2.412440 1.861958 0.642857 00:02 1 1.202116 0.856408 0.261905 00:01 2 0.763907 0.548905 0.190476 00:01 3 0.543375 0.206846 0.095238 00:01 4 0.412221 0.038176 0.023810 00:01 learn . recorder . plot_loss ()","title":"Create a cnn_learner and Train the model"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#interpretation","text":"the model is performing quite well, only one note was incorrectly predicted (G3 predicted for F3 actual) interp . plot_top_losses ( k = 4 , figsize = ( 6 , 6 )) interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix () learn . save ( 'base_cqt_model' ) Path('models/base_cqt_model.pth') Fine tune to try improve accuracy.. learn . fine_tune ( 1 ) epoch train_loss valid_loss error_rate time 0 0.000892 0.002096 0.000000 00:01 epoch train_loss valid_loss error_rate time 0 0.000499 0.000041 0.000000 00:02 interp = ClassificationInterpretation . from_learner ( learn ) interp . plot_confusion_matrix ()","title":"Interpretation"},{"location":"fastai%20deep%20learning%202020/lesson%2002/#summary","text":"This wasn't the hardest problem in the world. The Constant Q transform really simplifies pitch detection. I think this is an interesting problem space because there are opportunities to progress these examples; I'd like to try classify all 12 notes (by adding sharps/flats), then try multi-label classification using a phrase of notes, and hopefully then addressing the issue of identifying notes across octaves.","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 3: Under the Hood: Training a Digit Classifier 20-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. Primer load in some data visualise it from fastai.vision.all import * from pathlib import Path For this model we are going to create a digit classifier that will be able to classify an image as a 3 or a 7. Fastai has a sample of the MNIST dataset that we will be using. First, let's load the data and check that there are indeed, 3's an 7's in one of the folders. The folder layout is fairly typical, separate training and validation sets. path = untar_data ( URLs . MNIST_SAMPLE ) ( path / 'train' ) . ls () (#2) [Path('/storage/data/mnist_sample/train/7'),Path('/storage/data/mnist_sample/train/3')] Visualising Data There are a number of ways we can check our data to get a better understanding of how it is structured and how it looks Take a look inside one of the folders and check file names Use PIL to open one of the images Use PyTorch/Numpy to check the tensor/array values Get creative # 1 - check file names threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () threes (#6131) [Path('/storage/data/mnist_sample/train/3/10.png'),Path('/storage/data/mnist_sample/train/3/10000.png'),Path('/storage/data/mnist_sample/train/3/10011.png'),Path('/storage/data/mnist_sample/train/3/10031.png'),Path('/storage/data/mnist_sample/train/3/10034.png'),Path('/storage/data/mnist_sample/train/3/10042.png'),Path('/storage/data/mnist_sample/train/3/10052.png'),Path('/storage/data/mnist_sample/train/3/1007.png'),Path('/storage/data/mnist_sample/train/3/10074.png'),Path('/storage/data/mnist_sample/train/3/10091.png')...] # 2 - use PIL to open image im3_path = threes [ 1 ] im3 = Image . open ( im3_path ) im3 # 3 - use PyTorch to view tensor values tensor ( im3 )[ 4 : 10 , 4 : 10 ] tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) # 4 - Getting creative with Pandas im3_t = tensor ( im3 ) df = pd . DataFrame ( im3_t [ 4 : 15 , 4 : 22 ]) df . style . set_properties ( ** { 'font-size' : '6pt' }) . background_gradient ( 'Greys' ) #T_71814984_fd78_11ea_966e_0242ac110002row0_col0,#T_71814984_fd78_11ea_966e_0242ac110002row0_col1,#T_71814984_fd78_11ea_966e_0242ac110002row0_col2,#T_71814984_fd78_11ea_966e_0242ac110002row0_col3,#T_71814984_fd78_11ea_966e_0242ac110002row0_col4,#T_71814984_fd78_11ea_966e_0242ac110002row0_col5,#T_71814984_fd78_11ea_966e_0242ac110002row0_col6,#T_71814984_fd78_11ea_966e_0242ac110002row0_col7,#T_71814984_fd78_11ea_966e_0242ac110002row0_col8,#T_71814984_fd78_11ea_966e_0242ac110002row0_col9,#T_71814984_fd78_11ea_966e_0242ac110002row0_col10,#T_71814984_fd78_11ea_966e_0242ac110002row0_col11,#T_71814984_fd78_11ea_966e_0242ac110002row0_col12,#T_71814984_fd78_11ea_966e_0242ac110002row0_col13,#T_71814984_fd78_11ea_966e_0242ac110002row0_col14,#T_71814984_fd78_11ea_966e_0242ac110002row0_col15,#T_71814984_fd78_11ea_966e_0242ac110002row0_col16,#T_71814984_fd78_11ea_966e_0242ac110002row0_col17,#T_71814984_fd78_11ea_966e_0242ac110002row1_col0,#T_71814984_fd78_11ea_966e_0242ac110002row1_col1,#T_71814984_fd78_11ea_966e_0242ac110002row1_col2,#T_71814984_fd78_11ea_966e_0242ac110002row1_col3,#T_71814984_fd78_11ea_966e_0242ac110002row1_col4,#T_71814984_fd78_11ea_966e_0242ac110002row1_col15,#T_71814984_fd78_11ea_966e_0242ac110002row1_col16,#T_71814984_fd78_11ea_966e_0242ac110002row1_col17,#T_71814984_fd78_11ea_966e_0242ac110002row2_col0,#T_71814984_fd78_11ea_966e_0242ac110002row2_col1,#T_71814984_fd78_11ea_966e_0242ac110002row2_col2,#T_71814984_fd78_11ea_966e_0242ac110002row2_col15,#T_71814984_fd78_11ea_966e_0242ac110002row2_col16,#T_71814984_fd78_11ea_966e_0242ac110002row2_col17,#T_71814984_fd78_11ea_966e_0242ac110002row3_col0,#T_71814984_fd78_11ea_966e_0242ac110002row3_col15,#T_71814984_fd78_11ea_966e_0242ac110002row3_col16,#T_71814984_fd78_11ea_966e_0242ac110002row3_col17,#T_71814984_fd78_11ea_966e_0242ac110002row4_col0,#T_71814984_fd78_11ea_966e_0242ac110002row4_col6,#T_71814984_fd78_11ea_966e_0242ac110002row4_col7,#T_71814984_fd78_11ea_966e_0242ac110002row4_col8,#T_71814984_fd78_11ea_966e_0242ac110002row4_col9,#T_71814984_fd78_11ea_966e_0242ac110002row4_col10,#T_71814984_fd78_11ea_966e_0242ac110002row4_col15,#T_71814984_fd78_11ea_966e_0242ac110002row4_col16,#T_71814984_fd78_11ea_966e_0242ac110002row4_col17,#T_71814984_fd78_11ea_966e_0242ac110002row5_col0,#T_71814984_fd78_11ea_966e_0242ac110002row5_col5,#T_71814984_fd78_11ea_966e_0242ac110002row5_col6,#T_71814984_fd78_11ea_966e_0242ac110002row5_col7,#T_71814984_fd78_11ea_966e_0242ac110002row5_col8,#T_71814984_fd78_11ea_966e_0242ac110002row5_col9,#T_71814984_fd78_11ea_966e_0242ac110002row5_col15,#T_71814984_fd78_11ea_966e_0242ac110002row5_col16,#T_71814984_fd78_11ea_966e_0242ac110002row5_col17,#T_71814984_fd78_11ea_966e_0242ac110002row6_col0,#T_71814984_fd78_11ea_966e_0242ac110002row6_col1,#T_71814984_fd78_11ea_966e_0242ac110002row6_col2,#T_71814984_fd78_11ea_966e_0242ac110002row6_col3,#T_71814984_fd78_11ea_966e_0242ac110002row6_col4,#T_71814984_fd78_11ea_966e_0242ac110002row6_col5,#T_71814984_fd78_11ea_966e_0242ac110002row6_col6,#T_71814984_fd78_11ea_966e_0242ac110002row6_col7,#T_71814984_fd78_11ea_966e_0242ac110002row6_col8,#T_71814984_fd78_11ea_966e_0242ac110002row6_col9,#T_71814984_fd78_11ea_966e_0242ac110002row6_col14,#T_71814984_fd78_11ea_966e_0242ac110002row6_col15,#T_71814984_fd78_11ea_966e_0242ac110002row6_col16,#T_71814984_fd78_11ea_966e_0242ac110002row6_col17,#T_71814984_fd78_11ea_966e_0242ac110002row7_col0,#T_71814984_fd78_11ea_966e_0242ac110002row7_col1,#T_71814984_fd78_11ea_966e_0242ac110002row7_col2,#T_71814984_fd78_11ea_966e_0242ac110002row7_col3,#T_71814984_fd78_11ea_966e_0242ac110002row7_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col5,#T_71814984_fd78_11ea_966e_0242ac110002row7_col6,#T_71814984_fd78_11ea_966e_0242ac110002row7_col13,#T_71814984_fd78_11ea_966e_0242ac110002row7_col14,#T_71814984_fd78_11ea_966e_0242ac110002row7_col15,#T_71814984_fd78_11ea_966e_0242ac110002row7_col16,#T_71814984_fd78_11ea_966e_0242ac110002row7_col17,#T_71814984_fd78_11ea_966e_0242ac110002row8_col0,#T_71814984_fd78_11ea_966e_0242ac110002row8_col1,#T_71814984_fd78_11ea_966e_0242ac110002row8_col2,#T_71814984_fd78_11ea_966e_0242ac110002row8_col3,#T_71814984_fd78_11ea_966e_0242ac110002row8_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col13,#T_71814984_fd78_11ea_966e_0242ac110002row8_col14,#T_71814984_fd78_11ea_966e_0242ac110002row8_col15,#T_71814984_fd78_11ea_966e_0242ac110002row8_col16,#T_71814984_fd78_11ea_966e_0242ac110002row8_col17,#T_71814984_fd78_11ea_966e_0242ac110002row9_col0,#T_71814984_fd78_11ea_966e_0242ac110002row9_col1,#T_71814984_fd78_11ea_966e_0242ac110002row9_col2,#T_71814984_fd78_11ea_966e_0242ac110002row9_col3,#T_71814984_fd78_11ea_966e_0242ac110002row9_col4,#T_71814984_fd78_11ea_966e_0242ac110002row9_col16,#T_71814984_fd78_11ea_966e_0242ac110002row9_col17,#T_71814984_fd78_11ea_966e_0242ac110002row10_col0,#T_71814984_fd78_11ea_966e_0242ac110002row10_col1,#T_71814984_fd78_11ea_966e_0242ac110002row10_col2,#T_71814984_fd78_11ea_966e_0242ac110002row10_col3,#T_71814984_fd78_11ea_966e_0242ac110002row10_col4,#T_71814984_fd78_11ea_966e_0242ac110002row10_col5,#T_71814984_fd78_11ea_966e_0242ac110002row10_col6,#T_71814984_fd78_11ea_966e_0242ac110002row10_col17{ font-size: 6pt; background-color: #ffffff; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col5{ font-size: 6pt; background-color: #efefef; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col6,#T_71814984_fd78_11ea_966e_0242ac110002row1_col13{ font-size: 6pt; background-color: #7c7c7c; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col7{ font-size: 6pt; background-color: #4a4a4a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col8,#T_71814984_fd78_11ea_966e_0242ac110002row1_col9,#T_71814984_fd78_11ea_966e_0242ac110002row1_col10,#T_71814984_fd78_11ea_966e_0242ac110002row2_col5,#T_71814984_fd78_11ea_966e_0242ac110002row2_col6,#T_71814984_fd78_11ea_966e_0242ac110002row2_col7,#T_71814984_fd78_11ea_966e_0242ac110002row2_col11,#T_71814984_fd78_11ea_966e_0242ac110002row2_col12,#T_71814984_fd78_11ea_966e_0242ac110002row2_col13,#T_71814984_fd78_11ea_966e_0242ac110002row3_col4,#T_71814984_fd78_11ea_966e_0242ac110002row3_col12,#T_71814984_fd78_11ea_966e_0242ac110002row3_col13,#T_71814984_fd78_11ea_966e_0242ac110002row4_col1,#T_71814984_fd78_11ea_966e_0242ac110002row4_col2,#T_71814984_fd78_11ea_966e_0242ac110002row4_col3,#T_71814984_fd78_11ea_966e_0242ac110002row4_col12,#T_71814984_fd78_11ea_966e_0242ac110002row4_col13,#T_71814984_fd78_11ea_966e_0242ac110002row5_col12,#T_71814984_fd78_11ea_966e_0242ac110002row6_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col12,#T_71814984_fd78_11ea_966e_0242ac110002row10_col13,#T_71814984_fd78_11ea_966e_0242ac110002row10_col14,#T_71814984_fd78_11ea_966e_0242ac110002row10_col15,#T_71814984_fd78_11ea_966e_0242ac110002row10_col16{ font-size: 6pt; background-color: #000000; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col11{ font-size: 6pt; background-color: #606060; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col12{ font-size: 6pt; background-color: #4d4d4d; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col14{ font-size: 6pt; background-color: #bbbbbb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col3{ font-size: 6pt; background-color: #e4e4e4; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col6{ font-size: 6pt; background-color: #6b6b6b; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col8,#T_71814984_fd78_11ea_966e_0242ac110002row2_col14,#T_71814984_fd78_11ea_966e_0242ac110002row3_col14{ font-size: 6pt; background-color: #171717; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col9,#T_71814984_fd78_11ea_966e_0242ac110002row3_col11{ font-size: 6pt; background-color: #4b4b4b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col10,#T_71814984_fd78_11ea_966e_0242ac110002row8_col8,#T_71814984_fd78_11ea_966e_0242ac110002row8_col10,#T_71814984_fd78_11ea_966e_0242ac110002row9_col8,#T_71814984_fd78_11ea_966e_0242ac110002row9_col10{ font-size: 6pt; background-color: #010101; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col1{ font-size: 6pt; background-color: #272727; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col2{ font-size: 6pt; background-color: #0a0a0a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col3{ font-size: 6pt; background-color: #050505; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col5{ font-size: 6pt; background-color: #333333; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col6{ font-size: 6pt; background-color: #e6e6e6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col7,#T_71814984_fd78_11ea_966e_0242ac110002row3_col10{ font-size: 6pt; background-color: #fafafa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col8{ font-size: 6pt; background-color: #fbfbfb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col9{ font-size: 6pt; background-color: #fdfdfd; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col4{ font-size: 6pt; background-color: #1b1b1b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col5{ font-size: 6pt; background-color: #e0e0e0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col11{ font-size: 6pt; background-color: #4e4e4e; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col14{ font-size: 6pt; background-color: #767676; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col1{ font-size: 6pt; background-color: #fcfcfc; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col2,#T_71814984_fd78_11ea_966e_0242ac110002row5_col3{ font-size: 6pt; background-color: #f6f6f6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col7{ font-size: 6pt; background-color: #f8f8f8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col10,#T_71814984_fd78_11ea_966e_0242ac110002row10_col7{ font-size: 6pt; background-color: #e8e8e8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col11{ font-size: 6pt; background-color: #222222; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col13,#T_71814984_fd78_11ea_966e_0242ac110002row6_col12{ font-size: 6pt; background-color: #090909; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col14{ font-size: 6pt; background-color: #d0d0d0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col6{ font-size: 6pt; background-color: #060606; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col13{ font-size: 6pt; background-color: #979797; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col8{ font-size: 6pt; background-color: #b6b6b6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col9{ font-size: 6pt; background-color: #252525; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col12{ font-size: 6pt; background-color: #999999; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col5{ font-size: 6pt; background-color: #f9f9f9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col7{ font-size: 6pt; background-color: #101010; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col9,#T_71814984_fd78_11ea_966e_0242ac110002row9_col9{ font-size: 6pt; background-color: #020202; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col11{ font-size: 6pt; background-color: #545454; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col12{ font-size: 6pt; background-color: #f1f1f1; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col5{ font-size: 6pt; background-color: #f7f7f7; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col7{ font-size: 6pt; background-color: #030303; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col12{ font-size: 6pt; background-color: #181818; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col13{ font-size: 6pt; background-color: #303030; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col14{ font-size: 6pt; background-color: #a9a9a9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col15{ font-size: 6pt; background-color: #fefefe; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col8,#T_71814984_fd78_11ea_966e_0242ac110002row10_col9{ font-size: 6pt; background-color: #bababa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col10{ font-size: 6pt; background-color: #393939; color: #f1f1f1; } 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 29 150 195 254 255 254 176 193 150 96 0 0 0 2 0 0 0 48 166 224 253 253 234 196 253 253 253 253 233 0 0 0 3 0 93 244 249 253 187 46 10 8 4 10 194 253 253 233 0 0 0 4 0 107 253 253 230 48 0 0 0 0 0 192 253 253 156 0 0 0 5 0 3 20 20 15 0 0 0 0 0 43 224 253 245 74 0 0 0 6 0 0 0 0 0 0 0 0 0 0 249 253 245 126 0 0 0 0 7 0 0 0 0 0 0 0 14 101 223 253 248 124 0 0 0 0 0 8 0 0 0 0 0 11 166 239 253 253 253 187 30 0 0 0 0 0 9 0 0 0 0 0 16 248 250 253 253 253 253 232 213 111 2 0 0 10 0 0 0 0 0 0 0 43 98 98 208 253 253 253 253 187 22 0 Sidebar - think about the problem Before jumping into solution mode (ie apply deep learning to everything!) think about the problem space and how you might be able to solve it. For this problem (digit recognition) using a simple average might be enough to get a descent result. That is exactly what we will do first. Method 1: Pixel Similarity find the average pixel value for every pixel of the 3s and 7s this will give us 2 group averages that represent the \"ideal\" 3 and 7 to classify a digit, check the similarity against the ideal this method will form our baseline that we will improve upon later Step 1: Organise data create a tensor by stacking all of our 3s together we will use list comprehension for this # open all images, convert to tensor, store in list seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] len ( three_tensors ), len ( seven_tensors ) (6131, 6265) # lets check one of show_image ( three_tensors [ 1 ]); Step 2: Compute the average pixel value For every pixel position, compute the average over all the images of the intensity of that pixel. To do this combine all images into a single 3-dimensional tensor using stack which \" Concatenates sequence of tensors along a new dimension .\" PuyTorch needs us to cast the int values to floats in order to compute the average. \"Generally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here\" source stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 stacked_threes . shape torch.Size([6131, 28, 28]) tensor jargon - rank : the number of axis - shape : the size of each axis print ( 'rank:' , stacked_threes . ndim , ' \\n ' , 'shape:' , stacked_threes . shape ) rank: 3 shape: torch.Size([6131, 28, 28]) We have 6,131 images of size 28x28 Step 3: Compute the ideal digits compute the mean along the 0th dimension by visualising this ideal 3 we can indeed see that it represents a 3! mean3 = stacked_threes . mean ( dim = 0 ) show_image ( mean3 ); mean7 = stacked_sevens . mean ( dim = 0 ) show_image ( mean7 ); # here is a random three for comparison a_3 = stacked_threes [ 1 ] show_image ( a_3 ); How could we calculate how similar a 3 is from this ideal 3? Typically there are two methods take the absolute value of differences (where there are negatives, replace with postive) This is called the mean absolute difference or L1 norm take the mean squared difference (which also makes all results positive) then take the square root This is called the root mean squared error (RMSE) or L2 norm . # 3 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean3 ) . abs () . mean () dist_3_sqr = (( a_3 - mean3 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1114), tensor(0.2021)) # 7 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean7 ) . abs () . mean () dist_3_sqr = (( a_3 - mean7 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1586), tensor(0.3021)) The distance between our \"ideal\" 3 and the real 3 is less than the distance from the real 3 to \"ideal\" 7. This is good - it means both methods will work and our simple model will give the correct prediction. PyTorch already provides these loss functions for us (though RMSE is only MSE, but we can work with that). # check results with PyTorch F . l1_loss ( a_3 . float (), mean7 ), F . mse_loss ( a_3 , mean7 ) . sqrt () (tensor(0.1586), tensor(0.3021)) A simple model A validation set is usually used to help avoid overfitting, our model has no trained components so this isn't going to be an issue, but let's stick with best practices. We will also define a function that will decide if an arbitrary image is a 3 or a 7. This will be achieved by measuring the distance between this digit and our ideal digits and determining which ideal it is closer to. valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 valid_3_tens . shape , valid_7_tens . shape (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) mnist_distance calculates the difference between our ideal 3 and every 3 in the validation set. We call mean((-1,-2)) in our fuction. The tuple (-1,-2) represents a range of axes, so the last -1, and second last -2. mean((-1,-2)) says; take the mean over the last two axes in the tensor. Why? These axes represent the verticle and horizontal dimensions of an image. After taking the mean over the these axes, we have one axis left which indexes over the 1,010 images we have. # calculate the mean absolute error def mnist_distance ( a , b ): return ( a - b ) . abs () . mean (( - 1 , - 2 )) # check the function mnist_distance ( a_3 , mean3 ) # great! tensor(0.1114) This works for a single image, but in order to calculate the overall accuracy, we want to calculate this distance to the ideal 3 for all images in the validation set. This can be achieved using a loop, but there is another way - broadcasting. Take a look at the shape of valid_3_tens and mean3 , they are different... valid_3_tens . shape , mean3 . shape (torch.Size([1010, 28, 28]), torch.Size([28, 28])) Now calculate the distance between the two... valid_3_dist = mnist_distance ( valid_3_tens , mean3 ) valid_3_dist , valid_3_dist . shape (tensor([0.1290, 0.1223, 0.1380, ..., 0.1337, 0.1132, 0.1097]), torch.Size([1010])) our function has returned the distance for every single image as a rank-1 tensor of length 1,010. This is because we have added a subtraction (a-b) into our distance function and when PyTorch performs this subtraction, it uses broadcasting which will automatically expand the tensor with smaller rank to have the same size as the one with larger rank. Once this has happened, PyTorch will perform an element wise operation over the two tensors. In our case, PyTorch is treating mean3 (a rank 2 tensor) as if it were 1,010 copies of that tensor. You can see that by performing a subtraction and checking the shape ( valid_3_tens - mean3 ) . shape torch.Size([1010, 28, 28]) There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance: PyTorch doesn't actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn't actually allocate any additional memory It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!). source A funtion to make a decision is_3 is going to use mnist_distance to figure out whether an image is a 3 or a 7. To do this it will check whether the distance between a digit, x and mean3 is less than the difference between x and mean7 . def is_3 ( x ): return mnist_distance ( x , mean3 ) < mnist_distance ( x , mean7 ) # let's test it # you can convert a boolean to a float is_3 ( a_3 ), is_3 ( a_3 ) . float () (tensor(True), tensor(1.)) Calculate the accuracy for each 3 and 7 by taking the average of is_3 for all 3s and it's inverse for all 7s. accuracy_3s = is_3 ( valid_3_tens ) . float () . mean () accuracy_7s = ( 1 - is_3 ( valid_7_tens ) . float ()) . mean () accuracy_3s , accuracy_7s , ( accuracy_3s + accuracy_7s ) / 2 (tensor(0.9168), tensor(0.9854), tensor(0.9511)) Not bad, over 90% accuracy using a very simple model. This was also a very simple problem, 3s and 7s look very different so it's not really a surprise that this was so effective. We will nowe look at a system that will do some learning (automatically modify itself to improve its performance) In the next part, we will implement this more advanced model","title":"Lesson 03 pt 1"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#lesson-3-under-the-hood-training-a-digit-classifier","text":"20-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set.","title":"Lesson 3: Under the Hood: Training a Digit Classifier"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#primer","text":"load in some data visualise it from fastai.vision.all import * from pathlib import Path For this model we are going to create a digit classifier that will be able to classify an image as a 3 or a 7. Fastai has a sample of the MNIST dataset that we will be using. First, let's load the data and check that there are indeed, 3's an 7's in one of the folders. The folder layout is fairly typical, separate training and validation sets. path = untar_data ( URLs . MNIST_SAMPLE ) ( path / 'train' ) . ls () (#2) [Path('/storage/data/mnist_sample/train/7'),Path('/storage/data/mnist_sample/train/3')]","title":"Primer"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#visualising-data","text":"There are a number of ways we can check our data to get a better understanding of how it is structured and how it looks Take a look inside one of the folders and check file names Use PIL to open one of the images Use PyTorch/Numpy to check the tensor/array values Get creative # 1 - check file names threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () threes (#6131) [Path('/storage/data/mnist_sample/train/3/10.png'),Path('/storage/data/mnist_sample/train/3/10000.png'),Path('/storage/data/mnist_sample/train/3/10011.png'),Path('/storage/data/mnist_sample/train/3/10031.png'),Path('/storage/data/mnist_sample/train/3/10034.png'),Path('/storage/data/mnist_sample/train/3/10042.png'),Path('/storage/data/mnist_sample/train/3/10052.png'),Path('/storage/data/mnist_sample/train/3/1007.png'),Path('/storage/data/mnist_sample/train/3/10074.png'),Path('/storage/data/mnist_sample/train/3/10091.png')...] # 2 - use PIL to open image im3_path = threes [ 1 ] im3 = Image . open ( im3_path ) im3 # 3 - use PyTorch to view tensor values tensor ( im3 )[ 4 : 10 , 4 : 10 ] tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) # 4 - Getting creative with Pandas im3_t = tensor ( im3 ) df = pd . DataFrame ( im3_t [ 4 : 15 , 4 : 22 ]) df . style . set_properties ( ** { 'font-size' : '6pt' }) . background_gradient ( 'Greys' ) #T_71814984_fd78_11ea_966e_0242ac110002row0_col0,#T_71814984_fd78_11ea_966e_0242ac110002row0_col1,#T_71814984_fd78_11ea_966e_0242ac110002row0_col2,#T_71814984_fd78_11ea_966e_0242ac110002row0_col3,#T_71814984_fd78_11ea_966e_0242ac110002row0_col4,#T_71814984_fd78_11ea_966e_0242ac110002row0_col5,#T_71814984_fd78_11ea_966e_0242ac110002row0_col6,#T_71814984_fd78_11ea_966e_0242ac110002row0_col7,#T_71814984_fd78_11ea_966e_0242ac110002row0_col8,#T_71814984_fd78_11ea_966e_0242ac110002row0_col9,#T_71814984_fd78_11ea_966e_0242ac110002row0_col10,#T_71814984_fd78_11ea_966e_0242ac110002row0_col11,#T_71814984_fd78_11ea_966e_0242ac110002row0_col12,#T_71814984_fd78_11ea_966e_0242ac110002row0_col13,#T_71814984_fd78_11ea_966e_0242ac110002row0_col14,#T_71814984_fd78_11ea_966e_0242ac110002row0_col15,#T_71814984_fd78_11ea_966e_0242ac110002row0_col16,#T_71814984_fd78_11ea_966e_0242ac110002row0_col17,#T_71814984_fd78_11ea_966e_0242ac110002row1_col0,#T_71814984_fd78_11ea_966e_0242ac110002row1_col1,#T_71814984_fd78_11ea_966e_0242ac110002row1_col2,#T_71814984_fd78_11ea_966e_0242ac110002row1_col3,#T_71814984_fd78_11ea_966e_0242ac110002row1_col4,#T_71814984_fd78_11ea_966e_0242ac110002row1_col15,#T_71814984_fd78_11ea_966e_0242ac110002row1_col16,#T_71814984_fd78_11ea_966e_0242ac110002row1_col17,#T_71814984_fd78_11ea_966e_0242ac110002row2_col0,#T_71814984_fd78_11ea_966e_0242ac110002row2_col1,#T_71814984_fd78_11ea_966e_0242ac110002row2_col2,#T_71814984_fd78_11ea_966e_0242ac110002row2_col15,#T_71814984_fd78_11ea_966e_0242ac110002row2_col16,#T_71814984_fd78_11ea_966e_0242ac110002row2_col17,#T_71814984_fd78_11ea_966e_0242ac110002row3_col0,#T_71814984_fd78_11ea_966e_0242ac110002row3_col15,#T_71814984_fd78_11ea_966e_0242ac110002row3_col16,#T_71814984_fd78_11ea_966e_0242ac110002row3_col17,#T_71814984_fd78_11ea_966e_0242ac110002row4_col0,#T_71814984_fd78_11ea_966e_0242ac110002row4_col6,#T_71814984_fd78_11ea_966e_0242ac110002row4_col7,#T_71814984_fd78_11ea_966e_0242ac110002row4_col8,#T_71814984_fd78_11ea_966e_0242ac110002row4_col9,#T_71814984_fd78_11ea_966e_0242ac110002row4_col10,#T_71814984_fd78_11ea_966e_0242ac110002row4_col15,#T_71814984_fd78_11ea_966e_0242ac110002row4_col16,#T_71814984_fd78_11ea_966e_0242ac110002row4_col17,#T_71814984_fd78_11ea_966e_0242ac110002row5_col0,#T_71814984_fd78_11ea_966e_0242ac110002row5_col5,#T_71814984_fd78_11ea_966e_0242ac110002row5_col6,#T_71814984_fd78_11ea_966e_0242ac110002row5_col7,#T_71814984_fd78_11ea_966e_0242ac110002row5_col8,#T_71814984_fd78_11ea_966e_0242ac110002row5_col9,#T_71814984_fd78_11ea_966e_0242ac110002row5_col15,#T_71814984_fd78_11ea_966e_0242ac110002row5_col16,#T_71814984_fd78_11ea_966e_0242ac110002row5_col17,#T_71814984_fd78_11ea_966e_0242ac110002row6_col0,#T_71814984_fd78_11ea_966e_0242ac110002row6_col1,#T_71814984_fd78_11ea_966e_0242ac110002row6_col2,#T_71814984_fd78_11ea_966e_0242ac110002row6_col3,#T_71814984_fd78_11ea_966e_0242ac110002row6_col4,#T_71814984_fd78_11ea_966e_0242ac110002row6_col5,#T_71814984_fd78_11ea_966e_0242ac110002row6_col6,#T_71814984_fd78_11ea_966e_0242ac110002row6_col7,#T_71814984_fd78_11ea_966e_0242ac110002row6_col8,#T_71814984_fd78_11ea_966e_0242ac110002row6_col9,#T_71814984_fd78_11ea_966e_0242ac110002row6_col14,#T_71814984_fd78_11ea_966e_0242ac110002row6_col15,#T_71814984_fd78_11ea_966e_0242ac110002row6_col16,#T_71814984_fd78_11ea_966e_0242ac110002row6_col17,#T_71814984_fd78_11ea_966e_0242ac110002row7_col0,#T_71814984_fd78_11ea_966e_0242ac110002row7_col1,#T_71814984_fd78_11ea_966e_0242ac110002row7_col2,#T_71814984_fd78_11ea_966e_0242ac110002row7_col3,#T_71814984_fd78_11ea_966e_0242ac110002row7_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col5,#T_71814984_fd78_11ea_966e_0242ac110002row7_col6,#T_71814984_fd78_11ea_966e_0242ac110002row7_col13,#T_71814984_fd78_11ea_966e_0242ac110002row7_col14,#T_71814984_fd78_11ea_966e_0242ac110002row7_col15,#T_71814984_fd78_11ea_966e_0242ac110002row7_col16,#T_71814984_fd78_11ea_966e_0242ac110002row7_col17,#T_71814984_fd78_11ea_966e_0242ac110002row8_col0,#T_71814984_fd78_11ea_966e_0242ac110002row8_col1,#T_71814984_fd78_11ea_966e_0242ac110002row8_col2,#T_71814984_fd78_11ea_966e_0242ac110002row8_col3,#T_71814984_fd78_11ea_966e_0242ac110002row8_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col13,#T_71814984_fd78_11ea_966e_0242ac110002row8_col14,#T_71814984_fd78_11ea_966e_0242ac110002row8_col15,#T_71814984_fd78_11ea_966e_0242ac110002row8_col16,#T_71814984_fd78_11ea_966e_0242ac110002row8_col17,#T_71814984_fd78_11ea_966e_0242ac110002row9_col0,#T_71814984_fd78_11ea_966e_0242ac110002row9_col1,#T_71814984_fd78_11ea_966e_0242ac110002row9_col2,#T_71814984_fd78_11ea_966e_0242ac110002row9_col3,#T_71814984_fd78_11ea_966e_0242ac110002row9_col4,#T_71814984_fd78_11ea_966e_0242ac110002row9_col16,#T_71814984_fd78_11ea_966e_0242ac110002row9_col17,#T_71814984_fd78_11ea_966e_0242ac110002row10_col0,#T_71814984_fd78_11ea_966e_0242ac110002row10_col1,#T_71814984_fd78_11ea_966e_0242ac110002row10_col2,#T_71814984_fd78_11ea_966e_0242ac110002row10_col3,#T_71814984_fd78_11ea_966e_0242ac110002row10_col4,#T_71814984_fd78_11ea_966e_0242ac110002row10_col5,#T_71814984_fd78_11ea_966e_0242ac110002row10_col6,#T_71814984_fd78_11ea_966e_0242ac110002row10_col17{ font-size: 6pt; background-color: #ffffff; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col5{ font-size: 6pt; background-color: #efefef; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col6,#T_71814984_fd78_11ea_966e_0242ac110002row1_col13{ font-size: 6pt; background-color: #7c7c7c; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col7{ font-size: 6pt; background-color: #4a4a4a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col8,#T_71814984_fd78_11ea_966e_0242ac110002row1_col9,#T_71814984_fd78_11ea_966e_0242ac110002row1_col10,#T_71814984_fd78_11ea_966e_0242ac110002row2_col5,#T_71814984_fd78_11ea_966e_0242ac110002row2_col6,#T_71814984_fd78_11ea_966e_0242ac110002row2_col7,#T_71814984_fd78_11ea_966e_0242ac110002row2_col11,#T_71814984_fd78_11ea_966e_0242ac110002row2_col12,#T_71814984_fd78_11ea_966e_0242ac110002row2_col13,#T_71814984_fd78_11ea_966e_0242ac110002row3_col4,#T_71814984_fd78_11ea_966e_0242ac110002row3_col12,#T_71814984_fd78_11ea_966e_0242ac110002row3_col13,#T_71814984_fd78_11ea_966e_0242ac110002row4_col1,#T_71814984_fd78_11ea_966e_0242ac110002row4_col2,#T_71814984_fd78_11ea_966e_0242ac110002row4_col3,#T_71814984_fd78_11ea_966e_0242ac110002row4_col12,#T_71814984_fd78_11ea_966e_0242ac110002row4_col13,#T_71814984_fd78_11ea_966e_0242ac110002row5_col12,#T_71814984_fd78_11ea_966e_0242ac110002row6_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col11,#T_71814984_fd78_11ea_966e_0242ac110002row10_col12,#T_71814984_fd78_11ea_966e_0242ac110002row10_col13,#T_71814984_fd78_11ea_966e_0242ac110002row10_col14,#T_71814984_fd78_11ea_966e_0242ac110002row10_col15,#T_71814984_fd78_11ea_966e_0242ac110002row10_col16{ font-size: 6pt; background-color: #000000; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col11{ font-size: 6pt; background-color: #606060; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col12{ font-size: 6pt; background-color: #4d4d4d; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row1_col14{ font-size: 6pt; background-color: #bbbbbb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col3{ font-size: 6pt; background-color: #e4e4e4; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col4,#T_71814984_fd78_11ea_966e_0242ac110002row8_col6{ font-size: 6pt; background-color: #6b6b6b; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col8,#T_71814984_fd78_11ea_966e_0242ac110002row2_col14,#T_71814984_fd78_11ea_966e_0242ac110002row3_col14{ font-size: 6pt; background-color: #171717; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col9,#T_71814984_fd78_11ea_966e_0242ac110002row3_col11{ font-size: 6pt; background-color: #4b4b4b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row2_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col10,#T_71814984_fd78_11ea_966e_0242ac110002row8_col8,#T_71814984_fd78_11ea_966e_0242ac110002row8_col10,#T_71814984_fd78_11ea_966e_0242ac110002row9_col8,#T_71814984_fd78_11ea_966e_0242ac110002row9_col10{ font-size: 6pt; background-color: #010101; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col1{ font-size: 6pt; background-color: #272727; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col2{ font-size: 6pt; background-color: #0a0a0a; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col3{ font-size: 6pt; background-color: #050505; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col5{ font-size: 6pt; background-color: #333333; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col6{ font-size: 6pt; background-color: #e6e6e6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col7,#T_71814984_fd78_11ea_966e_0242ac110002row3_col10{ font-size: 6pt; background-color: #fafafa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col8{ font-size: 6pt; background-color: #fbfbfb; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row3_col9{ font-size: 6pt; background-color: #fdfdfd; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col4{ font-size: 6pt; background-color: #1b1b1b; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col5{ font-size: 6pt; background-color: #e0e0e0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col11{ font-size: 6pt; background-color: #4e4e4e; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row4_col14{ font-size: 6pt; background-color: #767676; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col1{ font-size: 6pt; background-color: #fcfcfc; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col2,#T_71814984_fd78_11ea_966e_0242ac110002row5_col3{ font-size: 6pt; background-color: #f6f6f6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col4,#T_71814984_fd78_11ea_966e_0242ac110002row7_col7{ font-size: 6pt; background-color: #f8f8f8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col10,#T_71814984_fd78_11ea_966e_0242ac110002row10_col7{ font-size: 6pt; background-color: #e8e8e8; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col11{ font-size: 6pt; background-color: #222222; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col13,#T_71814984_fd78_11ea_966e_0242ac110002row6_col12{ font-size: 6pt; background-color: #090909; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row5_col14{ font-size: 6pt; background-color: #d0d0d0; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col10,#T_71814984_fd78_11ea_966e_0242ac110002row7_col11,#T_71814984_fd78_11ea_966e_0242ac110002row9_col6{ font-size: 6pt; background-color: #060606; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row6_col13{ font-size: 6pt; background-color: #979797; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col8{ font-size: 6pt; background-color: #b6b6b6; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col9{ font-size: 6pt; background-color: #252525; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row7_col12{ font-size: 6pt; background-color: #999999; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col5{ font-size: 6pt; background-color: #f9f9f9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col7{ font-size: 6pt; background-color: #101010; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col9,#T_71814984_fd78_11ea_966e_0242ac110002row9_col9{ font-size: 6pt; background-color: #020202; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col11{ font-size: 6pt; background-color: #545454; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row8_col12{ font-size: 6pt; background-color: #f1f1f1; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col5{ font-size: 6pt; background-color: #f7f7f7; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col7{ font-size: 6pt; background-color: #030303; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col12{ font-size: 6pt; background-color: #181818; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col13{ font-size: 6pt; background-color: #303030; color: #f1f1f1; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col14{ font-size: 6pt; background-color: #a9a9a9; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row9_col15{ font-size: 6pt; background-color: #fefefe; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col8,#T_71814984_fd78_11ea_966e_0242ac110002row10_col9{ font-size: 6pt; background-color: #bababa; color: #000000; }#T_71814984_fd78_11ea_966e_0242ac110002row10_col10{ font-size: 6pt; background-color: #393939; color: #f1f1f1; } 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 29 150 195 254 255 254 176 193 150 96 0 0 0 2 0 0 0 48 166 224 253 253 234 196 253 253 253 253 233 0 0 0 3 0 93 244 249 253 187 46 10 8 4 10 194 253 253 233 0 0 0 4 0 107 253 253 230 48 0 0 0 0 0 192 253 253 156 0 0 0 5 0 3 20 20 15 0 0 0 0 0 43 224 253 245 74 0 0 0 6 0 0 0 0 0 0 0 0 0 0 249 253 245 126 0 0 0 0 7 0 0 0 0 0 0 0 14 101 223 253 248 124 0 0 0 0 0 8 0 0 0 0 0 11 166 239 253 253 253 187 30 0 0 0 0 0 9 0 0 0 0 0 16 248 250 253 253 253 253 232 213 111 2 0 0 10 0 0 0 0 0 0 0 43 98 98 208 253 253 253 253 187 22 0","title":"Visualising Data"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#sidebar-think-about-the-problem","text":"Before jumping into solution mode (ie apply deep learning to everything!) think about the problem space and how you might be able to solve it. For this problem (digit recognition) using a simple average might be enough to get a descent result. That is exactly what we will do first.","title":"Sidebar - think about the problem"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#method-1-pixel-similarity","text":"find the average pixel value for every pixel of the 3s and 7s this will give us 2 group averages that represent the \"ideal\" 3 and 7 to classify a digit, check the similarity against the ideal this method will form our baseline that we will improve upon later","title":"Method 1: Pixel Similarity"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#step-1-organise-data","text":"create a tensor by stacking all of our 3s together we will use list comprehension for this # open all images, convert to tensor, store in list seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] len ( three_tensors ), len ( seven_tensors ) (6131, 6265) # lets check one of show_image ( three_tensors [ 1 ]);","title":"Step 1: Organise data"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#step-2-compute-the-average-pixel-value","text":"For every pixel position, compute the average over all the images of the intensity of that pixel. To do this combine all images into a single 3-dimensional tensor using stack which \" Concatenates sequence of tensors along a new dimension .\" PuyTorch needs us to cast the int values to floats in order to compute the average. \"Generally when images are floats, the pixel values are expected to be between 0 and 1, so we will also divide by 255 here\" source stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 stacked_threes . shape torch.Size([6131, 28, 28]) tensor jargon - rank : the number of axis - shape : the size of each axis print ( 'rank:' , stacked_threes . ndim , ' \\n ' , 'shape:' , stacked_threes . shape ) rank: 3 shape: torch.Size([6131, 28, 28]) We have 6,131 images of size 28x28","title":"Step 2: Compute the average pixel value"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#step-3-compute-the-ideal-digits","text":"compute the mean along the 0th dimension by visualising this ideal 3 we can indeed see that it represents a 3! mean3 = stacked_threes . mean ( dim = 0 ) show_image ( mean3 ); mean7 = stacked_sevens . mean ( dim = 0 ) show_image ( mean7 ); # here is a random three for comparison a_3 = stacked_threes [ 1 ] show_image ( a_3 ); How could we calculate how similar a 3 is from this ideal 3? Typically there are two methods take the absolute value of differences (where there are negatives, replace with postive) This is called the mean absolute difference or L1 norm take the mean squared difference (which also makes all results positive) then take the square root This is called the root mean squared error (RMSE) or L2 norm . # 3 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean3 ) . abs () . mean () dist_3_sqr = (( a_3 - mean3 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1114), tensor(0.2021)) # 7 # L1 Norm # mean absolute difference dist_3_abs = ( a_3 - mean7 ) . abs () . mean () dist_3_sqr = (( a_3 - mean7 ) ** 2 ) . mean () . sqrt () dist_3_abs , dist_3_sqr (tensor(0.1586), tensor(0.3021)) The distance between our \"ideal\" 3 and the real 3 is less than the distance from the real 3 to \"ideal\" 7. This is good - it means both methods will work and our simple model will give the correct prediction. PyTorch already provides these loss functions for us (though RMSE is only MSE, but we can work with that). # check results with PyTorch F . l1_loss ( a_3 . float (), mean7 ), F . mse_loss ( a_3 , mean7 ) . sqrt () (tensor(0.1586), tensor(0.3021))","title":"Step 3: Compute the ideal digits"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#a-simple-model","text":"A validation set is usually used to help avoid overfitting, our model has no trained components so this isn't going to be an issue, but let's stick with best practices. We will also define a function that will decide if an arbitrary image is a 3 or a 7. This will be achieved by measuring the distance between this digit and our ideal digits and determining which ideal it is closer to. valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 valid_3_tens . shape , valid_7_tens . shape (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) mnist_distance calculates the difference between our ideal 3 and every 3 in the validation set. We call mean((-1,-2)) in our fuction. The tuple (-1,-2) represents a range of axes, so the last -1, and second last -2. mean((-1,-2)) says; take the mean over the last two axes in the tensor. Why? These axes represent the verticle and horizontal dimensions of an image. After taking the mean over the these axes, we have one axis left which indexes over the 1,010 images we have. # calculate the mean absolute error def mnist_distance ( a , b ): return ( a - b ) . abs () . mean (( - 1 , - 2 )) # check the function mnist_distance ( a_3 , mean3 ) # great! tensor(0.1114) This works for a single image, but in order to calculate the overall accuracy, we want to calculate this distance to the ideal 3 for all images in the validation set. This can be achieved using a loop, but there is another way - broadcasting. Take a look at the shape of valid_3_tens and mean3 , they are different... valid_3_tens . shape , mean3 . shape (torch.Size([1010, 28, 28]), torch.Size([28, 28])) Now calculate the distance between the two... valid_3_dist = mnist_distance ( valid_3_tens , mean3 ) valid_3_dist , valid_3_dist . shape (tensor([0.1290, 0.1223, 0.1380, ..., 0.1337, 0.1132, 0.1097]), torch.Size([1010])) our function has returned the distance for every single image as a rank-1 tensor of length 1,010. This is because we have added a subtraction (a-b) into our distance function and when PyTorch performs this subtraction, it uses broadcasting which will automatically expand the tensor with smaller rank to have the same size as the one with larger rank. Once this has happened, PyTorch will perform an element wise operation over the two tensors. In our case, PyTorch is treating mean3 (a rank 2 tensor) as if it were 1,010 copies of that tensor. You can see that by performing a subtraction and checking the shape ( valid_3_tens - mean3 ) . shape torch.Size([1010, 28, 28]) There are a couple of important points about how broadcasting is implemented, which make it valuable not just for expressivity but also for performance: PyTorch doesn't actually copy mean3 1,010 times. It pretends it were a tensor of that shape, but doesn't actually allocate any additional memory It does the whole calculation in C (or, if you're using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!). source","title":"A simple model"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%201/#a-funtion-to-make-a-decision","text":"is_3 is going to use mnist_distance to figure out whether an image is a 3 or a 7. To do this it will check whether the distance between a digit, x and mean3 is less than the difference between x and mean7 . def is_3 ( x ): return mnist_distance ( x , mean3 ) < mnist_distance ( x , mean7 ) # let's test it # you can convert a boolean to a float is_3 ( a_3 ), is_3 ( a_3 ) . float () (tensor(True), tensor(1.)) Calculate the accuracy for each 3 and 7 by taking the average of is_3 for all 3s and it's inverse for all 7s. accuracy_3s = is_3 ( valid_3_tens ) . float () . mean () accuracy_7s = ( 1 - is_3 ( valid_7_tens ) . float ()) . mean () accuracy_3s , accuracy_7s , ( accuracy_3s + accuracy_7s ) / 2 (tensor(0.9168), tensor(0.9854), tensor(0.9511)) Not bad, over 90% accuracy using a very simple model. This was also a very simple problem, 3s and 7s look very different so it's not really a surprise that this was so effective. We will nowe look at a system that will do some learning (automatically modify itself to improve its performance) In the next part, we will implement this more advanced model","title":"A funtion to make a decision"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 3: Under the Hood: Training a Digit Classifier 26-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. In part 1 we used a simple model that had no learned components. In this notebook, we will explore a smarter solution. We will apply this method back to the MNIST problem in the next notebook. Stocastic Gradient Descent (SGD) Instead of measuring how close something is to an \"ideal\" image, we could find a set of weights for each pixel, the highest weights will be associated with pixels that are most likely to be black for a particular category (in our case, number). This can be represented by a function and a set of weight values for each possible category - ie the probability of a category being an 8. def prob_eight(x,w) = (x*w)sum() Here x is a vector that represents an image (with all rows stacked up) and w is a vector of weights. With this function, we now just need a way to gradually update the weights to make them better and better until they are as good as they can get. In other words, we want to find the specific values of w that will cause the result of our function to be high when passed images of 8s and low for other digits. So by updating w we are optimising the function to recognise 8s. The steps we will follow are: 1. Initialize the weights. - start out with a random guess. 2. For each image, use these weights to predict whether it appears to be a 3 or a 7. 3. Based on these predictions, calculate how good the model is (its loss). 4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss. 5. Step (that is, update) all the weights based on that calculation. 6. Go back to the step 2, repeat the process. 7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer). source We will use a very simple example for illustration purposes. from fastai.vision.all import * from utils import * # define a quadratic function def f ( x ): return x ** 2 I had some trouble finding the plot_function function so found the source code from the forums. def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title ) # plot that function plot_function ( f , 'x' , 'x**2' ) # start with a random value for a parameter plt . scatter ( - 1.5 , f ( - 1.5 ), color = 'red' ); Now we need to see what would happen if we increase or decrease our parameter by a small amout. The goal, to find the lowest point in the curve. We do this by calculating the gradient at a particular point. We can change our weight by a small amount in the direction of the slope, then calculate the loss, make an adjustment and repeat until we reach our goal. Calculating Gradients The slope of a line can be described as the rate of change of a verticle variable with respect to a horizontal variabe. This is the gradient. By calculating the gradient, it will tell us how much we have to change each weight to make our model better. A Derivative is the instantaneous rate of change at a particular point. So how much is y changing with respect to x at that point. You can achieve this by calculating the slope of a tangent line. This video provides a good explanation of the concept. We can calculate the derivative for any function. For the quadratic above, the derivative is another function that calculates change, rather than the value. If we know how our function changes at a particular value, then we know how to minimize it. \"This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradients of our functions\" source PyTorch helps us do this using vector calculus. eg. # define a vector xt # requires_grad_ lets OyTorch know we need to calculate gradients xt = tensor ([ 3. , 4. , 10. ]) . requires_grad_ () xt tensor([ 3., 4., 10.], requires_grad=True) # define a function x that takes a vector (rank 1 tensor) # and returns a scalar (rank 0 tensor) # do this by summing the result of x**2 def f ( x ): return ( x ** 2 ) . sum () yt = f ( xt ) yt tensor(125., grad_fn=<SumBackward0>) calling backward refers to back propagation, which is the process of calculating the derivative of each layer. We can then use .grad to view the gradients. We can confirm that the derivative of x**2 is 2*x yt . backward () xt . grad tensor([ 6., 8., 20.]) Stepping with a learning rate Now that we know how to calculate the slope of our function, that tells us if we change our input a little bit, how will our out change correspondingly. let's call the weights w to update them we do the following w -= gradient(w) * lr So update w by subtracting the gradient of w multiplied by the learning rate lr . This process is known as, stepping your parameters, using and optimiser step. Defining a good learning rate is one of the key principles in machine learning. An intuitive way to think about it is, if the lr is too small, it will take you forever to reach the goal, if it is too big, you will over shoot that target. End-to-end Gradient Descent example. Use gradient descent to see how finding a minimum can be used to train a model to fir better data.. Let's measure the speed of a rollercoaster as it goes over a rise - starting fast, then slowing down at the peak, then speeding up again. If we measured the speed at 20 second intervals it might look something like this. time = torch . arange ( 0 , 20 ) . float () # speed is a quadratic with some added noise speed = torch . randn ( 20 ) * 3 + 0.75 * ( time - 9.5 ) ** 2 + 1 plt . scatter ( time , speed ); We need to create a function that estimates at any time, the speed of the rollercoaster. Start with a random guess, here let's use a quadratic a*(time**2) + (b*time) + c # here the input t is time, params will be a list a,b,c def f ( t , params ): a , b , c = params return a * ( t ** 2 ) + ( b * t ) + c So the goal is to find some function, or the best imaginable function that fits the data (we have simplified to finding the best quadratic function which is defined by the params a , b and c ). So finding the best f can be achieved by finding the best a , b and c values. We will need a loss function for this task. def mse ( preds , targets ): return (( preds - targets ) ** 2 ) . mean () Implement the 7 steps 1. Initialize the weights (params) to random values. Let PyTorch know that we will need to calculate the gradients. params = torch . randn ( 3 ) . requires_grad_ () orig_params = params . clone () # save these to check later 2. Calculate the predictions using our function Then check how close/far the predictions are from the targets. preds = f ( time , params ) def show_preds ( preds , ax = None ): if ax is None : ax = plt . subplots ()[ 1 ] ax . scatter ( time , speed ) ax . scatter ( time , to_np ( preds ), color = 'r' ) ax . set_ylim ( - 300 , 100 ) show_preds ( preds ) 3. Calculate the loss the goal is to improve the loss so we will need to know the gradients loss = mse ( preds , speed ) loss tensor(4422.5112, grad_fn=<MeanBackward0>) 4. Calculate the gradients Then use these to improve the parameters. We need a learning rate for this loss . backward () params . grad tensor([-20635.2617, -1319.6385, -108.2016]) lr = 1e-5 params . grad * lr tensor([-0.2064, -0.0132, -0.0011]) 5. Step the weights update the parameters based on the gradients we have calculated. Stepping the weights is w -= gradient(w) * lr .data is a special attribute in torch that means we don't want the gradient calculated. Here, we do not want the gradient calculated for the step we are doing, we only want the gradiend of the function f to be calculated params . data -= lr * params . grad . data params . grad = None # delete the gradients we already had # check if loss has improved # previous was 4422.5 preds = f ( time , params ) mse ( preds , speed ) tensor(1354.7021, grad_fn=<MeanBackward0>) show_preds ( preds ) # the preds have indeed improved! # repeat a few times def apply_step ( params , prn = True ): preds = f ( time , params ) loss = mse ( preds , speed ) loss . backward () params . data -= lr * params . grad . data params . grad = None if prn : print ( loss . item ()) return preds 6. Repeat the process by looping through and making improvements # repeat 10 times for i in range ( 10 ): apply_step ( params ) 1354.7021484375 774.1762084960938 664.3204956054688 643.5296630859375 639.5927734375 638.8450317382812 638.7008666992188 638.6709594726562 638.6625366210938 638.6583862304688 We can visualise this to see that for each step, an entirely different quadratic function is being tried params = orig_params . detach () . requires_grad_ () _ , axs = plt . subplots ( 1 , 4 , figsize = ( 12 , 3 )) for ax in axs : show_preds ( apply_step ( params , False ), ax ) plt . tight_layout () 7. Stop Summary We have just seen that by comparing the outputs of our model to our targets using a loss function, we are able to minimize the loss by gradually improving our weights (params).","title":"Lesson 03 pt 2"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#lesson-3-under-the-hood-training-a-digit-classifier","text":"26-09-2020 This notebook will go over some of the practical material discussed in lesson 3 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. In part 1 we used a simple model that had no learned components. In this notebook, we will explore a smarter solution. We will apply this method back to the MNIST problem in the next notebook.","title":"Lesson 3: Under the Hood: Training a Digit Classifier"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#stocastic-gradient-descent-sgd","text":"Instead of measuring how close something is to an \"ideal\" image, we could find a set of weights for each pixel, the highest weights will be associated with pixels that are most likely to be black for a particular category (in our case, number). This can be represented by a function and a set of weight values for each possible category - ie the probability of a category being an 8. def prob_eight(x,w) = (x*w)sum() Here x is a vector that represents an image (with all rows stacked up) and w is a vector of weights. With this function, we now just need a way to gradually update the weights to make them better and better until they are as good as they can get. In other words, we want to find the specific values of w that will cause the result of our function to be high when passed images of 8s and low for other digits. So by updating w we are optimising the function to recognise 8s. The steps we will follow are: 1. Initialize the weights. - start out with a random guess. 2. For each image, use these weights to predict whether it appears to be a 3 or a 7. 3. Based on these predictions, calculate how good the model is (its loss). 4. Calculate the gradient, which measures for each weight, how changing that weight would change the loss. 5. Step (that is, update) all the weights based on that calculation. 6. Go back to the step 2, repeat the process. 7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer). source We will use a very simple example for illustration purposes. from fastai.vision.all import * from utils import * # define a quadratic function def f ( x ): return x ** 2 I had some trouble finding the plot_function function so found the source code from the forums. def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title ) # plot that function plot_function ( f , 'x' , 'x**2' ) # start with a random value for a parameter plt . scatter ( - 1.5 , f ( - 1.5 ), color = 'red' ); Now we need to see what would happen if we increase or decrease our parameter by a small amout. The goal, to find the lowest point in the curve. We do this by calculating the gradient at a particular point. We can change our weight by a small amount in the direction of the slope, then calculate the loss, make an adjustment and repeat until we reach our goal.","title":"Stocastic Gradient Descent (SGD)"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#calculating-gradients","text":"The slope of a line can be described as the rate of change of a verticle variable with respect to a horizontal variabe. This is the gradient. By calculating the gradient, it will tell us how much we have to change each weight to make our model better. A Derivative is the instantaneous rate of change at a particular point. So how much is y changing with respect to x at that point. You can achieve this by calculating the slope of a tangent line. This video provides a good explanation of the concept. We can calculate the derivative for any function. For the quadratic above, the derivative is another function that calculates change, rather than the value. If we know how our function changes at a particular value, then we know how to minimize it. \"This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradients of our functions\" source PyTorch helps us do this using vector calculus. eg. # define a vector xt # requires_grad_ lets OyTorch know we need to calculate gradients xt = tensor ([ 3. , 4. , 10. ]) . requires_grad_ () xt tensor([ 3., 4., 10.], requires_grad=True) # define a function x that takes a vector (rank 1 tensor) # and returns a scalar (rank 0 tensor) # do this by summing the result of x**2 def f ( x ): return ( x ** 2 ) . sum () yt = f ( xt ) yt tensor(125., grad_fn=<SumBackward0>) calling backward refers to back propagation, which is the process of calculating the derivative of each layer. We can then use .grad to view the gradients. We can confirm that the derivative of x**2 is 2*x yt . backward () xt . grad tensor([ 6., 8., 20.])","title":"Calculating Gradients"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#stepping-with-a-learning-rate","text":"Now that we know how to calculate the slope of our function, that tells us if we change our input a little bit, how will our out change correspondingly. let's call the weights w to update them we do the following w -= gradient(w) * lr So update w by subtracting the gradient of w multiplied by the learning rate lr . This process is known as, stepping your parameters, using and optimiser step. Defining a good learning rate is one of the key principles in machine learning. An intuitive way to think about it is, if the lr is too small, it will take you forever to reach the goal, if it is too big, you will over shoot that target.","title":"Stepping with a learning rate"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#end-to-end-gradient-descent-example","text":"Use gradient descent to see how finding a minimum can be used to train a model to fir better data.. Let's measure the speed of a rollercoaster as it goes over a rise - starting fast, then slowing down at the peak, then speeding up again. If we measured the speed at 20 second intervals it might look something like this. time = torch . arange ( 0 , 20 ) . float () # speed is a quadratic with some added noise speed = torch . randn ( 20 ) * 3 + 0.75 * ( time - 9.5 ) ** 2 + 1 plt . scatter ( time , speed ); We need to create a function that estimates at any time, the speed of the rollercoaster. Start with a random guess, here let's use a quadratic a*(time**2) + (b*time) + c # here the input t is time, params will be a list a,b,c def f ( t , params ): a , b , c = params return a * ( t ** 2 ) + ( b * t ) + c So the goal is to find some function, or the best imaginable function that fits the data (we have simplified to finding the best quadratic function which is defined by the params a , b and c ). So finding the best f can be achieved by finding the best a , b and c values. We will need a loss function for this task. def mse ( preds , targets ): return (( preds - targets ) ** 2 ) . mean ()","title":"End-to-end Gradient Descent example."},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#implement-the-7-steps","text":"1. Initialize the weights (params) to random values. Let PyTorch know that we will need to calculate the gradients. params = torch . randn ( 3 ) . requires_grad_ () orig_params = params . clone () # save these to check later 2. Calculate the predictions using our function Then check how close/far the predictions are from the targets. preds = f ( time , params ) def show_preds ( preds , ax = None ): if ax is None : ax = plt . subplots ()[ 1 ] ax . scatter ( time , speed ) ax . scatter ( time , to_np ( preds ), color = 'r' ) ax . set_ylim ( - 300 , 100 ) show_preds ( preds ) 3. Calculate the loss the goal is to improve the loss so we will need to know the gradients loss = mse ( preds , speed ) loss tensor(4422.5112, grad_fn=<MeanBackward0>) 4. Calculate the gradients Then use these to improve the parameters. We need a learning rate for this loss . backward () params . grad tensor([-20635.2617, -1319.6385, -108.2016]) lr = 1e-5 params . grad * lr tensor([-0.2064, -0.0132, -0.0011]) 5. Step the weights update the parameters based on the gradients we have calculated. Stepping the weights is w -= gradient(w) * lr .data is a special attribute in torch that means we don't want the gradient calculated. Here, we do not want the gradient calculated for the step we are doing, we only want the gradiend of the function f to be calculated params . data -= lr * params . grad . data params . grad = None # delete the gradients we already had # check if loss has improved # previous was 4422.5 preds = f ( time , params ) mse ( preds , speed ) tensor(1354.7021, grad_fn=<MeanBackward0>) show_preds ( preds ) # the preds have indeed improved! # repeat a few times def apply_step ( params , prn = True ): preds = f ( time , params ) loss = mse ( preds , speed ) loss . backward () params . data -= lr * params . grad . data params . grad = None if prn : print ( loss . item ()) return preds 6. Repeat the process by looping through and making improvements # repeat 10 times for i in range ( 10 ): apply_step ( params ) 1354.7021484375 774.1762084960938 664.3204956054688 643.5296630859375 639.5927734375 638.8450317382812 638.7008666992188 638.6709594726562 638.6625366210938 638.6583862304688 We can visualise this to see that for each step, an entirely different quadratic function is being tried params = orig_params . detach () . requires_grad_ () _ , axs = plt . subplots ( 1 , 4 , figsize = ( 12 , 3 )) for ax in axs : show_preds ( apply_step ( params , False ), ax ) plt . tight_layout () 7. Stop","title":"Implement the 7 steps"},{"location":"fastai%20deep%20learning%202020/lesson%2003%20pt%202/#summary","text":"We have just seen that by comparing the outputs of our model to our targets using a loss function, we are able to minimize the loss by gradually improving our weights (params).","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2004/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 4: Under the Hood: Training a Digit Classifier 28-09-2020 This notebook will go over some of the practical material discussed in lesson 4 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. The lesson 4 video is an extension on the lesson 3 video. There is a lot to cover... In the last notebook we looked at some simple examples of using SGD to optimise a model. In this notebook we will apply the concepts to the MNIST problem from scratch then leter, we will refactor the code using PyTorch and fastai modules. # imports and things we need from previous notebooks from fastai.vision.all import * # data path = untar_data ( URLs . MNIST_SAMPLE ) threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title ) MNIST Loss function Our X values will be pixels, we need to reshape the data using view . We want to concatenate our x's into a single tensor, then change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). Why? Because this example is meant to be simplified. view will return a new tensor with the same data as the original tensor but with a different shape that we define. # concat 3s and 7s, then reshape into a matrix # so that each row is 1 image, with all rows and columns in a single vector train_x = torch . cat ([ stacked_threes , stacked_sevens ]) . view ( - 1 , 28 * 28 ) # label the data # 3 == 1 # 7 == 0 # we need this to be a matrix # unsqueeze will do this for us train_y = tensor ([ 1 ] * len ( threes ) + [ 0 ] * len ( sevens )) . unsqueeze ( 1 ) # check the shape train_x . shape , train_y . shape (torch.Size([12396, 784]), torch.Size([12396, 1])) # in PyTorch we need data to be in a tuple for each row # zip will help us with this dset = list ( zip ( train_x , train_y )) # take a look at the first thing x , y = dset [ 0 ] x . shape , y (torch.Size([784]), tensor([1])) (torch.Size([784]), tensor([1])) this matches what we would expect # repeat for validation valid_x = torch . cat ([ valid_3_tens , valid_7_tens ]) . view ( - 1 , 28 * 28 ) valid_y = tensor ([ 1 ] * len ( valid_3_tens ) + [ 0 ] * len ( valid_7_tens )) . unsqueeze ( 1 ) valid_dset = list ( zip ( valid_x , valid_y )) Now we have training and validation data sets 1. Randomly initialise weights for each pixel - use torch.randn to create tensor of randomly initialised weights def init_params ( size , var = 1.0 ): return ( torch . randn ( size ) * var ) . requires_grad_ () weights = init_params (( 28 * 28 , 1 )) weights . shape torch.Size([784, 1]) We need to add a bias term because just using weights*pixels will not be flexible enough. Our function will always be equal to zero when the pixels are equal to zero. bias = init_params ( 1 ) y = w*x+b is the formula for a line, where w are the weights, b is the bias. In neural network jargon, the weights and bias will be our parameters . This linear equation is one of the two fundamental equations of any neural network. The other is an activation function that we will see shortly. Let's use this to calculate a prediction for one image... weights.T will transpose the weights, this is done to make sure the rows and columns match up for our multiplication ( train_x [ 0 ] * weights . T ) . sum () + bias tensor([13.3326], grad_fn=<AddBackward0>) Now we need to do this for all images. A for loop will be too slow. In PyTorch we can perform matrix multiplication using the @ operator OR by using torch.matmul() . # define a linear function that will # multiple the input by weights then add a bias term def linear1 ( xb ): return xb @weights + bias preds = linear1 ( train_x ) preds tensor([[13.3326], [ 9.1011], [ 9.4999], ..., [-1.0068], [15.9130], [12.6228]], grad_fn=<AddBackward0>) Notice the result are the same as we just saw above. We can confirm our function is working and can also see that the operation is performed for every image in train_x checking accuracy - if a prediction is above the threshold, ie if > 0 then it is a 3, less than 0, 7. - so we check if a prediction is greater than our threshold of 0, then check these against the validation set. - this will return true when a row is correctly predicted - we can convert these to floats using .float() then take their mean to check overall accuracy of our randomly initialised model threshold = 0.0 accuracy = ( preds > threshold ) . float () == train_y accuracy tensor([[ True], [ True], [ True], ..., [ True], [False], [False]]) accuracy . float () . mean () . item () 0.484188437461853 Let's change one of the weights by a small amount to see how accuracy is affected. weights [ 0 ] += 1.0001 # increase the weigh a little preds = linear1 ( train_x ) accuracy2 = (( preds > threshold ) . float () == train_y ) . float () . mean () . item () accuracy2 0.484188437461853 This is exactly the same as before. We have a problem, when we calculate the change, our gradient is now 0, this is because if we change a single pixel by a very small amount we might not change an actual prediction. So because our gradient is 0, our step will be 0 which means our prediction will be unchanged. So our accuracy loss function is not very good. A small change in our weights does not result in a small change in accuracy, so we will have zero gradients. We need a new function that won't have a zero gradient, it needs to be more sensitive to small changes, so that a slightly better prediction needs to have a slightly better loss. In other words, then the predictions are close to the targets the loss needs to be small, when they are far away, it needs to be big. So let's create a new function to address this issue. # MNIST loss def mnist_loss ( preds , targets ): return torch . where ( targets == 1. , 1. - preds , preds ) . mean () # test case t = torch . tensor ([ 1 , 0 , 1 ]) # targets p = torch . tensor ([ 0.9 , 0.4 , 0.2 ]) # predictions # this is the same as mnist_loss but before the mean torch . where ( t == 1 , 1 - p , p ) tensor([0.1000, 0.4000, 0.8000]) torch.where is like list comprehension for tensors. This function returns a lower loss when predictions are more accurate and a higher loss when they are not. But for this to work, we need our predictions to be between 0 and 1, otherwise things do not work. p2 = torch . tensor ([ 1.2 , - 1 , 0 ]) # predictions outside 0, 1 range torch . where ( t == 1 , 1 - p2 , p2 ) tensor([-0.2000, -1.0000, 1.0000]) The Sigmoid function This function will constrain our numbers between 0 and 1. It squashes any input in the range (-inf, inf) to some value in the range (0, 1) def sigmoid ( x ) : return 1 / ( 1 + torch . exp ( - x )) plot_function ( torch . sigmoid , title = 'Sigmoid' , min =- 4 , max = 4 ) # MNIST loss with sigmoid def mnist_loss ( predictions , targets ): preds = predictions . sigmoid () return torch . where ( targets == 1. , 1. - preds , preds ) . mean () SGD and Mini-batches By batching images and running computations over them is a way to compromise between speed and computational efficiency. The size of the batch will impact your accuracy and estimates as well as the speed at which you are able to run computations. The batch size is something to be considered during training. The DataLoader class in pytorch helps with batching. It returns an iterator which we can loop through. coll = range ( 15 ) dl = DataLoader ( coll , batch_size = 5 , shuffle = True ) list ( dl ) [tensor([ 4, 12, 5, 6, 3]), tensor([10, 9, 2, 0, 14]), tensor([ 7, 13, 8, 11, 1])] Putting it together # re-initialise weights and params weights = init_params (( 28 * 28 , 1 )) bias = init_params ( 1 ) # create a data loader dl = DataLoader ( dset , batch_size = 256 ) # grab the first x and y xb , yb = first ( dl ) # check the shape xb . shape , yb . shape (torch.Size([256, 784]), torch.Size([256, 1])) # repeat for validation set valid_dl = DataLoader ( valid_dset , batch_size = 256 ) # grab a mini batch to test on batch = train_x [: 4 ] batch . shape torch.Size([4, 784]) # make some predictions preds = linear1 ( batch ) preds tensor([[-1.1306], [-3.9293], [-0.6736], [-6.9805]], grad_fn=<AddBackward0>) loss = mnist_loss ( preds , train_y [: 4 ]) loss tensor(0.8495, grad_fn=<MeanBackward0>) # calculate gradients loss . backward () weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0153), tensor([-0.1070])) # take those 3 steps and put it in a function def calc_grad ( xb , yb , model ): preds = model ( xb ) loss = mnist_loss ( preds , yb ) loss . backward () # test it calc_grad ( batch , train_y [: 4 ], linear1 ) weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0306), tensor([-0.2140])) # zero the gradients weights . grad . zero_ () bias . grad . zero_ () tensor([0.]) The last step is to work out how to update the weights and bias based on the gradient and learning rate. train_epoch loops through the data loader, grab x batch and y batch, calculate the gradient, make a prediction and calculate the loss. Go through each parameter (weights and bias) and for each update with gradient * lr, then zero these in prep for the next loop. p.data is used because PyTorch keeps track of all operations so it can calculate the gradients, but we do not want the gradients to be calculated on the gradient descent step. def train_epoch ( model , lr , params ): for xb , yb in dl : calc_grad ( xb , yb , model ) for p in params : p . data -= p . grad * lr p . grad . zero_ () batch_accuracy is similar to the previous loss function, but since we use a sigmoid, which constrains our preds between 0 and 1, we need to check whether preds > 0.5. def batch_accuracy ( xb , yb ): preds = xb . sigmoid () correct = ( preds > 0.5 ) == yb # check predictions against target return correct . float () . mean () batch_accuracy ( linear1 ( train_x [: 4 ]), train_y [: 4 ]) tensor(0.) # check accuracy for every batch in the validation set # stack converts the list of items into tensor def validate_epoch ( model ): accs = [ batch_accuracy ( model ( xb ), yb ) for xb , yb in valid_dl ] return round ( torch . stack ( accs ) . mean () . item (), 4 ) validate_epoch ( linear1 ) 0.407 This is a starting point, let's train for one epoch and see if accuracy improves. as a reminder, the linear1 function was... - def linear1(xb): return xb@weights + bias lr = 1. params = weights , bias train_epoch ( linear1 , lr , params ) validate_epoch ( linear1 ) 0.6932 for i in range ( 20 ): train_epoch ( linear1 , lr , params ) print ( validate_epoch ( linear1 ), end = ' ' ) 0.8242 0.9042 0.9355 0.9501 0.9555 0.9614 0.9638 0.9677 0.9736 0.9751 0.9751 0.976 0.977 0.9775 0.9775 0.978 0.9785 0.979 0.9795 0.979 Accuracy has indeed improved! We have built an SGD optimizer that has reached about 97% accuracy. Refactor and clean up create an optimiser use PyTorch modules and functions where available like nn.Linear which \"Applies a linear transformation to the incoming data: $y = xA^T + b$\" nn . Linear ? # remove our linear function # in place for torch module # creates a matrix of size 28*28 # with bias of 1 linear_model = nn . Linear ( 28 * 28 , 1 ) # check model params w , b = linear_model . parameters () w . shape , b . shape (torch.Size([1, 784]), torch.Size([1])) Create a basic optimiser pass in params to optimise and lr store these away step though each param (weights and bias) and for each, update with gradient * lr zero the gradients in prep for the next step class BasicOptim : def __init__ ( self , params , lr ): self . params , self . lr = list ( params ), lr def step ( self , * args , ** kwargs ): for p in self . params : p . data -= p . grad . data * self . lr def zero_grad ( self , * args , ** kwargs ): for p in self . params : p . grad = None # create an optimiser by passing in parameters from model opt = BasicOptim ( linear_model . parameters (), lr ) # simplify the training loop def train_epoch ( model ): for xb , yb in dl : calc_grad ( xb , yb , model ) opt . step () opt . zero_grad () validate_epoch ( linear_model ) 0.5075 Now create a function train_model that will call train_epoch on our model for the specified number of epochs def train_model ( model , epochs ): for i in range ( epochs ): train_epoch ( model ) print ( validate_epoch ( model ), end = ' ' ) train_model ( linear_model , 20 ) 0.4932 0.7876 0.852 0.916 0.9345 0.9497 0.957 0.9638 0.9658 0.9677 0.9697 0.9721 0.9731 0.9751 0.9755 0.9765 0.9775 0.9775 0.978 0.9785 The results are very similar to what we have seen before. Fastai provides SGD that we can use instead of writing our own, again the results are very similar. linear_model = nn . Linear ( 28 * 28 , 1 ) opt = SGD ( linear_model . parameters (), lr ) train_model ( linear_model , 20 ) 0.4932 0.9091 0.8056 0.9043 0.9316 0.9443 0.9546 0.9619 0.9648 0.9668 0.9692 0.9707 0.9731 0.9746 0.976 0.976 0.9775 0.9775 0.9785 0.979 Let's refactor some more, using some fastai classes. The Learner implements everything we have implemented manually. # Previously we used DataLoader not DataLoaders dls = DataLoaders ( dl , valid_dl ) learn = Learner ( dls , nn . Linear ( 28 * 28 , 1 ), opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 10 ) epoch train_loss valid_loss batch_accuracy time 0 0.480832 0.461122 0.843474 00:00 1 0.466804 0.441299 0.908734 00:00 2 0.450758 0.422136 0.934249 00:00 3 0.433590 0.403750 0.943572 00:00 4 0.416052 0.386223 0.949460 00:00 5 0.398675 0.369610 0.952404 00:00 6 0.381799 0.353943 0.956820 00:00 7 0.365630 0.339228 0.957311 00:00 8 0.350289 0.325455 0.959764 00:00 9 0.335835 0.312599 0.960255 00:00 The results again are very similar, but with some additional functionality (like printing out results in a pretty table! Non-Linearity To create a simple neural net, using a linear function like we did before is not enough. We need to add in a non-linearity between two linear functions. This is the basic definition for a neural net.. The universal approximation theorem says, that given any arbitrarily complex continuous function, we can approximate it with a neural network. I found this useful for visualising how this works. This is what we are trying to do. In our basic_net , each line represents a layer in our network, the first and 3rd layers are known as linear layers the second, as a nonlinearity or an activation. res.max(tensor(0.0)) takes the result of our linear function and sets any negative value to 0.0 while maintaining any positive values. def basic_net ( xb ): res = xb @w1 + b1 res = res . max ( tensor ( 0.0 )) res = res @w2 + b2 return res plot_function ( F . relu ) Like we have seen previously.. - w1 and w2 are weight tensors - b1 and b2 are bias tensors we can initialise these the same as we have done previously.. w1 has 30 output activations, so in order for w2 to match it require 30 input activations. w1 = init_params (( 28 * 28 , 30 )) b1 = init_params ( 30 ) w2 = init_params (( 30 , 1 )) b2 = init_params ( 1 ) We can simplify further using PyTorch... What we did in basic_net was called function composition, where we passed the results of one function into another function and then into another function. This is what neural nets are doing with linear layers and activation functions. nn.Sequential() will do this for us... simple_net = nn . Sequential ( nn . Linear ( 28 * 28 , 30 ), # 28*28 in, 30 out nn . ReLU (), nn . Linear ( 30 , 1 ) # 30 in 1 out ) learn = Learner ( dls , simple_net , opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 40 , 0.1 ) epoch train_loss valid_loss batch_accuracy time 0 0.301185 0.414520 0.506379 00:00 1 0.142869 0.223012 0.814033 00:00 2 0.079959 0.114103 0.916094 00:00 3 0.053115 0.077652 0.939156 00:00 4 0.040578 0.060868 0.953876 00:00 5 0.034118 0.051373 0.963690 00:00 6 0.030368 0.045362 0.965653 00:00 7 0.027905 0.041246 0.965162 00:00 8 0.026117 0.038246 0.968597 00:00 9 0.024726 0.035950 0.969087 00:00 10 0.023596 0.034124 0.971050 00:00 11 0.022651 0.032629 0.972031 00:00 12 0.021847 0.031376 0.973503 00:00 13 0.021151 0.030301 0.974485 00:00 14 0.020542 0.029363 0.974485 00:00 15 0.020002 0.028535 0.975957 00:00 16 0.019519 0.027797 0.976448 00:00 17 0.019083 0.027134 0.976938 00:00 18 0.018687 0.026535 0.977920 00:00 19 0.018325 0.025991 0.978901 00:00 20 0.017992 0.025495 0.978901 00:00 21 0.017684 0.025040 0.978901 00:00 22 0.017398 0.024621 0.979392 00:00 23 0.017131 0.024233 0.979392 00:00 24 0.016881 0.023874 0.980373 00:00 25 0.016645 0.023541 0.980373 00:00 26 0.016424 0.023232 0.980373 00:00 27 0.016214 0.022943 0.980864 00:00 28 0.016015 0.022673 0.980864 00:00 29 0.015827 0.022421 0.981354 00:00 30 0.015648 0.022185 0.981845 00:00 31 0.015476 0.021963 0.982336 00:00 32 0.015313 0.021756 0.982336 00:00 33 0.015156 0.021561 0.982336 00:00 34 0.015006 0.021378 0.982826 00:00 35 0.014863 0.021204 0.982826 00:00 36 0.014725 0.021041 0.982336 00:00 37 0.014592 0.020886 0.982336 00:00 38 0.014464 0.020740 0.982336 00:00 39 0.014341 0.020601 0.982336 00:00 # this is what our model now looks like learn . model Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) # plot the loss learn . recorder . plot_loss () # learn.recorder.values hold the table values above # lets plot the accuracy plt . plot ( L ( learn . recorder . values ) . itemgot ( 2 )); Looking inside... # let's visualise some of the parameters # 1. grab your model m = learn . model # (0): Linear(in_features=784, out_features=30, bias=True) # 2. look inside and grab the weights and biases w , b = m [ 0 ] . parameters () # 3. grab first (or any) row, reshape, and plot show_image ( w [ 0 ] . view ( 28 , 28 ), figsize = ( 4 , 4 )) <AxesSubplot:> fastai in full from fastai.vision.all import * from pathlib import Path path = Path . cwd () / 'datasets/fastai/mnist_sample' dls = ImageDataLoaders . from_folder ( path ) learn = cnn_learner ( dls , resnet18 , pretrained = False , loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 1 , 0.1 ) epoch train_loss valid_loss accuracy time 0 0.086805 0.025215 0.994603 00:11 Summary We have gone over creating and training a neural network from scratch using the simple example of a digit classifier. The key idea for the last few notebooks was to start with planning out the problem and identifying a way to solve it using a simple common sense solution - the pixel similarity model. This proved successful but it was not really robust beyond the straightforward example we chose - identifying 3s and 7s. We then implemented a more complex solution that could be applied to more complicated problems. After each step or concept had been implemented manually, we refactored the code to use convenient PyTorch functions and modules, eventually ending up with using fastai's implementation which abstracts away from all of the underlying heavy lifting. This is done for convenience and in my own opinion, to help lower the entry barrier into deep learning. Ultimately I believe it is fundamentally important to understand the concepts and implementation if your goal (and this is my goal) is to implement deep learning solutions to solve business problems within your industry.","title":"Lesson 04"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#lesson-4-under-the-hood-training-a-digit-classifier","text":"28-09-2020 This notebook will go over some of the practical material discussed in lesson 4 of the fastai 2020 course, namely, some different ways of training a digit classifier using the MNIST data set. The lesson 4 video is an extension on the lesson 3 video. There is a lot to cover... In the last notebook we looked at some simple examples of using SGD to optimise a model. In this notebook we will apply the concepts to the MNIST problem from scratch then leter, we will refactor the code using PyTorch and fastai modules. # imports and things we need from previous notebooks from fastai.vision.all import * # data path = untar_data ( URLs . MNIST_SAMPLE ) threes = ( path / 'train' / '3' ) . ls () . sorted () sevens = ( path / 'train' / '7' ) . ls () . sorted () seven_tensors = [ tensor ( Image . open ( o )) for o in sevens ] three_tensors = [ tensor ( Image . open ( o )) for o in threes ] stacked_sevens = torch . stack ( seven_tensors ) . float () / 255 stacked_threes = torch . stack ( three_tensors ) . float () / 255 valid_3_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '3' ) . ls ()]) valid_3_tens = valid_3_tens . float () / 255 valid_7_tens = torch . stack ([ tensor ( Image . open ( o )) for o in ( path / 'valid' / '7' ) . ls ()]) valid_7_tens = valid_7_tens . float () / 255 def plot_function ( f , tx = None , ty = None , title = None , min =- 2 , max = 2 , figsize = ( 6 , 4 )): x = torch . linspace ( min , max ) fig , ax = plt . subplots ( figsize = figsize ) ax . plot ( x , f ( x )) if tx is not None : ax . set_xlabel ( tx ) if ty is not None : ax . set_ylabel ( ty ) if title is not None : ax . set_title ( title )","title":"Lesson 4: Under the Hood: Training a Digit Classifier"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#mnist-loss-function","text":"Our X values will be pixels, we need to reshape the data using view . We want to concatenate our x's into a single tensor, then change them from a list of matrices (a rank-3 tensor) to a list of vectors (a rank-2 tensor). Why? Because this example is meant to be simplified. view will return a new tensor with the same data as the original tensor but with a different shape that we define. # concat 3s and 7s, then reshape into a matrix # so that each row is 1 image, with all rows and columns in a single vector train_x = torch . cat ([ stacked_threes , stacked_sevens ]) . view ( - 1 , 28 * 28 ) # label the data # 3 == 1 # 7 == 0 # we need this to be a matrix # unsqueeze will do this for us train_y = tensor ([ 1 ] * len ( threes ) + [ 0 ] * len ( sevens )) . unsqueeze ( 1 ) # check the shape train_x . shape , train_y . shape (torch.Size([12396, 784]), torch.Size([12396, 1])) # in PyTorch we need data to be in a tuple for each row # zip will help us with this dset = list ( zip ( train_x , train_y )) # take a look at the first thing x , y = dset [ 0 ] x . shape , y (torch.Size([784]), tensor([1])) (torch.Size([784]), tensor([1])) this matches what we would expect # repeat for validation valid_x = torch . cat ([ valid_3_tens , valid_7_tens ]) . view ( - 1 , 28 * 28 ) valid_y = tensor ([ 1 ] * len ( valid_3_tens ) + [ 0 ] * len ( valid_7_tens )) . unsqueeze ( 1 ) valid_dset = list ( zip ( valid_x , valid_y )) Now we have training and validation data sets 1. Randomly initialise weights for each pixel - use torch.randn to create tensor of randomly initialised weights def init_params ( size , var = 1.0 ): return ( torch . randn ( size ) * var ) . requires_grad_ () weights = init_params (( 28 * 28 , 1 )) weights . shape torch.Size([784, 1]) We need to add a bias term because just using weights*pixels will not be flexible enough. Our function will always be equal to zero when the pixels are equal to zero. bias = init_params ( 1 ) y = w*x+b is the formula for a line, where w are the weights, b is the bias. In neural network jargon, the weights and bias will be our parameters . This linear equation is one of the two fundamental equations of any neural network. The other is an activation function that we will see shortly. Let's use this to calculate a prediction for one image... weights.T will transpose the weights, this is done to make sure the rows and columns match up for our multiplication ( train_x [ 0 ] * weights . T ) . sum () + bias tensor([13.3326], grad_fn=<AddBackward0>) Now we need to do this for all images. A for loop will be too slow. In PyTorch we can perform matrix multiplication using the @ operator OR by using torch.matmul() . # define a linear function that will # multiple the input by weights then add a bias term def linear1 ( xb ): return xb @weights + bias preds = linear1 ( train_x ) preds tensor([[13.3326], [ 9.1011], [ 9.4999], ..., [-1.0068], [15.9130], [12.6228]], grad_fn=<AddBackward0>) Notice the result are the same as we just saw above. We can confirm our function is working and can also see that the operation is performed for every image in train_x checking accuracy - if a prediction is above the threshold, ie if > 0 then it is a 3, less than 0, 7. - so we check if a prediction is greater than our threshold of 0, then check these against the validation set. - this will return true when a row is correctly predicted - we can convert these to floats using .float() then take their mean to check overall accuracy of our randomly initialised model threshold = 0.0 accuracy = ( preds > threshold ) . float () == train_y accuracy tensor([[ True], [ True], [ True], ..., [ True], [False], [False]]) accuracy . float () . mean () . item () 0.484188437461853 Let's change one of the weights by a small amount to see how accuracy is affected. weights [ 0 ] += 1.0001 # increase the weigh a little preds = linear1 ( train_x ) accuracy2 = (( preds > threshold ) . float () == train_y ) . float () . mean () . item () accuracy2 0.484188437461853 This is exactly the same as before. We have a problem, when we calculate the change, our gradient is now 0, this is because if we change a single pixel by a very small amount we might not change an actual prediction. So because our gradient is 0, our step will be 0 which means our prediction will be unchanged. So our accuracy loss function is not very good. A small change in our weights does not result in a small change in accuracy, so we will have zero gradients. We need a new function that won't have a zero gradient, it needs to be more sensitive to small changes, so that a slightly better prediction needs to have a slightly better loss. In other words, then the predictions are close to the targets the loss needs to be small, when they are far away, it needs to be big. So let's create a new function to address this issue. # MNIST loss def mnist_loss ( preds , targets ): return torch . where ( targets == 1. , 1. - preds , preds ) . mean () # test case t = torch . tensor ([ 1 , 0 , 1 ]) # targets p = torch . tensor ([ 0.9 , 0.4 , 0.2 ]) # predictions # this is the same as mnist_loss but before the mean torch . where ( t == 1 , 1 - p , p ) tensor([0.1000, 0.4000, 0.8000]) torch.where is like list comprehension for tensors. This function returns a lower loss when predictions are more accurate and a higher loss when they are not. But for this to work, we need our predictions to be between 0 and 1, otherwise things do not work. p2 = torch . tensor ([ 1.2 , - 1 , 0 ]) # predictions outside 0, 1 range torch . where ( t == 1 , 1 - p2 , p2 ) tensor([-0.2000, -1.0000, 1.0000])","title":"MNIST Loss function"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#the-sigmoid-function","text":"This function will constrain our numbers between 0 and 1. It squashes any input in the range (-inf, inf) to some value in the range (0, 1) def sigmoid ( x ) : return 1 / ( 1 + torch . exp ( - x )) plot_function ( torch . sigmoid , title = 'Sigmoid' , min =- 4 , max = 4 ) # MNIST loss with sigmoid def mnist_loss ( predictions , targets ): preds = predictions . sigmoid () return torch . where ( targets == 1. , 1. - preds , preds ) . mean ()","title":"The Sigmoid function"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#sgd-and-mini-batches","text":"By batching images and running computations over them is a way to compromise between speed and computational efficiency. The size of the batch will impact your accuracy and estimates as well as the speed at which you are able to run computations. The batch size is something to be considered during training. The DataLoader class in pytorch helps with batching. It returns an iterator which we can loop through. coll = range ( 15 ) dl = DataLoader ( coll , batch_size = 5 , shuffle = True ) list ( dl ) [tensor([ 4, 12, 5, 6, 3]), tensor([10, 9, 2, 0, 14]), tensor([ 7, 13, 8, 11, 1])]","title":"SGD and Mini-batches"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#putting-it-together","text":"# re-initialise weights and params weights = init_params (( 28 * 28 , 1 )) bias = init_params ( 1 ) # create a data loader dl = DataLoader ( dset , batch_size = 256 ) # grab the first x and y xb , yb = first ( dl ) # check the shape xb . shape , yb . shape (torch.Size([256, 784]), torch.Size([256, 1])) # repeat for validation set valid_dl = DataLoader ( valid_dset , batch_size = 256 ) # grab a mini batch to test on batch = train_x [: 4 ] batch . shape torch.Size([4, 784]) # make some predictions preds = linear1 ( batch ) preds tensor([[-1.1306], [-3.9293], [-0.6736], [-6.9805]], grad_fn=<AddBackward0>) loss = mnist_loss ( preds , train_y [: 4 ]) loss tensor(0.8495, grad_fn=<MeanBackward0>) # calculate gradients loss . backward () weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0153), tensor([-0.1070])) # take those 3 steps and put it in a function def calc_grad ( xb , yb , model ): preds = model ( xb ) loss = mnist_loss ( preds , yb ) loss . backward () # test it calc_grad ( batch , train_y [: 4 ], linear1 ) weights . grad . shape , weights . grad . mean (), bias . grad (torch.Size([784, 1]), tensor(-0.0306), tensor([-0.2140])) # zero the gradients weights . grad . zero_ () bias . grad . zero_ () tensor([0.]) The last step is to work out how to update the weights and bias based on the gradient and learning rate. train_epoch loops through the data loader, grab x batch and y batch, calculate the gradient, make a prediction and calculate the loss. Go through each parameter (weights and bias) and for each update with gradient * lr, then zero these in prep for the next loop. p.data is used because PyTorch keeps track of all operations so it can calculate the gradients, but we do not want the gradients to be calculated on the gradient descent step. def train_epoch ( model , lr , params ): for xb , yb in dl : calc_grad ( xb , yb , model ) for p in params : p . data -= p . grad * lr p . grad . zero_ () batch_accuracy is similar to the previous loss function, but since we use a sigmoid, which constrains our preds between 0 and 1, we need to check whether preds > 0.5. def batch_accuracy ( xb , yb ): preds = xb . sigmoid () correct = ( preds > 0.5 ) == yb # check predictions against target return correct . float () . mean () batch_accuracy ( linear1 ( train_x [: 4 ]), train_y [: 4 ]) tensor(0.) # check accuracy for every batch in the validation set # stack converts the list of items into tensor def validate_epoch ( model ): accs = [ batch_accuracy ( model ( xb ), yb ) for xb , yb in valid_dl ] return round ( torch . stack ( accs ) . mean () . item (), 4 ) validate_epoch ( linear1 ) 0.407 This is a starting point, let's train for one epoch and see if accuracy improves. as a reminder, the linear1 function was... - def linear1(xb): return xb@weights + bias lr = 1. params = weights , bias train_epoch ( linear1 , lr , params ) validate_epoch ( linear1 ) 0.6932 for i in range ( 20 ): train_epoch ( linear1 , lr , params ) print ( validate_epoch ( linear1 ), end = ' ' ) 0.8242 0.9042 0.9355 0.9501 0.9555 0.9614 0.9638 0.9677 0.9736 0.9751 0.9751 0.976 0.977 0.9775 0.9775 0.978 0.9785 0.979 0.9795 0.979 Accuracy has indeed improved! We have built an SGD optimizer that has reached about 97% accuracy.","title":"Putting it together"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#refactor-and-clean-up","text":"create an optimiser use PyTorch modules and functions where available like nn.Linear which \"Applies a linear transformation to the incoming data: $y = xA^T + b$\" nn . Linear ? # remove our linear function # in place for torch module # creates a matrix of size 28*28 # with bias of 1 linear_model = nn . Linear ( 28 * 28 , 1 ) # check model params w , b = linear_model . parameters () w . shape , b . shape (torch.Size([1, 784]), torch.Size([1])) Create a basic optimiser pass in params to optimise and lr store these away step though each param (weights and bias) and for each, update with gradient * lr zero the gradients in prep for the next step class BasicOptim : def __init__ ( self , params , lr ): self . params , self . lr = list ( params ), lr def step ( self , * args , ** kwargs ): for p in self . params : p . data -= p . grad . data * self . lr def zero_grad ( self , * args , ** kwargs ): for p in self . params : p . grad = None # create an optimiser by passing in parameters from model opt = BasicOptim ( linear_model . parameters (), lr ) # simplify the training loop def train_epoch ( model ): for xb , yb in dl : calc_grad ( xb , yb , model ) opt . step () opt . zero_grad () validate_epoch ( linear_model ) 0.5075 Now create a function train_model that will call train_epoch on our model for the specified number of epochs def train_model ( model , epochs ): for i in range ( epochs ): train_epoch ( model ) print ( validate_epoch ( model ), end = ' ' ) train_model ( linear_model , 20 ) 0.4932 0.7876 0.852 0.916 0.9345 0.9497 0.957 0.9638 0.9658 0.9677 0.9697 0.9721 0.9731 0.9751 0.9755 0.9765 0.9775 0.9775 0.978 0.9785 The results are very similar to what we have seen before. Fastai provides SGD that we can use instead of writing our own, again the results are very similar. linear_model = nn . Linear ( 28 * 28 , 1 ) opt = SGD ( linear_model . parameters (), lr ) train_model ( linear_model , 20 ) 0.4932 0.9091 0.8056 0.9043 0.9316 0.9443 0.9546 0.9619 0.9648 0.9668 0.9692 0.9707 0.9731 0.9746 0.976 0.976 0.9775 0.9775 0.9785 0.979 Let's refactor some more, using some fastai classes. The Learner implements everything we have implemented manually. # Previously we used DataLoader not DataLoaders dls = DataLoaders ( dl , valid_dl ) learn = Learner ( dls , nn . Linear ( 28 * 28 , 1 ), opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 10 ) epoch train_loss valid_loss batch_accuracy time 0 0.480832 0.461122 0.843474 00:00 1 0.466804 0.441299 0.908734 00:00 2 0.450758 0.422136 0.934249 00:00 3 0.433590 0.403750 0.943572 00:00 4 0.416052 0.386223 0.949460 00:00 5 0.398675 0.369610 0.952404 00:00 6 0.381799 0.353943 0.956820 00:00 7 0.365630 0.339228 0.957311 00:00 8 0.350289 0.325455 0.959764 00:00 9 0.335835 0.312599 0.960255 00:00 The results again are very similar, but with some additional functionality (like printing out results in a pretty table!","title":"Refactor and clean up"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#non-linearity","text":"To create a simple neural net, using a linear function like we did before is not enough. We need to add in a non-linearity between two linear functions. This is the basic definition for a neural net.. The universal approximation theorem says, that given any arbitrarily complex continuous function, we can approximate it with a neural network. I found this useful for visualising how this works. This is what we are trying to do. In our basic_net , each line represents a layer in our network, the first and 3rd layers are known as linear layers the second, as a nonlinearity or an activation. res.max(tensor(0.0)) takes the result of our linear function and sets any negative value to 0.0 while maintaining any positive values. def basic_net ( xb ): res = xb @w1 + b1 res = res . max ( tensor ( 0.0 )) res = res @w2 + b2 return res plot_function ( F . relu ) Like we have seen previously.. - w1 and w2 are weight tensors - b1 and b2 are bias tensors we can initialise these the same as we have done previously.. w1 has 30 output activations, so in order for w2 to match it require 30 input activations. w1 = init_params (( 28 * 28 , 30 )) b1 = init_params ( 30 ) w2 = init_params (( 30 , 1 )) b2 = init_params ( 1 ) We can simplify further using PyTorch... What we did in basic_net was called function composition, where we passed the results of one function into another function and then into another function. This is what neural nets are doing with linear layers and activation functions. nn.Sequential() will do this for us... simple_net = nn . Sequential ( nn . Linear ( 28 * 28 , 30 ), # 28*28 in, 30 out nn . ReLU (), nn . Linear ( 30 , 1 ) # 30 in 1 out ) learn = Learner ( dls , simple_net , opt_func = SGD , loss_func = mnist_loss , metrics = batch_accuracy ) learn . fit ( 40 , 0.1 ) epoch train_loss valid_loss batch_accuracy time 0 0.301185 0.414520 0.506379 00:00 1 0.142869 0.223012 0.814033 00:00 2 0.079959 0.114103 0.916094 00:00 3 0.053115 0.077652 0.939156 00:00 4 0.040578 0.060868 0.953876 00:00 5 0.034118 0.051373 0.963690 00:00 6 0.030368 0.045362 0.965653 00:00 7 0.027905 0.041246 0.965162 00:00 8 0.026117 0.038246 0.968597 00:00 9 0.024726 0.035950 0.969087 00:00 10 0.023596 0.034124 0.971050 00:00 11 0.022651 0.032629 0.972031 00:00 12 0.021847 0.031376 0.973503 00:00 13 0.021151 0.030301 0.974485 00:00 14 0.020542 0.029363 0.974485 00:00 15 0.020002 0.028535 0.975957 00:00 16 0.019519 0.027797 0.976448 00:00 17 0.019083 0.027134 0.976938 00:00 18 0.018687 0.026535 0.977920 00:00 19 0.018325 0.025991 0.978901 00:00 20 0.017992 0.025495 0.978901 00:00 21 0.017684 0.025040 0.978901 00:00 22 0.017398 0.024621 0.979392 00:00 23 0.017131 0.024233 0.979392 00:00 24 0.016881 0.023874 0.980373 00:00 25 0.016645 0.023541 0.980373 00:00 26 0.016424 0.023232 0.980373 00:00 27 0.016214 0.022943 0.980864 00:00 28 0.016015 0.022673 0.980864 00:00 29 0.015827 0.022421 0.981354 00:00 30 0.015648 0.022185 0.981845 00:00 31 0.015476 0.021963 0.982336 00:00 32 0.015313 0.021756 0.982336 00:00 33 0.015156 0.021561 0.982336 00:00 34 0.015006 0.021378 0.982826 00:00 35 0.014863 0.021204 0.982826 00:00 36 0.014725 0.021041 0.982336 00:00 37 0.014592 0.020886 0.982336 00:00 38 0.014464 0.020740 0.982336 00:00 39 0.014341 0.020601 0.982336 00:00 # this is what our model now looks like learn . model Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) # plot the loss learn . recorder . plot_loss () # learn.recorder.values hold the table values above # lets plot the accuracy plt . plot ( L ( learn . recorder . values ) . itemgot ( 2 ));","title":"Non-Linearity"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#looking-inside","text":"# let's visualise some of the parameters # 1. grab your model m = learn . model # (0): Linear(in_features=784, out_features=30, bias=True) # 2. look inside and grab the weights and biases w , b = m [ 0 ] . parameters () # 3. grab first (or any) row, reshape, and plot show_image ( w [ 0 ] . view ( 28 , 28 ), figsize = ( 4 , 4 )) <AxesSubplot:>","title":"Looking inside..."},{"location":"fastai%20deep%20learning%202020/lesson%2004/#fastai-in-full","text":"from fastai.vision.all import * from pathlib import Path path = Path . cwd () / 'datasets/fastai/mnist_sample' dls = ImageDataLoaders . from_folder ( path ) learn = cnn_learner ( dls , resnet18 , pretrained = False , loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 1 , 0.1 ) epoch train_loss valid_loss accuracy time 0 0.086805 0.025215 0.994603 00:11","title":"fastai in full"},{"location":"fastai%20deep%20learning%202020/lesson%2004/#summary","text":"We have gone over creating and training a neural network from scratch using the simple example of a digit classifier. The key idea for the last few notebooks was to start with planning out the problem and identifying a way to solve it using a simple common sense solution - the pixel similarity model. This proved successful but it was not really robust beyond the straightforward example we chose - identifying 3s and 7s. We then implemented a more complex solution that could be applied to more complicated problems. After each step or concept had been implemented manually, we refactored the code to use convenient PyTorch functions and modules, eventually ending up with using fastai's implementation which abstracts away from all of the underlying heavy lifting. This is done for convenience and in my own opinion, to help lower the entry barrier into deep learning. Ultimately I believe it is fundamentally important to understand the concepts and implementation if your goal (and this is my goal) is to implement deep learning solutions to solve business problems within your industry.","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 6: Multi-Label Classification 06-10-2020 This notebook will go over some of the practical material discussed in lesson 6 of the fastai 2020 course. Lesson 5 was an extension on the pet classifier we built as well as a discussion on data ethics. # imports from fastai.vision.all import * # dataset path = untar_data ( URLs . PASCAL_2007 ) Dataset: PASCAL Our data set contains images along with a csv containing the labels. There are multiple labels per image, these are space sparated. df = pd . read_csv ( path / 'train.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fname labels is_valid 0 000005.jpg chair True 1 000007.jpg car True 2 000009.jpg horse person True 3 000012.jpg car False 4 000016.jpg bicycle True Let's create a dataset from scratch using fastais suggested methods. We need to grab the appropriate fields from the data frame, that is... The Independent variable will be the images The Label will be extracted from space separated strings # convenience for setting the base path to the path Path . BASE_PATH = path DataBlock We need to tell our datablock where to get the data from where to get the labels from what kind of data we are working with this is an image classification problem so images and labels MultiCategoryBlock expects a list of category labels how to split our data # helper functions # create functions to grab x from training path # create a function to split lables for y def get_x ( r ): return path / 'train' / r [ 'fname' ] def get_y ( r ): return r [ 'labels' ] . split ( ' ' ) # training/valid splitter def splitter ( df ): train = df . index [ ~ df [ 'is_valid' ]] . tolist () valid = df . index [ df [ 'is_valid' ]] . tolist () return train , valid dblock = DataBlock ( blocks = ( ImageBlock , MultiCategoryBlock ), splitter = splitter , get_x = get_x , get_y = get_y , item_tfms = RandomResizedCrop ( 128 , min_scale = 0.35 )) dsets = dblock . datasets ( df ) dsets . train [ 0 ] (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) dsets . train [ 0 ][ 0 ] . to_thumb ( 192 ) # for our image, check where the vocab == 1 # filter vocab by this to check that our car is a car idx = torch . where ( dsets . train [ 0 ][ 1 ] == 1. )[ 0 ] dsets . train . vocab [ idx ] (#1) ['car'] TensorMultiCategory is a one-hot encoded vector, this means that instead of having a label or list of labels, for each image we have a tensor that will have a 1 for the labels for that image, and a 0 for all other labels. The vocab is useful to see which label classes are avaiable. dsets . train . vocab . o2i {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19} putting it all together we now have a complete data block so will swap out dsets = dblock.datasets(df) for dsets = dblock.dataloaders(df) dblock = DataBlock ( blocks = ( ImageBlock , MultiCategoryBlock ), splitter = splitter , get_x = get_x , get_y = get_y , item_tfms = RandomResizedCrop ( 128 , min_scale = 0.35 )) dls = dblock . dataloaders ( df ) dls . show_batch ( nrows = 1 , ncols = 3 ) Binary cross entropy loss Binary Cross entropy loss is used for classificaiton problems. For multi-label classification, we do not need the sum of different classes to add up to one so we will not need softmax here. WHy? Because we might see multiple objects that we are confident appear in an image, so using softmax to restrict this is not a good idea for this problem. We may also want the sum to be less than one if the model is not confident that any of the categories appear in the image. Each activation will be compared to each target for each column, so we don't have to do anything to make this function work for multiple columns. Some explainations I found useful... here and here # Create a CNN learner learn = cnn_learner ( dls , resnet18 ) # we can check the activations of our model # by passing in a mini-batch of the independent variable x , y = to_cpu ( dls . train . one_batch ()) activs = learn . model ( x ) activs . shape torch.Size([64, 20]) Why this shape? batch size = 64 number of categories = 20 # check the 20 activations # this is just to see what they look like activs [ 0 ] tensor([-1.0697, 2.5707, -0.2860, -2.3535, 3.0095, 4.3694, 1.0534, 0.7723, -3.6078, 3.1691, -2.0013, 0.9257, 2.9621, -1.3111, 0.7584, 0.0951, 1.2465, 0.9465, -0.1643, 0.6763], grad_fn=<SelectBackward>) These activations are not between 0 and 1. We need them to represent probabilities so will need to run them through a sigmoid. Remember, here, we do not need the sum of them to add up to one. We will use F.binary_cross_entropy_with_logits because this contains a sigmoid. loss_func = nn . BCEWithLogitsLoss () loss = loss_func ( activs , y ) loss tensor(1.0935, grad_fn=<BinaryCrossEntropyWithLogitsBackward>) Defining a metric accuracy will only work for single label classification problems. The reason is because it takes the input (final layer activatons) and performs argmax on these. Argmax will return the largest value from the inputs. Then it compares this to the target and takes the mean. So it will only make sense when there is a single maximum we are looking for. accuracy_multi will be used instead We need something that works for multiple labels. To do this we will compare the final layer activations to a threshold (0.5 by default). Then we say, if the sigmoid is grater than the threshold, assume that category is there, else if it is less, then it is not there. We then compare this list of trues and falses to the target, then take the mean. We might not want to use 0.5 for our threshold. We can do this using partials when we create our learner by passing in the required argument thresh=0.2 . Fastai by default will know we are doing a multilabel classification problem so don't need to specify the loss. learn = cnn_learner ( dls , resnet50 , metrics = partial ( accuracy_multi , thresh = 0.2 )) learn . fine_tune ( 3 , base_lr = 3e-3 , freeze_epocs = 4 ) epoch train_loss valid_loss accuracy_multi time 0 0.855198 0.581906 0.323546 00:21 epoch train_loss valid_loss accuracy_multi time 0 0.562123 0.351448 0.488068 00:24 1 0.379880 0.169467 0.896534 00:23 2 0.268020 0.150422 0.924243 00:23 How do we pick a good threshold? By trial and error! preds , targs = learn . get_preds () xs = torch . linspace ( 0.05 , 0.95 , 29 ) accs = [ accuracy_multi ( preds , targs , thresh = i , sigmoid = False ) for i in xs ] plt . plot ( xs , accs ); # somewhere just above 0.5 Image Regression path = untar_data ( URLs . BIWI_HEAD_POSE ) Inspect the data, there are 24 directories that correspond to 24 different people photographed. Path . BASE_PATH = path path . ls () . sorted () (#50) [Path('01'),Path('01.obj'),Path('02'),Path('02.obj'),Path('03'),Path('03.obj'),Path('04'),Path('04.obj'),Path('05'),Path('05.obj')...] # take a look inside one directory ( path / '01' ) . ls () . sorted () (#1000) [Path('01/depth.cal'),Path('01/frame_00003_pose.txt'),Path('01/frame_00003_rgb.jpg'),Path('01/frame_00004_pose.txt'),Path('01/frame_00004_rgb.jpg'),Path('01/frame_00005_pose.txt'),Path('01/frame_00005_rgb.jpg'),Path('01/frame_00006_pose.txt'),Path('01/frame_00006_rgb.jpg'),Path('01/frame_00007_pose.txt')...] each directory contains image files and a pose file wich shows the location of the centre of the head. We can use a function that will return the cooordinates of the head centre point. get_image_files will recursively get all image files img2pose will convert an image filename to its associated pose file img_files = get_image_files ( path ) def img2pose ( x ): return Path ( f ' { str ( x )[: - 7 ] } pose.txt' ) img2pose ( img_files [ 0 ]) Path('16/frame_00182_pose.txt') # take alook at the shape of an image and a sample image im = PILImage . create ( img_files [ 0 ]) im . shape (480, 640) im . to_thumb ( 160 ) # this function is supplied by BIWI dataset website # returns the coordinates as a tensor cal = np . genfromtxt ( path / '01' / 'rgb.cal' , skip_footer = 6 ) def get_ctr ( f ): ctr = np . genfromtxt ( img2pose ( f ), skip_header = 3 ) c1 = ctr [ 0 ] * cal [ 0 ][ 0 ] / ctr [ 2 ] + cal [ 0 ][ 2 ] c2 = ctr [ 1 ] * cal [ 1 ][ 1 ] / ctr [ 2 ] + cal [ 1 ][ 2 ] return tensor ([ c1 , c2 ]) # test it get_ctr ( img_files [ 0 ]) tensor([324.0023, 251.5637]) create a DataBlock we will not use a random splitter because there are multiple images of each person in the data set. We want the model to generalise well on people it has not yet seen. So we will hold back one person for validation. Our data block will be an ImageBlock with two continuous values (the coordinate files). PointBlock specifies this for us. get_ctr will return our y values. we will also half the size of our images with aug_transforms(size=(240,320)) biwi = DataBlock ( blocks = ( ImageBlock , PointBlock ), get_items = get_image_files , get_y = get_ctr , splitter = FuncSplitter ( lambda o : o . parent . name == '13' ), batch_tfms = [ * aug_transforms ( size = ( 240 , 320 )), Normalize . from_stats ( * imagenet_stats )] ) # check some data dls = biwi . dataloaders ( path ) dls . show_batch ( max_n = 9 , figsize = ( 8 , 6 )) # check the shape of one batch xb , yb = dls . one_batch () xb . shape , yb . shape (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) Understanding torch.Size([64, 3, 240, 320]) mini batch is 64 items there are 3 channels R,G,B image size is 240x320 torch.Size([64, 1, 2]) 64 items in mini batch each item is one point, represented by 2 coordinates (1,2) Training the model create a learner in the standard way with cnn_learner . We use y_range=(-1,1) to tell fastai what range of data we expect to see in the dependent variable. y_range uses a sigmoid function mapped to the low and high values you supplied. learn = cnn_learner ( dls , resnet18 , y_range = ( - 1 , 1 )) # we didn't specify the loss # what has fastai picked? dls . loss_func FlattenedLoss of MSELoss() MSE will be suitable for this problem since we are trying to predict something as close as possible to the given coordinates. lr_min , lr_steep = learn . lr_find () learn . recorder . plot_lr_find () plt . axvline ( x = lr_min , color = 'orange' ) plt . axvline ( x = lr_steep , color = 'r' ); lr = lr_min learn . fine_tune ( 3 , lr ) epoch train_loss valid_loss time 0 0.059350 0.002665 01:51 epoch train_loss valid_loss time 0 0.005905 0.002256 02:29 1 0.003000 0.000682 02:29 2 0.001558 0.000083 02:29 We got a loss of 0.000083 This corresponds to an average coordinate prediction error of... math . sqrt ( 0.000083 ) 0.0091104335791443 # check results against actuals learn . show_results ( ds_idx = 1 , nrows = 3 , figsize = ( 6 , 8 )) Summary For the image regression problem, we were able to use fine_tune rather than train from scratch because our pre-trained model has enough information about faces that retro-fitting it to a different problem is somewhat trivial for it. This is a pretty powerful idea, these algoriths are advanced enough so that there is a certain amount of flexibility in them that can be trained further and utilised on other problems.","title":"Lesson 06 pt 1"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#lesson-6-multi-label-classification","text":"06-10-2020 This notebook will go over some of the practical material discussed in lesson 6 of the fastai 2020 course. Lesson 5 was an extension on the pet classifier we built as well as a discussion on data ethics. # imports from fastai.vision.all import * # dataset path = untar_data ( URLs . PASCAL_2007 )","title":"Lesson 6: Multi-Label Classification"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#dataset-pascal","text":"Our data set contains images along with a csv containing the labels. There are multiple labels per image, these are space sparated. df = pd . read_csv ( path / 'train.csv' ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } fname labels is_valid 0 000005.jpg chair True 1 000007.jpg car True 2 000009.jpg horse person True 3 000012.jpg car False 4 000016.jpg bicycle True Let's create a dataset from scratch using fastais suggested methods. We need to grab the appropriate fields from the data frame, that is... The Independent variable will be the images The Label will be extracted from space separated strings # convenience for setting the base path to the path Path . BASE_PATH = path","title":"Dataset: PASCAL"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#datablock","text":"We need to tell our datablock where to get the data from where to get the labels from what kind of data we are working with this is an image classification problem so images and labels MultiCategoryBlock expects a list of category labels how to split our data # helper functions # create functions to grab x from training path # create a function to split lables for y def get_x ( r ): return path / 'train' / r [ 'fname' ] def get_y ( r ): return r [ 'labels' ] . split ( ' ' ) # training/valid splitter def splitter ( df ): train = df . index [ ~ df [ 'is_valid' ]] . tolist () valid = df . index [ df [ 'is_valid' ]] . tolist () return train , valid dblock = DataBlock ( blocks = ( ImageBlock , MultiCategoryBlock ), splitter = splitter , get_x = get_x , get_y = get_y , item_tfms = RandomResizedCrop ( 128 , min_scale = 0.35 )) dsets = dblock . datasets ( df ) dsets . train [ 0 ] (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) dsets . train [ 0 ][ 0 ] . to_thumb ( 192 ) # for our image, check where the vocab == 1 # filter vocab by this to check that our car is a car idx = torch . where ( dsets . train [ 0 ][ 1 ] == 1. )[ 0 ] dsets . train . vocab [ idx ] (#1) ['car'] TensorMultiCategory is a one-hot encoded vector, this means that instead of having a label or list of labels, for each image we have a tensor that will have a 1 for the labels for that image, and a 0 for all other labels. The vocab is useful to see which label classes are avaiable. dsets . train . vocab . o2i {'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3, 'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8, 'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12, 'motorbike': 13, 'person': 14, 'pottedplant': 15, 'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19}","title":"DataBlock"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#putting-it-all-together","text":"we now have a complete data block so will swap out dsets = dblock.datasets(df) for dsets = dblock.dataloaders(df) dblock = DataBlock ( blocks = ( ImageBlock , MultiCategoryBlock ), splitter = splitter , get_x = get_x , get_y = get_y , item_tfms = RandomResizedCrop ( 128 , min_scale = 0.35 )) dls = dblock . dataloaders ( df ) dls . show_batch ( nrows = 1 , ncols = 3 )","title":"putting it all together"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#binary-cross-entropy-loss","text":"Binary Cross entropy loss is used for classificaiton problems. For multi-label classification, we do not need the sum of different classes to add up to one so we will not need softmax here. WHy? Because we might see multiple objects that we are confident appear in an image, so using softmax to restrict this is not a good idea for this problem. We may also want the sum to be less than one if the model is not confident that any of the categories appear in the image. Each activation will be compared to each target for each column, so we don't have to do anything to make this function work for multiple columns. Some explainations I found useful... here and here # Create a CNN learner learn = cnn_learner ( dls , resnet18 ) # we can check the activations of our model # by passing in a mini-batch of the independent variable x , y = to_cpu ( dls . train . one_batch ()) activs = learn . model ( x ) activs . shape torch.Size([64, 20]) Why this shape? batch size = 64 number of categories = 20 # check the 20 activations # this is just to see what they look like activs [ 0 ] tensor([-1.0697, 2.5707, -0.2860, -2.3535, 3.0095, 4.3694, 1.0534, 0.7723, -3.6078, 3.1691, -2.0013, 0.9257, 2.9621, -1.3111, 0.7584, 0.0951, 1.2465, 0.9465, -0.1643, 0.6763], grad_fn=<SelectBackward>) These activations are not between 0 and 1. We need them to represent probabilities so will need to run them through a sigmoid. Remember, here, we do not need the sum of them to add up to one. We will use F.binary_cross_entropy_with_logits because this contains a sigmoid. loss_func = nn . BCEWithLogitsLoss () loss = loss_func ( activs , y ) loss tensor(1.0935, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)","title":"Binary cross entropy loss"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#defining-a-metric","text":"accuracy will only work for single label classification problems. The reason is because it takes the input (final layer activatons) and performs argmax on these. Argmax will return the largest value from the inputs. Then it compares this to the target and takes the mean. So it will only make sense when there is a single maximum we are looking for. accuracy_multi will be used instead We need something that works for multiple labels. To do this we will compare the final layer activations to a threshold (0.5 by default). Then we say, if the sigmoid is grater than the threshold, assume that category is there, else if it is less, then it is not there. We then compare this list of trues and falses to the target, then take the mean. We might not want to use 0.5 for our threshold. We can do this using partials when we create our learner by passing in the required argument thresh=0.2 . Fastai by default will know we are doing a multilabel classification problem so don't need to specify the loss. learn = cnn_learner ( dls , resnet50 , metrics = partial ( accuracy_multi , thresh = 0.2 )) learn . fine_tune ( 3 , base_lr = 3e-3 , freeze_epocs = 4 ) epoch train_loss valid_loss accuracy_multi time 0 0.855198 0.581906 0.323546 00:21 epoch train_loss valid_loss accuracy_multi time 0 0.562123 0.351448 0.488068 00:24 1 0.379880 0.169467 0.896534 00:23 2 0.268020 0.150422 0.924243 00:23 How do we pick a good threshold? By trial and error! preds , targs = learn . get_preds () xs = torch . linspace ( 0.05 , 0.95 , 29 ) accs = [ accuracy_multi ( preds , targs , thresh = i , sigmoid = False ) for i in xs ] plt . plot ( xs , accs ); # somewhere just above 0.5","title":"Defining a metric"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#image-regression","text":"path = untar_data ( URLs . BIWI_HEAD_POSE ) Inspect the data, there are 24 directories that correspond to 24 different people photographed. Path . BASE_PATH = path path . ls () . sorted () (#50) [Path('01'),Path('01.obj'),Path('02'),Path('02.obj'),Path('03'),Path('03.obj'),Path('04'),Path('04.obj'),Path('05'),Path('05.obj')...] # take a look inside one directory ( path / '01' ) . ls () . sorted () (#1000) [Path('01/depth.cal'),Path('01/frame_00003_pose.txt'),Path('01/frame_00003_rgb.jpg'),Path('01/frame_00004_pose.txt'),Path('01/frame_00004_rgb.jpg'),Path('01/frame_00005_pose.txt'),Path('01/frame_00005_rgb.jpg'),Path('01/frame_00006_pose.txt'),Path('01/frame_00006_rgb.jpg'),Path('01/frame_00007_pose.txt')...] each directory contains image files and a pose file wich shows the location of the centre of the head. We can use a function that will return the cooordinates of the head centre point. get_image_files will recursively get all image files img2pose will convert an image filename to its associated pose file img_files = get_image_files ( path ) def img2pose ( x ): return Path ( f ' { str ( x )[: - 7 ] } pose.txt' ) img2pose ( img_files [ 0 ]) Path('16/frame_00182_pose.txt') # take alook at the shape of an image and a sample image im = PILImage . create ( img_files [ 0 ]) im . shape (480, 640) im . to_thumb ( 160 ) # this function is supplied by BIWI dataset website # returns the coordinates as a tensor cal = np . genfromtxt ( path / '01' / 'rgb.cal' , skip_footer = 6 ) def get_ctr ( f ): ctr = np . genfromtxt ( img2pose ( f ), skip_header = 3 ) c1 = ctr [ 0 ] * cal [ 0 ][ 0 ] / ctr [ 2 ] + cal [ 0 ][ 2 ] c2 = ctr [ 1 ] * cal [ 1 ][ 1 ] / ctr [ 2 ] + cal [ 1 ][ 2 ] return tensor ([ c1 , c2 ]) # test it get_ctr ( img_files [ 0 ]) tensor([324.0023, 251.5637])","title":"Image Regression"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#create-a-datablock","text":"we will not use a random splitter because there are multiple images of each person in the data set. We want the model to generalise well on people it has not yet seen. So we will hold back one person for validation. Our data block will be an ImageBlock with two continuous values (the coordinate files). PointBlock specifies this for us. get_ctr will return our y values. we will also half the size of our images with aug_transforms(size=(240,320)) biwi = DataBlock ( blocks = ( ImageBlock , PointBlock ), get_items = get_image_files , get_y = get_ctr , splitter = FuncSplitter ( lambda o : o . parent . name == '13' ), batch_tfms = [ * aug_transforms ( size = ( 240 , 320 )), Normalize . from_stats ( * imagenet_stats )] ) # check some data dls = biwi . dataloaders ( path ) dls . show_batch ( max_n = 9 , figsize = ( 8 , 6 )) # check the shape of one batch xb , yb = dls . one_batch () xb . shape , yb . shape (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) Understanding torch.Size([64, 3, 240, 320]) mini batch is 64 items there are 3 channels R,G,B image size is 240x320 torch.Size([64, 1, 2]) 64 items in mini batch each item is one point, represented by 2 coordinates (1,2)","title":"create a DataBlock"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#training-the-model","text":"create a learner in the standard way with cnn_learner . We use y_range=(-1,1) to tell fastai what range of data we expect to see in the dependent variable. y_range uses a sigmoid function mapped to the low and high values you supplied. learn = cnn_learner ( dls , resnet18 , y_range = ( - 1 , 1 )) # we didn't specify the loss # what has fastai picked? dls . loss_func FlattenedLoss of MSELoss() MSE will be suitable for this problem since we are trying to predict something as close as possible to the given coordinates. lr_min , lr_steep = learn . lr_find () learn . recorder . plot_lr_find () plt . axvline ( x = lr_min , color = 'orange' ) plt . axvline ( x = lr_steep , color = 'r' ); lr = lr_min learn . fine_tune ( 3 , lr ) epoch train_loss valid_loss time 0 0.059350 0.002665 01:51 epoch train_loss valid_loss time 0 0.005905 0.002256 02:29 1 0.003000 0.000682 02:29 2 0.001558 0.000083 02:29 We got a loss of 0.000083 This corresponds to an average coordinate prediction error of... math . sqrt ( 0.000083 ) 0.0091104335791443 # check results against actuals learn . show_results ( ds_idx = 1 , nrows = 3 , figsize = ( 6 , 8 ))","title":"Training the model"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%201/#summary","text":"For the image regression problem, we were able to use fine_tune rather than train from scratch because our pre-trained model has enough information about faces that retro-fitting it to a different problem is somewhat trivial for it. This is a pretty powerful idea, these algoriths are advanced enough so that there is a certain amount of flexibility in them that can be trained further and utilised on other problems.","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 6: Collaborative Filtering This notebook will cover collaborative filtering using the MovieLens data set. from fastai.collab import * from fastai.tabular.all import * path = untar_data ( URLs . ML_100k ) Data \"MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota. This data set consists of: 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies. Simple demographic info for the users (age, gender, occupation, zip)\" Additional Info u.data contains the full data set, 100,000 ratings by 943 users on 1,682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp The time stamps are unix seconds since 1/1/1970 UTC source ratings = pd . read_csv ( path / 'u.data' , delimiter = ' \\t ' , header = None , names = [ 'user' , 'movie' , 'rating' , 'timestamp' ]) ratings . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user movie rating timestamp 0 196 242 3 881250949 1 186 302 3 891717742 2 22 377 1 878887116 3 244 51 2 880606923 4 166 346 1 886397596 The goal is to predict or guess what films users might like to watch. You could easily imagine that a user might have a preference for certain genres, and based on films they have seen from a particular genre, you might be able to say something like, user 123 likes action movies, therefore it would be safe to suggest an action movie to them. Given that we have minimal information in our data set (userid, movieid, rating and timestamp), collaborative filter seeks to solve this problem by extracting latent features from the data. For example, assume that these features range between -1 and +1, with postive numbers indicating stronger mathes to certain factors. We can use a simple example to illustrate the point. Take the following three dummy factors science-fiction , action , and old movies , we can compare user preferences against these for two different movies and see how they score. import numpy as np last_skywalker = np . array ([ 0.98 , 0.9 , - 0.9 ]) casablanca = np . array ([ - 0.99 , - 0.3 , 0.8 ]) user1 = np . array ([ 0.9 , 0.8 , - 0.6 ]) We can compute the dot product and arrive at a match m1 = ( user1 * last_skywalker ) . sum () m2 = ( user1 * casablanca ) . sum () print ( f 'last skywalker match: { m1 . round ( 2 ) } \\n casablanca match: { m2 . round ( 2 ) } ' ) last skywalker match: 2.14 casablanca match: -1.61 Voila! based on this we might want to recommend Last Skywalker but not Casablanca to this user. So how do we find these latent factors? They can be learned. Step 1: Randomly Initialize Parameters Randomly assign parameters to represent our latent factors for each user and each movie. We get to decide how many of these factors we want to use. Step 2: Calculate Predictions Calculate predictions. This is done as we have just seen, by computing the dot product. Step 3: Improve Predictions Then improve the prediction using gradient descent on these latent factors First, let's add the movie titles to our data set for readability movies = pd . read_csv ( path / 'u.item' , delimiter = '|' , encoding = 'latin-1' , usecols = ( 0 , 1 ), names = ( 'movie' , 'title' ), header = None ) movies . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } movie title 0 1 Toy Story (1995) 1 2 GoldenEye (1995) 2 3 Four Rooms (1995) 3 4 Get Shorty (1995) 4 5 Copycat (1995) # join on movie titles ratings = ratings . merge ( movies ) ratings . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user movie rating timestamp title 0 196 242 3 881250949 Kolya (1996) 1 63 242 3 875747190 Kolya (1996) 2 226 242 5 883888671 Kolya (1996) 3 154 242 3 879138235 Kolya (1996) 4 306 242 5 876503793 Kolya (1996) Create a DataLoader dls = CollabDataLoaders . from_df ( ratings , item_name = 'title' , bs = 64 ) dls . show_batch () user title rating 0 494 Shawshank Redemption, The (1994) 5 1 806 Wrong Trousers, The (1993) 5 2 91 Glory (1989) 5 3 497 Lawnmower Man 2: Beyond Cyberspace (1996) 2 4 630 Rainmaker, The (1997) 3 5 89 That Thing You Do! (1996) 2 6 442 Brothers McMullen, The (1995) 3 7 37 Braveheart (1995) 5 8 159 Kansas City (1996) 1 9 585 Cinema Paradiso (1988) 5 dls . classes {'user': (#944) ['#na#',1,2,3,4,5,6,7,8,9...], 'title': (#1665) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...]} Randomly initialize parameters create user_factors and movie_factors of size n users x n factors and n movies by n factors n_users = len ( dls . classes [ 'user' ]) n_movies = len ( dls . classes [ 'title' ]) n_factors = 5 user_factors = torch . randn ( n_users , n_factors ) movie_factors = torch . randn ( n_movies , n_factors ) user_factors . size () torch.Size([944, 5]) One Hot Encoding We need to prepare our data in a specific way before we can pass it to our model. Some algorithms can work directly with category labels, but many cannot. One hot encoding is a way of representing categorical data by transforming categorical labels into vectors of 0s and 1s. To get a sense of how one-hot-encoding is operating, here is a simple example... df = pd . DataFrame ([ 'adam' , 'beatrix' , 'cam' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 0 adam 1 beatrix 2 cam # the categorical variables are represented by 1s and 0s pd . get_dummies ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0_adam 0_beatrix 0_cam 0 1 0 0 1 0 1 0 2 0 0 1 In order to calculate a result for a particular user and movie combination, we weill need to look up the index of the movie, and the index of a user in the respective latent factor matrices, then perform a dot product. But this is not something that our model is capable of doing. It is capable of performing dot products though.. Here is another example of how we can use dot products to return elements from a matrix... say the rows in matrix w represent latent movie factors and the matrix v is our one hot encoded movie ids w = torch . randn (( 3 , 3 )) w tensor([[-1.7224, -0.4789, 0.3553], [-1.3465, -0.3057, 0.6882], [ 0.4594, 0.7893, 0.0150]]) v = torch . tensor ([[ 1 , 0 , 0 ],[ 0 , 1 , 0 ],[ 0 , 0 , 1 ]]) v tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) we can retrieve the factors for movie 2 (row 2 of w ) by preforming a matrix product on the corresponding row of v v [ 1 ] . float () @ w tensor([-1.3465, -0.3057, 0.6882]) this is the same as calling.. w [ 1 ] tensor([-1.3465, -0.3057, 0.6882]) One hot encoding is basically performing an index lookup on our data. This is however memory intensive since we now have these huge matrices to deal with and most of the values will be 0. Enter embeddings . Embeddings are a computational shortcut for doing matrix multiplication of one-hot-encoded vectors I found this link useful Collaborative filtering from Scratch forward is a very important method name in PyTorch. forward will be the method that handles computation. DotProduct The forward method here will get passed users and movies in two column ( x ). We then grab the factors from an embedding by calling user_factors just like a function. Assign these to users and movies then perform the dot product using (users * movies).sum(dim=1) . dim=1 is used because we want to sum over the second index class DotProduct ( Module ): def __init__ ( self , n_users , n_movies , n_factors ): self . user_factors = Embedding ( n_users , n_factors ) self . movie_factors = Embedding ( n_movies , n_factors ) def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) return ( users * movies ) . sum ( dim = 1 ) class DotProduct ( Module ): def __init__ ( self , n_users , n_movies , n_factors ): self . user_factors = Embedding ( n_users , n_factors ) self . movie_factors = Embedding ( n_movies , n_factors ) def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) return ( users * movies ) . sum ( dim = 1 ) x , y = dls . one_batch () # check the shape of x x . shape torch.Size([64, 2]) # check the first 5 things in x x [: 5 ] # user ids and movie ids tensor([[ 396, 1021], [ 118, 887], [ 206, 380], [ 207, 938], [ 923, 861]], device='cuda:0') What does torch.Size([64, 2]) tell us? - batch size is 64 - then we have 2 items, the user ids and movie ids - check with x[:,0] and x[:,1] # check the first 5 things of 7 y [: 5 ] # these are the ratings tensor([[4], [5], [1], [3], [4]], device='cuda:0', dtype=torch.int8) Create a learner our model will be the DotProduct class with 50 latent factors loss function will be MSE this is a regression problem for continuous variables model = DotProduct ( n_users , n_movies , n_factors = 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 ) epoch train_loss valid_loss time 0 1.362978 1.286954 00:08 1 1.051144 1.094442 00:08 2 0.963542 0.975438 00:08 3 0.841074 0.893598 00:08 4 0.788252 0.873474 00:08 Not bad but we can make some improvements. We can use a sigmoid to contrain our ratings between 0 and 5, matching what we see in our original data set. We will actually use 0-5.5 otherwise the sigmoid will prevent us ever getting a 5. # check rating scores/categories ratings . rating . value_counts () 4 34174 3 27145 5 21201 2 11370 1 6110 Name: rating, dtype: int64 help ( sigmoid_range ) Help on function sigmoid_range in module fastai.layers: sigmoid_range(x, low, high) Sigmoid function with range `(low, high)` class DotProduct ( Module ): def __init__ ( self , n_users , n_movies , n_factors , y_range = ( 0 , 5.5 )): self . user_factors = Embedding ( n_users , n_factors ) self . movie_factors = Embedding ( n_movies , n_factors ) self . y_range = y_range def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) return sigmoid_range (( users * movies ) . sum ( dim = 1 ), * self . y_range ) model = DotProduct ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 ) epoch train_loss valid_loss time 0 1.024472 0.989683 00:08 1 0.882552 0.901929 00:08 2 0.684762 0.860367 00:08 3 0.479302 0.864543 00:08 4 0.347512 0.870277 00:08 Not much better.... Adding in a Bias term We can make further improvements by adding a bias term to our model. Why would we do this? Some movies may have a high rating because they are genuinely better movies, and some users may skew towards being more positive and therefore their rating could generally be more positive. The idea of the bias term is that we now have a way to represent this missing piece of information. # Add in a bias term for each user and each movie. class DotProductBias ( Module ): def __init__ ( self , n_users , n_movies , n_factors , y_range = ( 0 , 5.5 )): self . user_factors = Embedding ( n_users , n_factors ) self . user_bias = Embedding ( n_users , 1 ) self . movie_factors = Embedding ( n_movies , n_factors ) self . movie_bias = Embedding ( n_movies , 1 ) self . y_range = y_range def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) res = ( users * movies ) . sum ( dim = 1 , keepdim = True ) res += self . user_bias ( x [:, 0 ]) + self . movie_bias ( x [:, 1 ]) return sigmoid_range ( res , * self . y_range ) model = DotProduct ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 ) epoch train_loss valid_loss time 0 0.969996 0.988517 00:08 1 0.900296 0.907242 00:08 2 0.691395 0.859838 00:08 3 0.491507 0.861104 00:08 4 0.365541 0.864733 00:08 Our final result is slightly better but we are actually overfitting! How can we stop this and train for longer? Regularisation Regularisation is a set of techniques that help to reduce the capacity of the model. Regularisations helps to prevent overfitting of models. Rather than reduce the parameters, we can try to force the parameters to be smaller, unless they are required to be big. Weight Decay Also known as L2 regularisation. It consists of adding to the loss function the sum of all the parameters squared. Why does this work and why would this prevent overfitting? - one way to decrease loss is to decrease the weights - limiting the weights is going to hinder the training (won't fit training set as well) but will help the model to generalise better In practice what we are doing is adding onto the gradients, the weights multiplied by some hyper parameter. model = DotProductBias ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 , wd = 0.1 ) epoch train_loss valid_loss time 0 0.953578 0.935531 00:08 1 0.846484 0.874122 00:07 2 0.718206 0.829690 00:07 3 0.593549 0.817133 00:07 4 0.474108 0.818135 00:08 much better! Creating our own Embedding module Let's recreate the DotProductBias without using the Embedding class. to recap: - an embedding layer is a computational shortcut for performing a matrix multiplication by a one hot encoded matrix, which is the same as indexing into an array. # create a tensor as a parameter, with random initialization def create_params ( size ): return nn . Parameter ( torch . zeros ( * size ) . normal_ ( 0 , 0.01 )) The below is DotProductBias refactored class DotProductBias ( Module ): def __init__ ( self , n_users , n_movies , n_factors , y_range = ( 0 , 5.5 )): self . user_factors = create_params ([ n_users , n_factors ]) self . user_bias = create_params ([ n_users ]) self . movie_factors = create_params ([ n_movies , n_factors ]) self . movie_bias = create_params ([ n_movies ]) self . y_range = y_range def forward ( self , x ): users = self . user_factors [ x [:, 0 ]] movies = self . movie_factors [ x [:, 1 ]] res = ( users * movies ) . sum ( dim = 1 ) res += self . user_bias [ x [:, 0 ]] + self . movie_bias [ x [:, 1 ]] return sigmoid_range ( res , * self . y_range ) model = DotProductBias ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 , wd = 0.1 ) epoch train_loss valid_loss time 0 0.958299 0.949737 00:08 1 0.890685 0.870207 00:08 2 0.742867 0.825684 00:08 3 0.575261 0.817917 00:08 4 0.467277 0.818017 00:09 Interpreting Embeddings and Biases Let's take a look at some of the films with the smallest bias. These would be movies that were liked a lot less than others. We can then do the opposite to see the most liked movies (sorting by bias) The goal is to see what the model has learnt and to gain some information about how it is operating. Then using PCA we can reduce the number of latent factors and plot these to view the \"space\". Again, this is a way we can interpret what the model has learnt. movie_bias = learn . model . movie_bias . squeeze () idxs = movie_bias . argsort ()[: 5 ] [ dls . classes [ 'title' ][ i ] for i in idxs ] ['Children of the Corn: The Gathering (1996)', 'Lawnmower Man 2: Beyond Cyberspace (1996)', 'Robocop 3 (1993)', 'Leave It to Beaver (1997)', 'Vampire in Brooklyn (1995)'] # most liked films idxs = movie_bias . argsort ( descending = True )[: 5 ] [ dls . classes [ 'title' ][ i ] for i in idxs ] ['Titanic (1997)', \"Schindler's List (1993)\", 'As Good As It Gets (1997)', 'L.A. Confidential (1997)', 'Apt Pupil (1998)'] g = ratings . groupby ( 'title' )[ 'rating' ] . count () top_movies = g . sort_values ( ascending = False ) . index . values [: 1000 ] top_idxs = tensor ([ learn . dls . classes [ 'title' ] . o2i [ m ] for m in top_movies ]) movie_w = learn . model . movie_factors [ top_idxs ] . cpu () . detach () movie_pca = movie_w . pca ( 3 ) fac0 , fac1 , fac2 = movie_pca . t () idxs = list ( range ( 50 )) X = fac0 [ idxs ] Y = fac2 [ idxs ] plt . figure ( figsize = ( 12 , 12 )) plt . scatter ( X , Y ) for i , x , y in zip ( top_movies [ idxs ], X , Y ): plt . text ( x , y , i , color = np . random . rand ( 3 ) * 0.7 , fontsize = 11 ) plt . show () The most interesting cluster I can see is on the mid-right hand side. Conspiracy Theory, Mission Impossible, Air Force One etc. Asside from Liar Liar, these seem like the kinds of movies that someone who likes action films would likely enjoy. The fastai way learn = collab_learner ( dls , n_factors = 50 , y_range = ( 0 , 5.5 )) learn . fit_one_cycle ( 5 , 5e-3 , wd = 0.1 ) epoch train_loss valid_loss time 0 0.941142 0.945376 00:08 1 0.861970 0.873035 00:08 2 0.724418 0.828043 00:08 3 0.617956 0.815366 00:08 4 0.488715 0.815252 00:08 Summary We have just implemented a simple collaborative filtering model from scratch. The idea with this lesson, as with most of them so far, is to dig into the theory, code a model from scratch (mostly), improve the mode, then use the fastai implementation.","title":"Lesson 06 pt 2"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#lesson-6-collaborative-filtering","text":"This notebook will cover collaborative filtering using the MovieLens data set. from fastai.collab import * from fastai.tabular.all import * path = untar_data ( URLs . ML_100k )","title":"Lesson 6: Collaborative Filtering"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#data","text":"\"MovieLens data sets were collected by the GroupLens Research Project at the University of Minnesota. This data set consists of: 100,000 ratings (1-5) from 943 users on 1682 movies. Each user has rated at least 20 movies. Simple demographic info for the users (age, gender, occupation, zip)\" Additional Info u.data contains the full data set, 100,000 ratings by 943 users on 1,682 items. Each user has rated at least 20 movies. Users and items are numbered consecutively from 1. The data is randomly ordered. This is a tab separated list of user id | item id | rating | timestamp The time stamps are unix seconds since 1/1/1970 UTC source ratings = pd . read_csv ( path / 'u.data' , delimiter = ' \\t ' , header = None , names = [ 'user' , 'movie' , 'rating' , 'timestamp' ]) ratings . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user movie rating timestamp 0 196 242 3 881250949 1 186 302 3 891717742 2 22 377 1 878887116 3 244 51 2 880606923 4 166 346 1 886397596 The goal is to predict or guess what films users might like to watch. You could easily imagine that a user might have a preference for certain genres, and based on films they have seen from a particular genre, you might be able to say something like, user 123 likes action movies, therefore it would be safe to suggest an action movie to them. Given that we have minimal information in our data set (userid, movieid, rating and timestamp), collaborative filter seeks to solve this problem by extracting latent features from the data. For example, assume that these features range between -1 and +1, with postive numbers indicating stronger mathes to certain factors. We can use a simple example to illustrate the point. Take the following three dummy factors science-fiction , action , and old movies , we can compare user preferences against these for two different movies and see how they score. import numpy as np last_skywalker = np . array ([ 0.98 , 0.9 , - 0.9 ]) casablanca = np . array ([ - 0.99 , - 0.3 , 0.8 ]) user1 = np . array ([ 0.9 , 0.8 , - 0.6 ]) We can compute the dot product and arrive at a match m1 = ( user1 * last_skywalker ) . sum () m2 = ( user1 * casablanca ) . sum () print ( f 'last skywalker match: { m1 . round ( 2 ) } \\n casablanca match: { m2 . round ( 2 ) } ' ) last skywalker match: 2.14 casablanca match: -1.61 Voila! based on this we might want to recommend Last Skywalker but not Casablanca to this user. So how do we find these latent factors? They can be learned.","title":"Data"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#step-1-randomly-initialize-parameters","text":"Randomly assign parameters to represent our latent factors for each user and each movie. We get to decide how many of these factors we want to use.","title":"Step 1: Randomly Initialize Parameters"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#step-2-calculate-predictions","text":"Calculate predictions. This is done as we have just seen, by computing the dot product.","title":"Step 2: Calculate Predictions"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#step-3-improve-predictions","text":"Then improve the prediction using gradient descent on these latent factors First, let's add the movie titles to our data set for readability movies = pd . read_csv ( path / 'u.item' , delimiter = '|' , encoding = 'latin-1' , usecols = ( 0 , 1 ), names = ( 'movie' , 'title' ), header = None ) movies . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } movie title 0 1 Toy Story (1995) 1 2 GoldenEye (1995) 2 3 Four Rooms (1995) 3 4 Get Shorty (1995) 4 5 Copycat (1995) # join on movie titles ratings = ratings . merge ( movies ) ratings . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } user movie rating timestamp title 0 196 242 3 881250949 Kolya (1996) 1 63 242 3 875747190 Kolya (1996) 2 226 242 5 883888671 Kolya (1996) 3 154 242 3 879138235 Kolya (1996) 4 306 242 5 876503793 Kolya (1996)","title":"Step 3: Improve Predictions"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#create-a-dataloader","text":"dls = CollabDataLoaders . from_df ( ratings , item_name = 'title' , bs = 64 ) dls . show_batch () user title rating 0 494 Shawshank Redemption, The (1994) 5 1 806 Wrong Trousers, The (1993) 5 2 91 Glory (1989) 5 3 497 Lawnmower Man 2: Beyond Cyberspace (1996) 2 4 630 Rainmaker, The (1997) 3 5 89 That Thing You Do! (1996) 2 6 442 Brothers McMullen, The (1995) 3 7 37 Braveheart (1995) 5 8 159 Kansas City (1996) 1 9 585 Cinema Paradiso (1988) 5 dls . classes {'user': (#944) ['#na#',1,2,3,4,5,6,7,8,9...], 'title': (#1665) ['#na#',\"'Til There Was You (1997)\",'1-900 (1994)','101 Dalmatians (1996)','12 Angry Men (1957)','187 (1997)','2 Days in the Valley (1996)','20,000 Leagues Under the Sea (1954)','2001: A Space Odyssey (1968)','3 Ninjas: High Noon At Mega Mountain (1998)'...]}","title":"Create a DataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#randomly-initialize-parameters","text":"create user_factors and movie_factors of size n users x n factors and n movies by n factors n_users = len ( dls . classes [ 'user' ]) n_movies = len ( dls . classes [ 'title' ]) n_factors = 5 user_factors = torch . randn ( n_users , n_factors ) movie_factors = torch . randn ( n_movies , n_factors ) user_factors . size () torch.Size([944, 5])","title":"Randomly initialize parameters"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#one-hot-encoding","text":"We need to prepare our data in a specific way before we can pass it to our model. Some algorithms can work directly with category labels, but many cannot. One hot encoding is a way of representing categorical data by transforming categorical labels into vectors of 0s and 1s. To get a sense of how one-hot-encoding is operating, here is a simple example... df = pd . DataFrame ([ 'adam' , 'beatrix' , 'cam' ]) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 0 adam 1 beatrix 2 cam # the categorical variables are represented by 1s and 0s pd . get_dummies ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0_adam 0_beatrix 0_cam 0 1 0 0 1 0 1 0 2 0 0 1 In order to calculate a result for a particular user and movie combination, we weill need to look up the index of the movie, and the index of a user in the respective latent factor matrices, then perform a dot product. But this is not something that our model is capable of doing. It is capable of performing dot products though.. Here is another example of how we can use dot products to return elements from a matrix... say the rows in matrix w represent latent movie factors and the matrix v is our one hot encoded movie ids w = torch . randn (( 3 , 3 )) w tensor([[-1.7224, -0.4789, 0.3553], [-1.3465, -0.3057, 0.6882], [ 0.4594, 0.7893, 0.0150]]) v = torch . tensor ([[ 1 , 0 , 0 ],[ 0 , 1 , 0 ],[ 0 , 0 , 1 ]]) v tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]]) we can retrieve the factors for movie 2 (row 2 of w ) by preforming a matrix product on the corresponding row of v v [ 1 ] . float () @ w tensor([-1.3465, -0.3057, 0.6882]) this is the same as calling.. w [ 1 ] tensor([-1.3465, -0.3057, 0.6882]) One hot encoding is basically performing an index lookup on our data. This is however memory intensive since we now have these huge matrices to deal with and most of the values will be 0. Enter embeddings . Embeddings are a computational shortcut for doing matrix multiplication of one-hot-encoded vectors I found this link useful","title":"One Hot Encoding"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#collaborative-filtering-from-scratch","text":"forward is a very important method name in PyTorch. forward will be the method that handles computation.","title":"Collaborative filtering from Scratch"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#dotproduct","text":"The forward method here will get passed users and movies in two column ( x ). We then grab the factors from an embedding by calling user_factors just like a function. Assign these to users and movies then perform the dot product using (users * movies).sum(dim=1) . dim=1 is used because we want to sum over the second index class DotProduct ( Module ): def __init__ ( self , n_users , n_movies , n_factors ): self . user_factors = Embedding ( n_users , n_factors ) self . movie_factors = Embedding ( n_movies , n_factors ) def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) return ( users * movies ) . sum ( dim = 1 ) class DotProduct ( Module ): def __init__ ( self , n_users , n_movies , n_factors ): self . user_factors = Embedding ( n_users , n_factors ) self . movie_factors = Embedding ( n_movies , n_factors ) def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) return ( users * movies ) . sum ( dim = 1 ) x , y = dls . one_batch () # check the shape of x x . shape torch.Size([64, 2]) # check the first 5 things in x x [: 5 ] # user ids and movie ids tensor([[ 396, 1021], [ 118, 887], [ 206, 380], [ 207, 938], [ 923, 861]], device='cuda:0') What does torch.Size([64, 2]) tell us? - batch size is 64 - then we have 2 items, the user ids and movie ids - check with x[:,0] and x[:,1] # check the first 5 things of 7 y [: 5 ] # these are the ratings tensor([[4], [5], [1], [3], [4]], device='cuda:0', dtype=torch.int8)","title":"DotProduct"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#create-a-learner","text":"our model will be the DotProduct class with 50 latent factors loss function will be MSE this is a regression problem for continuous variables model = DotProduct ( n_users , n_movies , n_factors = 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 ) epoch train_loss valid_loss time 0 1.362978 1.286954 00:08 1 1.051144 1.094442 00:08 2 0.963542 0.975438 00:08 3 0.841074 0.893598 00:08 4 0.788252 0.873474 00:08 Not bad but we can make some improvements. We can use a sigmoid to contrain our ratings between 0 and 5, matching what we see in our original data set. We will actually use 0-5.5 otherwise the sigmoid will prevent us ever getting a 5. # check rating scores/categories ratings . rating . value_counts () 4 34174 3 27145 5 21201 2 11370 1 6110 Name: rating, dtype: int64 help ( sigmoid_range ) Help on function sigmoid_range in module fastai.layers: sigmoid_range(x, low, high) Sigmoid function with range `(low, high)` class DotProduct ( Module ): def __init__ ( self , n_users , n_movies , n_factors , y_range = ( 0 , 5.5 )): self . user_factors = Embedding ( n_users , n_factors ) self . movie_factors = Embedding ( n_movies , n_factors ) self . y_range = y_range def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) return sigmoid_range (( users * movies ) . sum ( dim = 1 ), * self . y_range ) model = DotProduct ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 ) epoch train_loss valid_loss time 0 1.024472 0.989683 00:08 1 0.882552 0.901929 00:08 2 0.684762 0.860367 00:08 3 0.479302 0.864543 00:08 4 0.347512 0.870277 00:08 Not much better....","title":"Create a learner"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#adding-in-a-bias-term","text":"We can make further improvements by adding a bias term to our model. Why would we do this? Some movies may have a high rating because they are genuinely better movies, and some users may skew towards being more positive and therefore their rating could generally be more positive. The idea of the bias term is that we now have a way to represent this missing piece of information. # Add in a bias term for each user and each movie. class DotProductBias ( Module ): def __init__ ( self , n_users , n_movies , n_factors , y_range = ( 0 , 5.5 )): self . user_factors = Embedding ( n_users , n_factors ) self . user_bias = Embedding ( n_users , 1 ) self . movie_factors = Embedding ( n_movies , n_factors ) self . movie_bias = Embedding ( n_movies , 1 ) self . y_range = y_range def forward ( self , x ): users = self . user_factors ( x [:, 0 ]) movies = self . movie_factors ( x [:, 1 ]) res = ( users * movies ) . sum ( dim = 1 , keepdim = True ) res += self . user_bias ( x [:, 0 ]) + self . movie_bias ( x [:, 1 ]) return sigmoid_range ( res , * self . y_range ) model = DotProduct ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 ) epoch train_loss valid_loss time 0 0.969996 0.988517 00:08 1 0.900296 0.907242 00:08 2 0.691395 0.859838 00:08 3 0.491507 0.861104 00:08 4 0.365541 0.864733 00:08 Our final result is slightly better but we are actually overfitting! How can we stop this and train for longer?","title":"Adding in a Bias term"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#regularisation","text":"Regularisation is a set of techniques that help to reduce the capacity of the model. Regularisations helps to prevent overfitting of models. Rather than reduce the parameters, we can try to force the parameters to be smaller, unless they are required to be big.","title":"Regularisation"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#weight-decay","text":"Also known as L2 regularisation. It consists of adding to the loss function the sum of all the parameters squared. Why does this work and why would this prevent overfitting? - one way to decrease loss is to decrease the weights - limiting the weights is going to hinder the training (won't fit training set as well) but will help the model to generalise better In practice what we are doing is adding onto the gradients, the weights multiplied by some hyper parameter. model = DotProductBias ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 , wd = 0.1 ) epoch train_loss valid_loss time 0 0.953578 0.935531 00:08 1 0.846484 0.874122 00:07 2 0.718206 0.829690 00:07 3 0.593549 0.817133 00:07 4 0.474108 0.818135 00:08 much better!","title":"Weight Decay"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#creating-our-own-embedding-module","text":"Let's recreate the DotProductBias without using the Embedding class. to recap: - an embedding layer is a computational shortcut for performing a matrix multiplication by a one hot encoded matrix, which is the same as indexing into an array. # create a tensor as a parameter, with random initialization def create_params ( size ): return nn . Parameter ( torch . zeros ( * size ) . normal_ ( 0 , 0.01 )) The below is DotProductBias refactored class DotProductBias ( Module ): def __init__ ( self , n_users , n_movies , n_factors , y_range = ( 0 , 5.5 )): self . user_factors = create_params ([ n_users , n_factors ]) self . user_bias = create_params ([ n_users ]) self . movie_factors = create_params ([ n_movies , n_factors ]) self . movie_bias = create_params ([ n_movies ]) self . y_range = y_range def forward ( self , x ): users = self . user_factors [ x [:, 0 ]] movies = self . movie_factors [ x [:, 1 ]] res = ( users * movies ) . sum ( dim = 1 ) res += self . user_bias [ x [:, 0 ]] + self . movie_bias [ x [:, 1 ]] return sigmoid_range ( res , * self . y_range ) model = DotProductBias ( n_users , n_movies , 50 ) learn = Learner ( dls , model , loss_func = MSELossFlat ()) learn . fit_one_cycle ( 5 , 5e-3 , wd = 0.1 ) epoch train_loss valid_loss time 0 0.958299 0.949737 00:08 1 0.890685 0.870207 00:08 2 0.742867 0.825684 00:08 3 0.575261 0.817917 00:08 4 0.467277 0.818017 00:09","title":"Creating our own Embedding module"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#interpreting-embeddings-and-biases","text":"Let's take a look at some of the films with the smallest bias. These would be movies that were liked a lot less than others. We can then do the opposite to see the most liked movies (sorting by bias) The goal is to see what the model has learnt and to gain some information about how it is operating. Then using PCA we can reduce the number of latent factors and plot these to view the \"space\". Again, this is a way we can interpret what the model has learnt. movie_bias = learn . model . movie_bias . squeeze () idxs = movie_bias . argsort ()[: 5 ] [ dls . classes [ 'title' ][ i ] for i in idxs ] ['Children of the Corn: The Gathering (1996)', 'Lawnmower Man 2: Beyond Cyberspace (1996)', 'Robocop 3 (1993)', 'Leave It to Beaver (1997)', 'Vampire in Brooklyn (1995)'] # most liked films idxs = movie_bias . argsort ( descending = True )[: 5 ] [ dls . classes [ 'title' ][ i ] for i in idxs ] ['Titanic (1997)', \"Schindler's List (1993)\", 'As Good As It Gets (1997)', 'L.A. Confidential (1997)', 'Apt Pupil (1998)'] g = ratings . groupby ( 'title' )[ 'rating' ] . count () top_movies = g . sort_values ( ascending = False ) . index . values [: 1000 ] top_idxs = tensor ([ learn . dls . classes [ 'title' ] . o2i [ m ] for m in top_movies ]) movie_w = learn . model . movie_factors [ top_idxs ] . cpu () . detach () movie_pca = movie_w . pca ( 3 ) fac0 , fac1 , fac2 = movie_pca . t () idxs = list ( range ( 50 )) X = fac0 [ idxs ] Y = fac2 [ idxs ] plt . figure ( figsize = ( 12 , 12 )) plt . scatter ( X , Y ) for i , x , y in zip ( top_movies [ idxs ], X , Y ): plt . text ( x , y , i , color = np . random . rand ( 3 ) * 0.7 , fontsize = 11 ) plt . show () The most interesting cluster I can see is on the mid-right hand side. Conspiracy Theory, Mission Impossible, Air Force One etc. Asside from Liar Liar, these seem like the kinds of movies that someone who likes action films would likely enjoy.","title":"Interpreting Embeddings and Biases"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#the-fastai-way","text":"learn = collab_learner ( dls , n_factors = 50 , y_range = ( 0 , 5.5 )) learn . fit_one_cycle ( 5 , 5e-3 , wd = 0.1 ) epoch train_loss valid_loss time 0 0.941142 0.945376 00:08 1 0.861970 0.873035 00:08 2 0.724418 0.828043 00:08 3 0.617956 0.815366 00:08 4 0.488715 0.815252 00:08","title":"The fastai way"},{"location":"fastai%20deep%20learning%202020/lesson%2006%20pt%202/#summary","text":"We have just implemented a simple collaborative filtering model from scratch. The idea with this lesson, as with most of them so far, is to dig into the theory, code a model from scratch (mostly), improve the mode, then use the fastai implementation.","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2007/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 7: Tabular Data Pt 1 Decision Tree Ensembles Some examples include... - Random Forest (Regressor / Classifier) - Gradient Boost (Regressor / Classifier) - XGBoost (Regressor / Classifier) We will use the Scikit-Learn library for this task rather than PyTorch. The Data Blue Book for Bulldozers Kaggle Competition The goal being to predict sale price of heavy equipment at auction based on ussage, type and configuration. Kaggle setup help There are a number of different ways to do this, I had some trouble doing this so after some reading, tried this instead. fastai helper functions Had some trouble importing fastais helper functions, found that someone has added them all here from pathlib import Path from pandas.api.types import is_string_dtype , is_numeric_dtype , is_categorical_dtype from fastai.tabular.all import * # helper functions from fastbook import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor , export_graphviz from dtreeviz.trees import * import IPython from IPython.display import Image , display_svg , SVG import os Load credentials from json file. This is a simple file with the following information... { \"username\" : \"xxx\" , \"key\" : \"xxx\" } import json with open ( 'creds.json' ) as f : creds = json . load ( f ) os . environ [ 'KAGGLE_USERNAME' ] = creds [ \"username\" ] os . environ [ 'KAGGLE_KEY' ] = creds [ \"key\" ] from kaggle import api api . competition_download_cli ( 'bluebook-for-bulldozers' ) 10%|\u2588 | 5.00M/48.4M [00:00<00:01, 30.5MB/s] Downloading bluebook-for-bulldozers.zip to /notebooks 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48.4M/48.4M [00:01<00:00, 28.5MB/s] p = Path . cwd () for i in p . iterdir (): print ( i ) /notebooks/.ipynb_checkpoints /notebooks/.kaggle /notebooks/course-v4 /notebooks/fastbook /notebooks/lesson1_assets /notebooks/models /notebooks/20200920_fastai_lesson_prod_app.ipynb /notebooks/20201006_fastai_lesson_6.ipynb /notebooks/20201026_fastai_lesson_6_collab.ipynb /notebooks/bluebook-for-bulldozers.zip /notebooks/lesson_7_tabular.ipynb /notebooks/storage /notebooks/datasets fname = p / 'bluebook-for-bulldozers.zip' dest = p / 'storage/data/bluebook' # dest.mkdir() # only run once! #file_extract(fname, dest) The Data each row of the dataset represents the sale of a single machine at an auction df = pd . read_csv ( dest / 'TrainAndValid.csv' , low_memory = False ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate ... Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls 0 1139246 66000.0 999089 3157 121 3.0 2004 68.0 Low 11/16/2006 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN Standard Conventional 1 1139248 57000.0 117657 77 121 3.0 1996 4640.0 Low 3/26/2004 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN Standard Conventional 2 1139249 10000.0 434808 7009 121 3.0 2001 2838.0 High 2/26/2004 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 1139251 38500.0 1026470 332 121 3.0 2001 3486.0 High 5/19/2011 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 1139253 11000.0 1057373 17311 121 3.0 2007 722.0 Medium 7/23/2009 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows \u00d7 53 columns df . columns Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource', 'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand', 'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls', 'Differential_Type', 'Steering_Controls'], dtype='object') Transforming data convert categorical ie ProductSize set order df [ 'ProductSize' ] . unique () array([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object) sizes = 'Large' , 'Large / Medium' , 'Medium' , 'Small' , 'Mini' , 'Compact' df [ 'ProductSize' ] = df [ 'ProductSize' ] . astype ( 'category' ) df [ 'ProductSize' ] . cat . set_categories ( sizes , ordered = True , inplace = True ) dependent variable here is SalePrice , this is the variable we want to predict. Kaggle specifically tells us the metric to use: root mean squared log error (RMSLE). To use this metric, we need to take the log of the dependent variable which will allow use to use RMSE dep_var = 'SalePrice' df [ dep_var ] = np . log ( df [ dep_var ]) Decision Trees ask binary questions about data ie is x > y the trouble is we don't know what binary questions to ask, and through machine learning, we need to decide on what these will be. Handling Dates df = add_datepart ( df , 'saledate' ) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/tabular/core.py:33: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead. for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) df_test = pd . read_csv ( dest / 'Test.csv' , low_memory = False ) df_test = add_datepart ( df_test , 'saledate' ) Using TabularPandas and TabularProc we will use two tabular transforms to modify our data. - Categorify - replaces a column with numeric category - FillMissign - fills any missing data with the median - also adds a boolean column where True will be set for any data point that was missing procs = [ Categorify , FillMissing ] Validation Set hold aside some data (approx 2 weeks) as per the competition rules - do this with np.where cond = ( df . saleYear < 2011 ) | ( df . saleMonth < 10 ) train_idx = np . where ( cond )[ 0 ] valid_idx = np . where ( ~ cond )[ 0 ] # inverse condition splits = ( list ( train_idx ), list ( valid_idx )) Here cont_cat_split is a helper function that returns column names of cont and cat variables from given df . # define continuous and categorical columns for TabularPandas cont , cat = cont_cat_split ( df , 1 , dep_var = dep_var ) create a Tabular Object (to) to = TabularPandas ( df , procs , cat , cont , y_names = dep_var , splits = splits ) len ( to . train ), len ( to . valid ) (404710, 7988) Look at the data with to.show . This will show the data in human readable form. to.items.head will show the processed data in numerical form. to . show ( 3 ) UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear SalePrice 0 Low 521D 521 D #na# #na# #na# Wheel Loader - 110.0 to 120.0 Horsepower Alabama WL Wheel Loader #na# EROPS w AC None or Unspecified #na# None or Unspecified #na# #na# #na# #na# #na# #na# #na# 2 Valve #na# #na# #na# #na# None or Unspecified None or Unspecified #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# Standard Conventional False False False False False False 1163635200 False False 1139246 999089 3157 121 3.0 2004 68.0 2006 11 46 16 3 320 11.097410 1 Low 950FII 950 F II #na# Medium Wheel Loader - 150.0 to 175.0 Horsepower North Carolina WL Wheel Loader #na# EROPS w AC None or Unspecified #na# None or Unspecified #na# #na# #na# #na# #na# #na# #na# 2 Valve #na# #na# #na# #na# 23.5 None or Unspecified #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# Standard Conventional False False False False False False 1080259200 False False 1139248 117657 77 121 3.0 1996 4640.0 2004 3 13 26 4 86 10.950807 2 High 226 226 #na# #na# #na# #na# Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity New York SSL Skid Steer Loaders #na# OROPS None or Unspecified #na# #na# #na# #na# #na# #na# #na# #na# #na# Auxiliary #na# #na# #na# #na# #na# None or Unspecified None or Unspecified None or Unspecified Standard #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# False False False False False False 1077753600 False False 1139249 434808 7009 121 3.0 2001 2838.0 2004 2 9 26 3 57 9.210340 to . items . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand fiModelDesc ... saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na 0 1139246 11.097410 999089 3157 121 3.0 2004 68.0 2 963 ... 320 1 1 1 1 1 1 2647 1 1 1 1139248 10.950807 117657 77 121 3.0 1996 4640.0 2 1745 ... 86 1 1 1 1 1 1 2148 1 1 2 1139249 9.210340 434808 7009 121 3.0 2001 2838.0 1 336 ... 57 1 1 1 1 1 1 2131 1 1 3 rows \u00d7 67 columns check the vocab with to.classes to . classes [ 'ProductSize' ] (#7) ['#na#','Large','Large / Medium','Medium','Small','Mini','Compact'] # save the tabular object for later # (dest/'to.pkl').save(to) Decision Tree Regressor: for continuous variables # had to copy from # https://github.com/anandsaha/fastai.part1.v2/blob/master/fastai/structured.py def draw_tree ( t , df , size = 10 , ratio = 0.6 , precision = 0 ): \"\"\" Draws a representation of a random forest in IPython. Parameters: ----------- t: The tree you wish to draw df: The data used to train the tree. This is used to get the names of the features. \"\"\" s = export_graphviz ( t , out_file = None , feature_names = df . columns , filled = True , special_characters = True , rotate = True , precision = precision ) IPython . display . display ( graphviz . Source ( re . sub ( 'Tree {' , f 'Tree {{ size= { size } ; ratio= { ratio } ' , s ))) xs , y = to . train . xs , to . train . y valid_xs , valid_y = to . valid . xs , to . valid . y m = DecisionTreeRegressor ( max_leaf_nodes = 4 ) m . fit ( xs , y ); draw_tree ( m , xs , precision = 2 ); Tree 0 Coupler_System \u2264 0.5 mse = 0.48 samples = 404710 value = 10.1 1 YearMade \u2264 1991.5 mse = 0.42 samples = 360847 value = 10.21 0->1 True 2 mse = 0.12 samples = 43863 value = 9.21 0->2 False 3 mse = 0.37 samples = 155724 value = 9.97 1->3 4 ProductSize \u2264 4.5 mse = 0.37 samples = 205123 value = 10.4 1->4 5 mse = 0.31 samples = 182403 value = 10.5 4->5 6 mse = 0.17 samples = 22720 value = 9.62 4->6 samp_idx = np . random . permutation ( len ( y ))[: 500 ] dtreeviz ( m , xs . iloc [ samp_idx ], y . iloc [ samp_idx ], xs . columns , dep_var , fontname = 'DejaVu Sans' , scale = 1.6 , label_fontsize = 10 , orientation = 'LR' ) G node4 2020-11-30T06:28:05.016208 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} leaf5 2020-11-30T06:28:05.560116 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf5 leaf6 2020-11-30T06:28:05.654756 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf6 node1 2020-11-30T06:28:05.131089 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->node4 leaf3 2020-11-30T06:28:05.398894 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->leaf3 leaf2 2020-11-30T06:28:05.742191 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0 2020-11-30T06:28:05.254063 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0->node1 < node0->leaf2 \u2265 Using Dtreeviz is a little more intuitive and provides a bit more information. For example, YearMade is showing that there are years equal to 1000, which is obviously an error in the data. Let's fix that xs . loc [ xs [ 'YearMade' ] < 1900 , 'YearMade' ] = 1950 valid_xs . loc [ valid_xs [ 'YearMade' ] < 1900 , 'YearMade' ] = 1950 m = DecisionTreeRegressor ( max_leaf_nodes = 4 ) . fit ( xs , y ) dtreeviz ( m , xs . iloc [ samp_idx ], y . iloc [ samp_idx ], xs . columns , dep_var , fontname = 'DejaVu Sans' , scale = 1.6 , label_fontsize = 10 , orientation = 'LR' ) G node4 2020-11-30T06:28:21.881440 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} leaf5 2020-11-30T06:28:22.494637 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf5 leaf6 2020-11-30T06:28:22.604794 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf6 node1 2020-11-30T06:28:22.010995 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->node4 leaf3 2020-11-30T06:28:22.367366 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->leaf3 leaf2 2020-11-30T06:28:22.694135 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0 2020-11-30T06:28:22.156796 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0->node1 < node0->leaf2 \u2265 # remove max_leaf_nodes # build a bigger tree m = DecisionTreeRegressor () m . fit ( xs , y ) DecisionTreeRegressor() def r_mse ( preds , y ): return round ( math . sqrt ((( preds - y ) ** 2 ) . mean ()), 6 ) def m_rmse ( m , xs , y ): return r_mse ( m . predict ( xs ), y ) m_rmse ( m , xs , y ) 0.0 m_rmse ( m , valid_xs , valid_y ) 0.333069 Our training set is 0 and validation set is worse than the observed value from the original tree viz. The reason is that there are almost as many leaves in our model than observations in our data set. To avoid this, we need to pick some stopping criteria, like some threshold that will tell the model, don't split this if there are less than x number of items in the leaf node. Do this with min_samples_leaf . m . get_n_leaves (), len ( xs ) (324549, 404710) we have nearly as many leaf nodes as observations in our dataset we need to create some rules here m = DecisionTreeRegressor ( min_samples_leaf = 25 ) m . fit ( to . train . xs , to . train . y ) m_rmse ( m , xs , y ), m_rmse ( m , valid_xs , valid_y ) (0.248593, 0.323391) m . get_n_leaves () 12397 Catagorical Variables Unlike with collab filtering, we do not need to create dummy variables with categorical values because through pre-processing, we have already transformed these into numerical values. However, you can one-hot encode if you like. Bagging A technique developed by professor Leo Breiman. The idea is that you can bootstrap subsets of your data, train your model, store the predictions, then average the predictions. Steps 1. randomly choose a subset 2. train a model on the subset 3. save the model, return to step one 4. make a prediction using all of the models, then take the average of each model's prediction Leo furthered his thinking by not only selecting a random subset of rows, but also a random subset of columns. This is known as a Random Forrest . The function rf below uses the following arguments - n_estimators defines the number of trees - max_samples defines how many rows to sample for training each tree - max_features defines how many columns to sample at each split point (0.5 means \"take half the total number of columns\"). - min_samples_leaf specify when to stop splitting the tree nodes - effectively limiting the depth of the tree - n_jobs=-1 use CPUs to build the trees in parallel. def rf ( xs , y , n_estimators = 40 , max_samples = 200_000 , max_features = 0.5 , min_samples_leaf = 5 , ** kwargs ): return RandomForestRegressor ( n_jobs =- 1 , n_estimators = n_estimators , max_samples = max_samples , max_features = max_features , min_samples_leaf = min_samples_leaf , oob_score = True ) . fit ( xs , y ) m = rf ( xs , y ) m_rmse ( m , xs , y ), m_rmse ( m , valid_xs , valid_y ) (0.171231, 0.234308) That is much better! Making Predictions To understand the impact of n_estimators you can get predictions from each individual tree in the forest preds = np . stack ([ t . predict ( valid_xs ) for t in m . estimators_ ]) preds array([[10.11121098, 9.94458659, 9.42150625, ..., 9.17998473, 9.29954442, 9.29954442], [10.14101274, 9.82777866, 9.55376172, ..., 9.48364408, 9.48364408, 9.48364408], [10.01018006, 10.1378665 , 9.27639723, ..., 9.49919689, 9.18244871, 9.18244871], ..., [ 9.88747565, 9.52539463, 9.46619672, ..., 9.24248886, 9.22252042, 9.22252042], [10.5004158 , 9.98228111, 9.20137348, ..., 9.43222591, 9.30666413, 9.30666413], [10.08093796, 10.61704159, 9.33421822, ..., 9.26213868, 9.29758778, 9.29758778]]) This represents every prediction for each and every tree for every row of data. r_mse ( preds . mean ( axis = 0 ), valid_y ) 0.234308 Here is how to make a single rediction. I think!! CHECK THIS # slice a row of data valid_xs . iloc [ 0 ] UsageBand 2.0 fiModelDesc 2301.0 fiBaseModel 706.0 fiSecondaryDesc 43.0 fiModelSeries 0.0 ... saleMonth 10.0 saleWeek 40.0 saleDay 3.0 saleDayofweek 0.0 saleDayofyear 276.0 Name: 22915, Length: 66, dtype: float64 # predict requires a 2D array, reshape your data data = valid_xs . iloc [ 0 ] . values . reshape ( 1 , - 1 ) m . predict ( data ) array([10.0005727]) You can visualise how RMSE improvs as more and more trees are added plt . plot ([ r_mse ( preds [: i + 1 ] . mean ( 0 ), valid_y ) for i in range ( 40 )]); validation set is worse than the training set. Why? we might be overfitting the last two weeks of the auction data may have been different somehow Out of Bag Error (OOB error) how can we check? we can use OOB predictions from the model and run rmse on the oob error. r_mse ( m . oob_prediction_ , y ) 0.211059 What is happening here? OOB error gives you a sense of how much you are overfitting. Model interpretation source For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: How confident are we in our predictions using a particular row of data? For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? Which columns are the strongest predictors, which can we ignore? Which columns are effectively redundant with each other, for purposes of prediction? How do predictions vary, as we vary these columns? preds_std = preds . std ( 0 ) preds_std array([0.24516726, 0.17149769, 0.12129906, ..., 0.1911843 , 0.16064838, 0.16064838]) Feature Important def rf_feat_importance ( m , df ): return pd . DataFrame ({ 'cols' : df . columns , 'imp' : m . feature_importances_ } ) . sort_values ( 'imp' , ascending = False ) fi = rf_feat_importance ( m , xs ) fi [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cols imp 58 YearMade 0.175870 6 ProductSize 0.118317 30 Coupler_System 0.099115 7 fiProductClassDesc 0.071253 31 Grouser_Tracks 0.064081 55 ModelID 0.061831 50 saleElapsed 0.051997 32 Hydraulics_Flow 0.042906 3 fiSecondaryDesc 0.039384 1 fiModelDesc 0.031282 def plot_fi ( fi ): return fi . plot . barh ( 'cols' , 'imp' , figsize = ( 12 , 8 ), legend = False ) plot_fi ( fi [: 30 ]); Removing low-importance variables to_keep = fi [ fi . imp > 0.005 ] . cols len ( to_keep ) 20 retrain model using only this subset of columns xs_imp = xs [ to_keep ] valid_xs_imp = valid_xs [ to_keep ] m = rf ( xs_imp , y ) m_rmse ( m , xs_imp , y ), m_rmse ( m , valid_xs , valid_y ) (0.180874, 0.231109) cluster_columns ?? cluster_columns ( xs_imp ) seems like we could remove some of the clustered columns. Let's calculate a baseline using a sample of data def get_oob ( df ): m = RandomForestRegressor ( n_estimators = 40 , min_samples_leaf = 15 , max_samples = 50000 , max_features = 0.5 , n_jobs =- 1 , oob_score = True ) m . fit ( df , y ) return m . oob_score_ get_oob ( xs_imp ) 0.8769414512037411 now remove redundant columns one at a time { c : get_oob ( xs_imp . drop ( c , axis = 1 )) for c in ( 'saleYear' , 'saleElapsed' , 'ProductGroupDesc' , 'ProductGroup' , 'fiModelDesc' , 'fiBaseModel' , 'Hydraulics_Flow' , 'Grouser_Tracks' , 'Coupler_System' )} {'saleYear': 0.8754647378064608, 'saleElapsed': 0.8723764211506366, 'ProductGroupDesc': 0.8765810711892142, 'ProductGroup': 0.8773280451665235, 'fiModelDesc': 0.8758816619476205, 'fiBaseModel': 0.8756967579434771, 'Hydraulics_Flow': 0.8769591574648032, 'Grouser_Tracks': 0.876871969234521, 'Coupler_System': 0.8762546455208067} not much change here so try dropping multiple variables to_drop = [ 'saleYear' , 'ProductGroupDesc' , 'fiBaseModel' , 'Grouser_Tracks' ] get_oob ( xs_imp . drop ( to_drop , axis = 1 )) 0.875680776530026 xs_final = xs_imp . drop ( to_drop , axis = 1 ) valid_xs_final = valid_xs_imp . drop ( to_drop , axis = 1 ) check rmse again to confirm accuracy hasn't really changed m = rf ( xs_final , y ) m_rmse ( m , xs_final , y ), m_rmse ( m , valid_xs_final , valid_y ) (0.182544, 0.23238) similar accuracy but less features! Partial Dependence what is the relationship between variables p = valid_xs_final [ 'ProductSize' ] . value_counts ( sort = False ) . plot . barh () c = to . classes [ 'ProductSize' ] plt . yticks ( range ( len ( c )), c ); largest group is actual #na# do the same for yearmade ax = valid_xs_final [ 'YearMade' ] . hist () from sklearn.inspection import plot_partial_dependence fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plot_partial_dependence ( m , valid_xs_final , [ 'YearMade' , 'ProductSize' ], grid_resolution = 20 , ax = ax ); what is partial dependence telling us? We want to look at how year affects the sale price, that is, all else being equal, what affect does year have on sale price Tree Interpreter #!pip install treeinterpreter #!pip install waterfallcharts import warnings warnings . simplefilter ( 'ignore' , FutureWarning ) from treeinterpreter import treeinterpreter from waterfall_chart import plot as waterfall row = valid_xs_final . iloc [: 5 ] prediction , bias , contributions = treeinterpreter . predict ( m , row . values ) prediction [ 0 ], bias [ 0 ], contributions [ 0 ] . sum () (array([10.03875756]), 10.104200155980113, -0.06544259554720655) waterfall ( valid_xs_final . columns , contributions [ 0 ], threshold = 0.08 , rotation_value = 45 , formatting = ' {:,.3f} ' ); The extrapolation problem np . random . seed ( 42 ) x_lin = torch . linspace ( 0 , 20 , steps = 40 ) y_lin = x_lin + torch . randn_like ( x_lin ) plt . scatter ( x_lin , y_lin ); xs_lin = x_lin . unsqueeze ( 1 ) x_lin . shape , xs_lin . shape (torch.Size([40]), torch.Size([40, 1])) you can do the same using None x_lin [:, None ] . shape torch.Size([40, 1]) m_lin = RandomForestRegressor () . fit ( x_lin [: 30 ] . reshape ( - 1 , 1 ), y_lin [: 30 ]) plt . scatter ( x_lin , y_lin , 20 ) plt . scatter ( x_lin , m_lin . predict ( xs_lin ), color = 'red' , alpha = 0.5 ); random forrext cannot extrapolate outside of the bounds of the training data we need to make sure validation set does not contain out of domain data test and training set may vary, how do we tell?? Finding out of domain data df_dom = pd . concat ([ xs_final , valid_xs_final ]) is_valid = np . array ([ 0 ] * len ( xs_final ) + [ 1 ] * len ( valid_xs_final )) m = rf ( df_dom , is_valid ) rf_feat_importance ( m , df_dom )[: 6 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cols imp 5 saleElapsed 0.915808 10 SalesID 0.069088 13 MachineID 0.011871 0 YearMade 0.000678 4 ModelID 0.000536 12 Hydraulics 0.000529 m = rf ( xs_final , y ) print ( 'orig' , m_rmse ( m , valid_xs_final , valid_y )) for c in ( 'SalesID' , 'saleElapsed' , 'MachineID' ): m = rf ( xs_final . drop ( c , axis = 1 ), y ) print ( c , m_rmse ( m , valid_xs_final . drop ( c , axis = 1 ), valid_y )) orig 0.233484 SalesID 0.231357 saleElapsed 0.236643 MachineID 0.231104 time_vars = [ 'SalesID' , 'MachineID' ] xs_final_time = xs_final . drop ( time_vars , axis = 1 ) valid_xs_time = valid_xs_final . drop ( time_vars , axis = 1 ) m = rf ( xs_final_time , y ) m_rmse ( m , valid_xs_time , valid_y ) 0.229127 xs [ 'saleYear' ] . hist (); filt = xs [ 'saleYear' ] > 2004 xs_filt = xs_final_time [ filt ] y_filt = y [ filt ] m = rf ( xs_filt , y_filt ) m_rmse ( m , xs_filt , y_filt ), m_rmse ( m , valid_xs_time , valid_y ) (0.176904, 0.22864) Using a Neural Net # load data df_nn = pd . read_csv ( dest / 'TrainAndValid.csv' , low_memory = False ) # set ProductSize as categorical df_nn [ 'ProductSize' ] = df_nn [ 'ProductSize' ] . astype ( 'category' ) df_nn [ 'ProductSize' ] . cat . set_categories ( sizes , ordered = True , inplace = True ) # take log of dependent variable df_nn [ dep_var ] = np . log ( df_nn [ dep_var ]) # do some date prep df_nn = add_datepart ( df_nn , 'saledate' ) df_nn_final = df_nn [ list ( xs_final_time . columns ) + [ dep_var ]] cont_nn , cat_nn = cont_cat_split ( df_nn_final , max_card = 9000 , dep_var = dep_var ) cont_nn . append ( 'saleElapsed' ) cat_nn . remove ( 'saleElapsed' ) df_nn [ 'saleElapsed' ] = df_nn [ 'saleElapsed' ] . astype ( int ) df_nn_final [ cat_nn ] . nunique () YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 Hydraulics_Flow 3 fiSecondaryDesc 177 fiModelDesc 5059 Enclosure 6 ProductGroup 6 Hydraulics 12 fiModelDescriptor 140 Drive_System 4 dtype: int64 xs_filt2 = xs_filt . drop ( 'fiModelDescriptor' , axis = 1 ) valid_xs_time2 = valid_xs_time . drop ( 'fiModelDescriptor' , axis = 1 ) m2 = rf ( xs_filt2 , y_filt ) m_rmse ( m2 , xs_filt2 , y_filt ), m_rmse ( m2 , valid_xs_time2 , valid_y ) (0.178922, 0.230357) cat_nn . remove ( 'fiModelDescriptor' ) df_nn_final [ 'saleElapsed' ] . astype ( 'int64' , copy = False ) df_nn_final . dtypes YearMade int64 ProductSize category Coupler_System object fiProductClassDesc object ModelID int64 saleElapsed int64 Hydraulics_Flow object fiSecondaryDesc object fiModelDesc object Enclosure object ProductGroup object Hydraulics object fiModelDescriptor object Drive_System object SalePrice float64 dtype: object Normalize Normalize subtracts the mean, then divides by the standard deviation. We didn't need this for a decision tree because we were only performing binary splits. However, we do need to normalize for neaural nets because we don't want things with crazy distributions. procs_nn = [ Categorify , FillMissing , Normalize ] to_nn = TabularPandas ( df = df_nn_final , procs = procs_nn , cat_names = cat_nn , cont_names = cont_nn , splits = splits , y_names = dep_var ) dls = to_nn . dataloaders ( 1024 ) This is a regression model so we want to set our y_range based on the min and max of the dependent variable. y = to_nn . train . y y . min (), y . max () (8.465899, 11.863583) learn = tabular_learner ( dls , y_range = ( 8 , 12 ), layers = [ 500 , 250 ], n_out = 1 , loss_func = F . mse_loss ) learn . lr_find () SuggestedLRs(lr_min=0.003981071710586548, lr_steep=0.00019054606673307717) learn . fit_one_cycle ( 5 , 1e-2 ) epoch train_loss valid_loss time 0 0.069223 0.062953 00:11 1 0.056285 0.055872 00:13 2 0.048484 0.055052 00:12 3 0.043525 0.051425 00:12 4 0.040454 0.051055 00:11 preds , targs = learn . get_preds () r_mse ( preds , targs ) 0.225954 Summary Random Forests are easy to train, resillient, don't require much pre-processing, train quickly and don't overfit. They can be less accurate than a neural net and can take longer at inference time to evaluate the trees. Neural Nets are probably the fiddliest models to implement and set up but can give slightly better results.","title":"Lesson 07"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#lesson-7-tabular-data","text":"","title":"Lesson 7: Tabular Data"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#pt-1","text":"","title":"Pt 1"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#decision-tree-ensembles","text":"Some examples include... - Random Forest (Regressor / Classifier) - Gradient Boost (Regressor / Classifier) - XGBoost (Regressor / Classifier) We will use the Scikit-Learn library for this task rather than PyTorch.","title":"Decision Tree Ensembles"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#the-data","text":"Blue Book for Bulldozers Kaggle Competition The goal being to predict sale price of heavy equipment at auction based on ussage, type and configuration.","title":"The Data"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#kaggle-setup-help","text":"There are a number of different ways to do this, I had some trouble doing this so after some reading, tried this instead.","title":"Kaggle setup help"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#fastai-helper-functions","text":"Had some trouble importing fastais helper functions, found that someone has added them all here from pathlib import Path from pandas.api.types import is_string_dtype , is_numeric_dtype , is_categorical_dtype from fastai.tabular.all import * # helper functions from fastbook import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor , export_graphviz from dtreeviz.trees import * import IPython from IPython.display import Image , display_svg , SVG import os Load credentials from json file. This is a simple file with the following information... { \"username\" : \"xxx\" , \"key\" : \"xxx\" } import json with open ( 'creds.json' ) as f : creds = json . load ( f ) os . environ [ 'KAGGLE_USERNAME' ] = creds [ \"username\" ] os . environ [ 'KAGGLE_KEY' ] = creds [ \"key\" ] from kaggle import api api . competition_download_cli ( 'bluebook-for-bulldozers' ) 10%|\u2588 | 5.00M/48.4M [00:00<00:01, 30.5MB/s] Downloading bluebook-for-bulldozers.zip to /notebooks 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48.4M/48.4M [00:01<00:00, 28.5MB/s] p = Path . cwd () for i in p . iterdir (): print ( i ) /notebooks/.ipynb_checkpoints /notebooks/.kaggle /notebooks/course-v4 /notebooks/fastbook /notebooks/lesson1_assets /notebooks/models /notebooks/20200920_fastai_lesson_prod_app.ipynb /notebooks/20201006_fastai_lesson_6.ipynb /notebooks/20201026_fastai_lesson_6_collab.ipynb /notebooks/bluebook-for-bulldozers.zip /notebooks/lesson_7_tabular.ipynb /notebooks/storage /notebooks/datasets fname = p / 'bluebook-for-bulldozers.zip' dest = p / 'storage/data/bluebook' # dest.mkdir() # only run once! #file_extract(fname, dest)","title":"fastai helper functions"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#the-data_1","text":"each row of the dataset represents the sale of a single machine at an auction df = pd . read_csv ( dest / 'TrainAndValid.csv' , low_memory = False ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand saledate ... Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls 0 1139246 66000.0 999089 3157 121 3.0 2004 68.0 Low 11/16/2006 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN Standard Conventional 1 1139248 57000.0 117657 77 121 3.0 1996 4640.0 Low 3/26/2004 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN Standard Conventional 2 1139249 10000.0 434808 7009 121 3.0 2001 2838.0 High 2/26/2004 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 3 1139251 38500.0 1026470 332 121 3.0 2001 3486.0 High 5/19/2011 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 4 1139253 11000.0 1057373 17311 121 3.0 2007 722.0 Medium 7/23/2009 0:00 ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 5 rows \u00d7 53 columns df . columns Index(['SalesID', 'SalePrice', 'MachineID', 'ModelID', 'datasource', 'auctioneerID', 'YearMade', 'MachineHoursCurrentMeter', 'UsageBand', 'saledate', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension', 'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls', 'Differential_Type', 'Steering_Controls'], dtype='object')","title":"The Data"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#transforming-data","text":"convert categorical ie ProductSize set order df [ 'ProductSize' ] . unique () array([nan, 'Medium', 'Small', 'Large / Medium', 'Mini', 'Large', 'Compact'], dtype=object) sizes = 'Large' , 'Large / Medium' , 'Medium' , 'Small' , 'Mini' , 'Compact' df [ 'ProductSize' ] = df [ 'ProductSize' ] . astype ( 'category' ) df [ 'ProductSize' ] . cat . set_categories ( sizes , ordered = True , inplace = True ) dependent variable here is SalePrice , this is the variable we want to predict. Kaggle specifically tells us the metric to use: root mean squared log error (RMSLE). To use this metric, we need to take the log of the dependent variable which will allow use to use RMSE dep_var = 'SalePrice' df [ dep_var ] = np . log ( df [ dep_var ])","title":"Transforming data"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#decision-trees","text":"ask binary questions about data ie is x > y the trouble is we don't know what binary questions to ask, and through machine learning, we need to decide on what these will be.","title":"Decision Trees"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#handling-dates","text":"df = add_datepart ( df , 'saledate' ) /opt/conda/envs/fastai/lib/python3.8/site-packages/fastai/tabular/core.py:33: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated. Please use Series.dt.isocalendar().week instead. for n in attr: df[prefix + n] = getattr(field.dt, n.lower()) df_test = pd . read_csv ( dest / 'Test.csv' , low_memory = False ) df_test = add_datepart ( df_test , 'saledate' )","title":"Handling Dates"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#using-tabularpandas-and-tabularproc","text":"we will use two tabular transforms to modify our data. - Categorify - replaces a column with numeric category - FillMissign - fills any missing data with the median - also adds a boolean column where True will be set for any data point that was missing procs = [ Categorify , FillMissing ] Validation Set hold aside some data (approx 2 weeks) as per the competition rules - do this with np.where cond = ( df . saleYear < 2011 ) | ( df . saleMonth < 10 ) train_idx = np . where ( cond )[ 0 ] valid_idx = np . where ( ~ cond )[ 0 ] # inverse condition splits = ( list ( train_idx ), list ( valid_idx )) Here cont_cat_split is a helper function that returns column names of cont and cat variables from given df . # define continuous and categorical columns for TabularPandas cont , cat = cont_cat_split ( df , 1 , dep_var = dep_var )","title":"Using TabularPandas and TabularProc"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#create-a-tabular-object-to","text":"to = TabularPandas ( df , procs , cat , cont , y_names = dep_var , splits = splits ) len ( to . train ), len ( to . valid ) (404710, 7988) Look at the data with to.show . This will show the data in human readable form. to.items.head will show the processed data in numerical form. to . show ( 3 ) UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear SalePrice 0 Low 521D 521 D #na# #na# #na# Wheel Loader - 110.0 to 120.0 Horsepower Alabama WL Wheel Loader #na# EROPS w AC None or Unspecified #na# None or Unspecified #na# #na# #na# #na# #na# #na# #na# 2 Valve #na# #na# #na# #na# None or Unspecified None or Unspecified #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# Standard Conventional False False False False False False 1163635200 False False 1139246 999089 3157 121 3.0 2004 68.0 2006 11 46 16 3 320 11.097410 1 Low 950FII 950 F II #na# Medium Wheel Loader - 150.0 to 175.0 Horsepower North Carolina WL Wheel Loader #na# EROPS w AC None or Unspecified #na# None or Unspecified #na# #na# #na# #na# #na# #na# #na# 2 Valve #na# #na# #na# #na# 23.5 None or Unspecified #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# Standard Conventional False False False False False False 1080259200 False False 1139248 117657 77 121 3.0 1996 4640.0 2004 3 13 26 4 86 10.950807 2 High 226 226 #na# #na# #na# #na# Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity New York SSL Skid Steer Loaders #na# OROPS None or Unspecified #na# #na# #na# #na# #na# #na# #na# #na# #na# Auxiliary #na# #na# #na# #na# #na# None or Unspecified None or Unspecified None or Unspecified Standard #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# #na# False False False False False False 1077753600 False False 1139249 434808 7009 121 3.0 2001 2838.0 2004 2 9 26 3 57 9.210340 to . items . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } SalesID SalePrice MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter UsageBand fiModelDesc ... saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed auctioneerID_na MachineHoursCurrentMeter_na 0 1139246 11.097410 999089 3157 121 3.0 2004 68.0 2 963 ... 320 1 1 1 1 1 1 2647 1 1 1 1139248 10.950807 117657 77 121 3.0 1996 4640.0 2 1745 ... 86 1 1 1 1 1 1 2148 1 1 2 1139249 9.210340 434808 7009 121 3.0 2001 2838.0 1 336 ... 57 1 1 1 1 1 1 2131 1 1 3 rows \u00d7 67 columns check the vocab with to.classes to . classes [ 'ProductSize' ] (#7) ['#na#','Large','Large / Medium','Medium','Small','Mini','Compact'] # save the tabular object for later # (dest/'to.pkl').save(to)","title":"create a Tabular Object (to)"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#decision-tree-regressor-for-continuous-variables","text":"# had to copy from # https://github.com/anandsaha/fastai.part1.v2/blob/master/fastai/structured.py def draw_tree ( t , df , size = 10 , ratio = 0.6 , precision = 0 ): \"\"\" Draws a representation of a random forest in IPython. Parameters: ----------- t: The tree you wish to draw df: The data used to train the tree. This is used to get the names of the features. \"\"\" s = export_graphviz ( t , out_file = None , feature_names = df . columns , filled = True , special_characters = True , rotate = True , precision = precision ) IPython . display . display ( graphviz . Source ( re . sub ( 'Tree {' , f 'Tree {{ size= { size } ; ratio= { ratio } ' , s ))) xs , y = to . train . xs , to . train . y valid_xs , valid_y = to . valid . xs , to . valid . y m = DecisionTreeRegressor ( max_leaf_nodes = 4 ) m . fit ( xs , y ); draw_tree ( m , xs , precision = 2 ); Tree 0 Coupler_System \u2264 0.5 mse = 0.48 samples = 404710 value = 10.1 1 YearMade \u2264 1991.5 mse = 0.42 samples = 360847 value = 10.21 0->1 True 2 mse = 0.12 samples = 43863 value = 9.21 0->2 False 3 mse = 0.37 samples = 155724 value = 9.97 1->3 4 ProductSize \u2264 4.5 mse = 0.37 samples = 205123 value = 10.4 1->4 5 mse = 0.31 samples = 182403 value = 10.5 4->5 6 mse = 0.17 samples = 22720 value = 9.62 4->6 samp_idx = np . random . permutation ( len ( y ))[: 500 ] dtreeviz ( m , xs . iloc [ samp_idx ], y . iloc [ samp_idx ], xs . columns , dep_var , fontname = 'DejaVu Sans' , scale = 1.6 , label_fontsize = 10 , orientation = 'LR' ) G node4 2020-11-30T06:28:05.016208 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} leaf5 2020-11-30T06:28:05.560116 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf5 leaf6 2020-11-30T06:28:05.654756 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf6 node1 2020-11-30T06:28:05.131089 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->node4 leaf3 2020-11-30T06:28:05.398894 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->leaf3 leaf2 2020-11-30T06:28:05.742191 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0 2020-11-30T06:28:05.254063 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0->node1 < node0->leaf2 \u2265 Using Dtreeviz is a little more intuitive and provides a bit more information. For example, YearMade is showing that there are years equal to 1000, which is obviously an error in the data. Let's fix that xs . loc [ xs [ 'YearMade' ] < 1900 , 'YearMade' ] = 1950 valid_xs . loc [ valid_xs [ 'YearMade' ] < 1900 , 'YearMade' ] = 1950 m = DecisionTreeRegressor ( max_leaf_nodes = 4 ) . fit ( xs , y ) dtreeviz ( m , xs . iloc [ samp_idx ], y . iloc [ samp_idx ], xs . columns , dep_var , fontname = 'DejaVu Sans' , scale = 1.6 , label_fontsize = 10 , orientation = 'LR' ) G node4 2020-11-30T06:28:21.881440 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} leaf5 2020-11-30T06:28:22.494637 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf5 leaf6 2020-11-30T06:28:22.604794 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node4->leaf6 node1 2020-11-30T06:28:22.010995 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->node4 leaf3 2020-11-30T06:28:22.367366 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node1->leaf3 leaf2 2020-11-30T06:28:22.694135 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0 2020-11-30T06:28:22.156796 image/svg+xml Matplotlib v3.3.1, https://matplotlib.org/ *{stroke-linecap:butt;stroke-linejoin:round;} node0->node1 < node0->leaf2 \u2265 # remove max_leaf_nodes # build a bigger tree m = DecisionTreeRegressor () m . fit ( xs , y ) DecisionTreeRegressor() def r_mse ( preds , y ): return round ( math . sqrt ((( preds - y ) ** 2 ) . mean ()), 6 ) def m_rmse ( m , xs , y ): return r_mse ( m . predict ( xs ), y ) m_rmse ( m , xs , y ) 0.0 m_rmse ( m , valid_xs , valid_y ) 0.333069 Our training set is 0 and validation set is worse than the observed value from the original tree viz. The reason is that there are almost as many leaves in our model than observations in our data set. To avoid this, we need to pick some stopping criteria, like some threshold that will tell the model, don't split this if there are less than x number of items in the leaf node. Do this with min_samples_leaf . m . get_n_leaves (), len ( xs ) (324549, 404710) we have nearly as many leaf nodes as observations in our dataset we need to create some rules here m = DecisionTreeRegressor ( min_samples_leaf = 25 ) m . fit ( to . train . xs , to . train . y ) m_rmse ( m , xs , y ), m_rmse ( m , valid_xs , valid_y ) (0.248593, 0.323391) m . get_n_leaves () 12397","title":"Decision Tree Regressor: for continuous variables"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#catagorical-variables","text":"Unlike with collab filtering, we do not need to create dummy variables with categorical values because through pre-processing, we have already transformed these into numerical values. However, you can one-hot encode if you like.","title":"Catagorical Variables"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#bagging","text":"A technique developed by professor Leo Breiman. The idea is that you can bootstrap subsets of your data, train your model, store the predictions, then average the predictions. Steps 1. randomly choose a subset 2. train a model on the subset 3. save the model, return to step one 4. make a prediction using all of the models, then take the average of each model's prediction Leo furthered his thinking by not only selecting a random subset of rows, but also a random subset of columns. This is known as a Random Forrest . The function rf below uses the following arguments - n_estimators defines the number of trees - max_samples defines how many rows to sample for training each tree - max_features defines how many columns to sample at each split point (0.5 means \"take half the total number of columns\"). - min_samples_leaf specify when to stop splitting the tree nodes - effectively limiting the depth of the tree - n_jobs=-1 use CPUs to build the trees in parallel. def rf ( xs , y , n_estimators = 40 , max_samples = 200_000 , max_features = 0.5 , min_samples_leaf = 5 , ** kwargs ): return RandomForestRegressor ( n_jobs =- 1 , n_estimators = n_estimators , max_samples = max_samples , max_features = max_features , min_samples_leaf = min_samples_leaf , oob_score = True ) . fit ( xs , y ) m = rf ( xs , y ) m_rmse ( m , xs , y ), m_rmse ( m , valid_xs , valid_y ) (0.171231, 0.234308) That is much better!","title":"Bagging"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#making-predictions","text":"To understand the impact of n_estimators you can get predictions from each individual tree in the forest preds = np . stack ([ t . predict ( valid_xs ) for t in m . estimators_ ]) preds array([[10.11121098, 9.94458659, 9.42150625, ..., 9.17998473, 9.29954442, 9.29954442], [10.14101274, 9.82777866, 9.55376172, ..., 9.48364408, 9.48364408, 9.48364408], [10.01018006, 10.1378665 , 9.27639723, ..., 9.49919689, 9.18244871, 9.18244871], ..., [ 9.88747565, 9.52539463, 9.46619672, ..., 9.24248886, 9.22252042, 9.22252042], [10.5004158 , 9.98228111, 9.20137348, ..., 9.43222591, 9.30666413, 9.30666413], [10.08093796, 10.61704159, 9.33421822, ..., 9.26213868, 9.29758778, 9.29758778]]) This represents every prediction for each and every tree for every row of data. r_mse ( preds . mean ( axis = 0 ), valid_y ) 0.234308 Here is how to make a single rediction. I think!!","title":"Making Predictions"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#check-this","text":"# slice a row of data valid_xs . iloc [ 0 ] UsageBand 2.0 fiModelDesc 2301.0 fiBaseModel 706.0 fiSecondaryDesc 43.0 fiModelSeries 0.0 ... saleMonth 10.0 saleWeek 40.0 saleDay 3.0 saleDayofweek 0.0 saleDayofyear 276.0 Name: 22915, Length: 66, dtype: float64 # predict requires a 2D array, reshape your data data = valid_xs . iloc [ 0 ] . values . reshape ( 1 , - 1 ) m . predict ( data ) array([10.0005727]) You can visualise how RMSE improvs as more and more trees are added plt . plot ([ r_mse ( preds [: i + 1 ] . mean ( 0 ), valid_y ) for i in range ( 40 )]); validation set is worse than the training set. Why? we might be overfitting the last two weeks of the auction data may have been different somehow","title":"CHECK THIS"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#out-of-bag-error-oob-error","text":"how can we check? we can use OOB predictions from the model and run rmse on the oob error. r_mse ( m . oob_prediction_ , y ) 0.211059 What is happening here? OOB error gives you a sense of how much you are overfitting.","title":"Out of Bag Error (OOB error)"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#model-interpretation","text":"source For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: How confident are we in our predictions using a particular row of data? For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? Which columns are the strongest predictors, which can we ignore? Which columns are effectively redundant with each other, for purposes of prediction? How do predictions vary, as we vary these columns? preds_std = preds . std ( 0 ) preds_std array([0.24516726, 0.17149769, 0.12129906, ..., 0.1911843 , 0.16064838, 0.16064838])","title":"Model interpretation"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#feature-important","text":"def rf_feat_importance ( m , df ): return pd . DataFrame ({ 'cols' : df . columns , 'imp' : m . feature_importances_ } ) . sort_values ( 'imp' , ascending = False ) fi = rf_feat_importance ( m , xs ) fi [: 10 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cols imp 58 YearMade 0.175870 6 ProductSize 0.118317 30 Coupler_System 0.099115 7 fiProductClassDesc 0.071253 31 Grouser_Tracks 0.064081 55 ModelID 0.061831 50 saleElapsed 0.051997 32 Hydraulics_Flow 0.042906 3 fiSecondaryDesc 0.039384 1 fiModelDesc 0.031282 def plot_fi ( fi ): return fi . plot . barh ( 'cols' , 'imp' , figsize = ( 12 , 8 ), legend = False ) plot_fi ( fi [: 30 ]);","title":"Feature Important"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#removing-low-importance-variables","text":"to_keep = fi [ fi . imp > 0.005 ] . cols len ( to_keep ) 20 retrain model using only this subset of columns xs_imp = xs [ to_keep ] valid_xs_imp = valid_xs [ to_keep ] m = rf ( xs_imp , y ) m_rmse ( m , xs_imp , y ), m_rmse ( m , valid_xs , valid_y ) (0.180874, 0.231109) cluster_columns ?? cluster_columns ( xs_imp ) seems like we could remove some of the clustered columns. Let's calculate a baseline using a sample of data def get_oob ( df ): m = RandomForestRegressor ( n_estimators = 40 , min_samples_leaf = 15 , max_samples = 50000 , max_features = 0.5 , n_jobs =- 1 , oob_score = True ) m . fit ( df , y ) return m . oob_score_ get_oob ( xs_imp ) 0.8769414512037411 now remove redundant columns one at a time { c : get_oob ( xs_imp . drop ( c , axis = 1 )) for c in ( 'saleYear' , 'saleElapsed' , 'ProductGroupDesc' , 'ProductGroup' , 'fiModelDesc' , 'fiBaseModel' , 'Hydraulics_Flow' , 'Grouser_Tracks' , 'Coupler_System' )} {'saleYear': 0.8754647378064608, 'saleElapsed': 0.8723764211506366, 'ProductGroupDesc': 0.8765810711892142, 'ProductGroup': 0.8773280451665235, 'fiModelDesc': 0.8758816619476205, 'fiBaseModel': 0.8756967579434771, 'Hydraulics_Flow': 0.8769591574648032, 'Grouser_Tracks': 0.876871969234521, 'Coupler_System': 0.8762546455208067} not much change here so try dropping multiple variables to_drop = [ 'saleYear' , 'ProductGroupDesc' , 'fiBaseModel' , 'Grouser_Tracks' ] get_oob ( xs_imp . drop ( to_drop , axis = 1 )) 0.875680776530026 xs_final = xs_imp . drop ( to_drop , axis = 1 ) valid_xs_final = valid_xs_imp . drop ( to_drop , axis = 1 ) check rmse again to confirm accuracy hasn't really changed m = rf ( xs_final , y ) m_rmse ( m , xs_final , y ), m_rmse ( m , valid_xs_final , valid_y ) (0.182544, 0.23238) similar accuracy but less features!","title":"Removing low-importance variables"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#partial-dependence","text":"what is the relationship between variables p = valid_xs_final [ 'ProductSize' ] . value_counts ( sort = False ) . plot . barh () c = to . classes [ 'ProductSize' ] plt . yticks ( range ( len ( c )), c ); largest group is actual #na# do the same for yearmade ax = valid_xs_final [ 'YearMade' ] . hist () from sklearn.inspection import plot_partial_dependence fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plot_partial_dependence ( m , valid_xs_final , [ 'YearMade' , 'ProductSize' ], grid_resolution = 20 , ax = ax ); what is partial dependence telling us? We want to look at how year affects the sale price, that is, all else being equal, what affect does year have on sale price","title":"Partial Dependence"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#tree-interpreter","text":"#!pip install treeinterpreter #!pip install waterfallcharts import warnings warnings . simplefilter ( 'ignore' , FutureWarning ) from treeinterpreter import treeinterpreter from waterfall_chart import plot as waterfall row = valid_xs_final . iloc [: 5 ] prediction , bias , contributions = treeinterpreter . predict ( m , row . values ) prediction [ 0 ], bias [ 0 ], contributions [ 0 ] . sum () (array([10.03875756]), 10.104200155980113, -0.06544259554720655) waterfall ( valid_xs_final . columns , contributions [ 0 ], threshold = 0.08 , rotation_value = 45 , formatting = ' {:,.3f} ' );","title":"Tree Interpreter"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#the-extrapolation-problem","text":"np . random . seed ( 42 ) x_lin = torch . linspace ( 0 , 20 , steps = 40 ) y_lin = x_lin + torch . randn_like ( x_lin ) plt . scatter ( x_lin , y_lin ); xs_lin = x_lin . unsqueeze ( 1 ) x_lin . shape , xs_lin . shape (torch.Size([40]), torch.Size([40, 1])) you can do the same using None x_lin [:, None ] . shape torch.Size([40, 1]) m_lin = RandomForestRegressor () . fit ( x_lin [: 30 ] . reshape ( - 1 , 1 ), y_lin [: 30 ]) plt . scatter ( x_lin , y_lin , 20 ) plt . scatter ( x_lin , m_lin . predict ( xs_lin ), color = 'red' , alpha = 0.5 ); random forrext cannot extrapolate outside of the bounds of the training data we need to make sure validation set does not contain out of domain data test and training set may vary, how do we tell??","title":"The extrapolation problem"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#finding-out-of-domain-data","text":"df_dom = pd . concat ([ xs_final , valid_xs_final ]) is_valid = np . array ([ 0 ] * len ( xs_final ) + [ 1 ] * len ( valid_xs_final )) m = rf ( df_dom , is_valid ) rf_feat_importance ( m , df_dom )[: 6 ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } cols imp 5 saleElapsed 0.915808 10 SalesID 0.069088 13 MachineID 0.011871 0 YearMade 0.000678 4 ModelID 0.000536 12 Hydraulics 0.000529 m = rf ( xs_final , y ) print ( 'orig' , m_rmse ( m , valid_xs_final , valid_y )) for c in ( 'SalesID' , 'saleElapsed' , 'MachineID' ): m = rf ( xs_final . drop ( c , axis = 1 ), y ) print ( c , m_rmse ( m , valid_xs_final . drop ( c , axis = 1 ), valid_y )) orig 0.233484 SalesID 0.231357 saleElapsed 0.236643 MachineID 0.231104 time_vars = [ 'SalesID' , 'MachineID' ] xs_final_time = xs_final . drop ( time_vars , axis = 1 ) valid_xs_time = valid_xs_final . drop ( time_vars , axis = 1 ) m = rf ( xs_final_time , y ) m_rmse ( m , valid_xs_time , valid_y ) 0.229127 xs [ 'saleYear' ] . hist (); filt = xs [ 'saleYear' ] > 2004 xs_filt = xs_final_time [ filt ] y_filt = y [ filt ] m = rf ( xs_filt , y_filt ) m_rmse ( m , xs_filt , y_filt ), m_rmse ( m , valid_xs_time , valid_y ) (0.176904, 0.22864)","title":"Finding out of domain data"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#using-a-neural-net","text":"# load data df_nn = pd . read_csv ( dest / 'TrainAndValid.csv' , low_memory = False ) # set ProductSize as categorical df_nn [ 'ProductSize' ] = df_nn [ 'ProductSize' ] . astype ( 'category' ) df_nn [ 'ProductSize' ] . cat . set_categories ( sizes , ordered = True , inplace = True ) # take log of dependent variable df_nn [ dep_var ] = np . log ( df_nn [ dep_var ]) # do some date prep df_nn = add_datepart ( df_nn , 'saledate' ) df_nn_final = df_nn [ list ( xs_final_time . columns ) + [ dep_var ]] cont_nn , cat_nn = cont_cat_split ( df_nn_final , max_card = 9000 , dep_var = dep_var ) cont_nn . append ( 'saleElapsed' ) cat_nn . remove ( 'saleElapsed' ) df_nn [ 'saleElapsed' ] = df_nn [ 'saleElapsed' ] . astype ( int ) df_nn_final [ cat_nn ] . nunique () YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 Hydraulics_Flow 3 fiSecondaryDesc 177 fiModelDesc 5059 Enclosure 6 ProductGroup 6 Hydraulics 12 fiModelDescriptor 140 Drive_System 4 dtype: int64 xs_filt2 = xs_filt . drop ( 'fiModelDescriptor' , axis = 1 ) valid_xs_time2 = valid_xs_time . drop ( 'fiModelDescriptor' , axis = 1 ) m2 = rf ( xs_filt2 , y_filt ) m_rmse ( m2 , xs_filt2 , y_filt ), m_rmse ( m2 , valid_xs_time2 , valid_y ) (0.178922, 0.230357) cat_nn . remove ( 'fiModelDescriptor' ) df_nn_final [ 'saleElapsed' ] . astype ( 'int64' , copy = False ) df_nn_final . dtypes YearMade int64 ProductSize category Coupler_System object fiProductClassDesc object ModelID int64 saleElapsed int64 Hydraulics_Flow object fiSecondaryDesc object fiModelDesc object Enclosure object ProductGroup object Hydraulics object fiModelDescriptor object Drive_System object SalePrice float64 dtype: object","title":"Using a Neural Net"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#normalize","text":"Normalize subtracts the mean, then divides by the standard deviation. We didn't need this for a decision tree because we were only performing binary splits. However, we do need to normalize for neaural nets because we don't want things with crazy distributions. procs_nn = [ Categorify , FillMissing , Normalize ] to_nn = TabularPandas ( df = df_nn_final , procs = procs_nn , cat_names = cat_nn , cont_names = cont_nn , splits = splits , y_names = dep_var ) dls = to_nn . dataloaders ( 1024 ) This is a regression model so we want to set our y_range based on the min and max of the dependent variable. y = to_nn . train . y y . min (), y . max () (8.465899, 11.863583) learn = tabular_learner ( dls , y_range = ( 8 , 12 ), layers = [ 500 , 250 ], n_out = 1 , loss_func = F . mse_loss ) learn . lr_find () SuggestedLRs(lr_min=0.003981071710586548, lr_steep=0.00019054606673307717) learn . fit_one_cycle ( 5 , 1e-2 ) epoch train_loss valid_loss time 0 0.069223 0.062953 00:11 1 0.056285 0.055872 00:13 2 0.048484 0.055052 00:12 3 0.043525 0.051425 00:12 4 0.040454 0.051055 00:11 preds , targs = learn . get_preds () r_mse ( preds , targs ) 0.225954","title":"Normalize"},{"location":"fastai%20deep%20learning%202020/lesson%2007/#summary","text":"Random Forests are easy to train, resillient, don't require much pre-processing, train quickly and don't overfit. They can be less accurate than a neural net and can take longer at inference time to evaluate the trees. Neural Nets are probably the fiddliest models to implement and set up but can give slightly better results.","title":"Summary"},{"location":"fastai%20deep%20learning%202020/lesson%2008/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Lesson 8: Deep Learning for Coders NLP This notebook will take a dive into Natural Language Processing and will attempt to train an NLP classifyer. This is a binary classification task using movie review sentiment. The pretrained model In lesson 1, we acheived over 90% accuracy because we were using a pre-trained model that we fine-tuned further. So what is a pre-trained language model? A language model is one where we try to predict the next word in a sentence. For lesson one, this was a neaural net pre-trained on wiki articles (Wikitext 103). How does this help with sentiment anlysis? Like pre-trained image models, language models too contain a lot of information that can be leveraged rather than training from scratch. Fine-tuning will throw away the last layer(s) and train these rather than the entire model. Through transfer learning, we will create an Imdb language model using the wikitext model as a base. Text preprocessing Tokenization : Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model) Numericalization : Make a list of all of the unique words that appear (the vocab), convert each word into a number, by looking up its index in the vocab. Language model data loader creation : fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required Language model creation : We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). source Tokenisation There are different approaches to tokenisation these are... - Word based: which splits a sentence on spaces - Subword based: splits words into smaller parts based on the most commonly occuring substrings - Character bases: splits a sentence into individual characters Word tokenisation with fastai there are a number of tokenisers out there, fastai makes it easy to switch between them. currently fastai default is from the spaCy library Data The IMDB Large dataset contains 25,000 highly polar movie reviews for training, and 25,000 for testing. It is very large! from fastai.text.all import * path = untar_data ( URLs . IMDB ) get_text_files gets all the text files in a path. We can also optionally pass folders to restrict the search to a particular list of subfolders: # only using 50k sample due to size of dataset # results may vary from fastai book files = get_text_files ( path , folders = [ 'train' , 'test' , 'unsup' ])[: 50000 ] # print out slice of first review txt = files [ 0 ] . open () . read () txt [: 75 ] \"The worst movie I've ever seen, hands down. It is ten times more a rip-off \" first() - First element of x , or None if missing coll_repr - String repr of up to max_n items of (possibly lazy) collection c spacy = WordTokenizer () toks = first ( spacy ([ txt ])) print ( coll_repr ( toks , 30 )) (#156) ['The','worst','movie','I',\"'ve\",'ever','seen',',','hands','down','.','It','is','ten','times','more','a','rip','-','off','of','Lake','Placid','than','it','is','a','sequel','.','Director'...] fastai provides additional functionality to tokenisers, such as adding in special tokens like begining of string xxbos or lowercasing all strings and adding the xxmaj token before. This is done to preserve importance and reduce some complexity. tkn = Tokenizer ( spacy ) print ( coll_repr ( tkn ( txt ), 31 )) (#176) ['xxbos','xxmaj','the','worst','movie','xxmaj','i',\"'ve\",'ever','seen',',','hands','down','.','xxmaj','it','is','ten','times','more','a','rip','-','off','of','xxmaj','lake','xxmaj','placid','than','it'...] You can explore the rules like so defaults . text_proc_rules [<function fastai.text.core.fix_html(x)>, <function fastai.text.core.replace_rep(t)>, <function fastai.text.core.replace_wrep(t)>, <function fastai.text.core.spec_add_spaces(t)>, <function fastai.text.core.rm_useless_spaces(t)>, <function fastai.text.core.replace_all_caps(t)>, <function fastai.text.core.replace_maj(t)>, <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>] then check the source code for each using ?? like fix_html?? Subword Tokenisation Word tokenisation relies on spaces within the document. Subword tokenisation does two things 1. analyses a corpus of documents to find the most commonly occurring groups of letters. These then become the vocab 2. Tokenise the corpus using this vocab of subword units txts = L ( o . open () . read () for o in files [: 2000 ]) We instantiate our tokeniser, by defining the size of the vocab, then training it. meaing, have the tokeniser read the documents, find the common sequences of characters then create the vocab. in fastai, this is done with setup . def subword ( sz ): sp = SubwordTokenizer ( vocab_sz = sz ) sp . setup ( txts ) return ' ' . join ( first ( sp ([ txt ]))[: 40 ]) subword ( 1000 ) \"\u2581The \u2581worst \u2581movie \u2581I ' ve \u2581ever \u2581seen , \u2581hand s \u2581down . \u2581It \u2581is \u2581t en \u2581time s \u2581more \u2581a \u2581 r i p - off \u2581of \u2581L ake \u2581P la ci d \u2581than \u2581it \u2581is \u2581a \u2581sequel .\" the special character \u2581 represents a space character in the original text. using a smaller vocab results in each token representing fewer characters, and will need more tokens to represent a sentence subword ( 200 ) \"\u2581The \u2581w or s t \u2581movie \u2581I ' ve \u2581 e ver \u2581s e en , \u2581 h an d s \u2581d o w n . \u2581I t \u2581is \u2581 t en \u2581 t i m es \u2581mo re \u2581a\" Using larger vocab will result in most common English words ending up in the vocab, and fewer tokens will be needed to represent a sentence subword ( 10000 ) \"\u2581The \u2581worst \u2581movie \u2581I ' ve \u2581ever \u2581seen , \u2581hands \u2581down . \u2581It \u2581is \u2581ten \u2581times \u2581more \u2581a \u2581rip - off \u2581of \u2581Lake \u2581Placid \u2581than \u2581it \u2581is \u2581a \u2581sequel . \u2581Director \u2581David \u2581F lo re s \u2581clearly \u2581did \u2581not \u2581go\" There are trade-off to be made here: larger vocab means fewer tokens per sentence leading to faster training and less memory and state required for the model. The downside is larger embedding matrices which require more data to learn. Subword tokenisation provides an easy way to scale between character and word tokenisation while also being useful for applications involving languages other than english. Numericalisation This is the process of mapping tokens to integers. It is nearly identical to the steps necessary to create a Category variable Make a list of all possible levels of that categorical variable (vocab) replace each level with it's index in the vocab toks = tkn ( txt ) print ( coll_repr ( tkn ( txt ), 32 )) (#176) ['xxbos','xxmaj','the','worst','movie','xxmaj','i',\"'ve\",'ever','seen',',','hands','down','.','xxmaj','it','is','ten','times','more','a','rip','-','off','of','xxmaj','lake','xxmaj','placid','than','it','is'...] # a small example toks200 = txts [: 200 ] . map ( tkn ) toks200 [ 0 ] (#176) ['xxbos','xxmaj','the','worst','movie','xxmaj','i',\"'ve\",'ever','seen'...] num = Numericalize () num . setup ( toks200 ) coll_repr ( num . vocab , 20 ) \"(#2144) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\" this is our vocab, starting with special tokens, then english words in order of highest frequency. we can now use the Numericalize object as a function and apply it to our tokens to see the integers they now represent nums = num ( toks )[: 20 ] nums tensor([ 2, 8, 9, 310, 27, 8, 19, 218, 158, 141, 10, 0, 229, 11, 8, 18, 16, 550, 299, 66]) Create batches for language model Batches are split based on the sequence length and batch size. Batches are created by concatenating individual texts into a stream. Order of inputs are randomised, meaning the order of the documents (not order of words in these) are shuffled. The stream is then divided into batches. This is done at every epoch - shuffle the collection of documents - concatenate them together into a stream of tokens - cut the stream into batches of fixed size consecutive mini streams This is all done in fastai using LMDataLoader . For example nums200 = toks200 . map ( num ) dl = LMDataLoader ( nums200 ) x , y = first ( dl ) x . shape , y . shape (torch.Size([64, 72]), torch.Size([64, 72])) batch size = 64 stream length = 72 Looking at the first row of the independent variable should contain the start of the text ' ' . join ( num . vocab [ o ] for o in x [ 0 ][: 20 ]) \"xxbos xxmaj the worst movie xxmaj i 've ever seen , xxunk down . xxmaj it is ten times more\" the dependent variable will be the same but offset by one token ' ' . join ( num . vocab [ o ] for o in y [ 0 ][: 20 ]) \"xxmaj the worst movie xxmaj i 've ever seen , xxunk down . xxmaj it is ten times more a\" Pt 1: Training a Text Classifier using fastai The reason that TextBlock is special is because setting up the numericalizer's vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible the TextBlock performs a few optimizations: It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs We need to tell TextBlock how to access the texts, so that it can do this initial preprocessing\u2014that's what from_folder does. show_batch then works in the usual way source get_imdb = partial ( get_text_files , folders = [ 'train' , 'test' , 'unsup' ]) dls_lm = DataBlock ( blocks = TextBlock . from_folder ( path , is_lm = True ), get_items = get_imdb , splitter = RandomSplitter ( 0.1 ) ) . dataloaders ( path , path = path , bs = 128 , seq_len = 80 ) dls_lm . show_batch ( max_n = 2 ) text text_ 0 xxbos xxmaj my sincere advice to all : do n't watch the movie . \\n\\n xxmaj do n't even go near to the theater where this movie is being played ! ! even a glimpse of it is bad for health . serious . no jokes . it 's xxunk am in the morning . and i returned from this crappiest movie on this universe . xxup four xxup hours xxup damn xxrep 3 ! i am proud that i xxmaj my sincere advice to all : do n't watch the movie . \\n\\n xxmaj do n't even go near to the theater where this movie is being played ! ! even a glimpse of it is bad for health . serious . no jokes . it 's xxunk am in the morning . and i returned from this crappiest movie on this universe . xxup four xxup hours xxup damn xxrep 3 ! i am proud that i survived 1 what has led to the overwhelmingly negative reaction . \\n\\n xxmaj the shock value is the least appealing thing about this film - a minor detail that has been blown out of proportion . xxmaj the story is of xxmaj pierre 's downfall - and the subsequent destruction of those around him - which is overtly demonstrated in his features , demeanour and xxunk . xxmaj the dialogue and soundtrack set this film apart from any other i have seen has led to the overwhelmingly negative reaction . \\n\\n xxmaj the shock value is the least appealing thing about this film - a minor detail that has been blown out of proportion . xxmaj the story is of xxmaj pierre 's downfall - and the subsequent destruction of those around him - which is overtly demonstrated in his features , demeanour and xxunk . xxmaj the dialogue and soundtrack set this film apart from any other i have seen , Fine tuning To convert the integer word indices into activations for the neural net, we will use embeddings. These are then fed into the RNN using an architecture called AWD_LSTM cross entropy loss is sutable here since this is a classification problem. Often a metric called perplexity is used in NLP, this is the exponential of the loss ( torch.exp(cross_entropy) ). To this we will also add accuracy to determine how the model performs when trying to predict the next word. to_fp16 uses less GPU memory and trains faster learn = language_model_learner ( dls_lm , AWD_LSTM , drop_mult = 0.3 , metrics = [ accuracy , Perplexity ()]) . to_fp16 () learn . fit_one_cycle ( 1 , 2e-2 ) epoch train_loss valid_loss accuracy perplexity time 0 4.129613 3.911054 0.299887 49.951557 34:09 Saving and Loading models learn . save ( '1epoch' ) learn = learn . load ( '1epoch' ) learn . unfreeze () learn . fit_one_cycle ( 5 , 2e-3 ) epoch train_loss valid_loss accuracy perplexity time 0 3.870227 3.766035 0.318266 43.208385 35:48 1 3.772985 3.673187 0.329042 39.377213 35:38 2 3.677068 3.615694 0.335646 37.177132 35:36 3 3.570553 3.582507 0.339907 35.963577 35:56 4 3.526067 3.577981 0.340754 35.801178 35:58 IOPub message rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_msg_rate_limit`. Current values: NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec) NotebookApp.rate_limit_window=3.0 (secs) IOPub message rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_msg_rate_limit`. Current values: NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec) NotebookApp.rate_limit_window=3.0 (secs) Pt 2: A Language Model from Scratch Data The Human Numbers data set contains the first 10,000 numbers written in english. It was created by Jeremy for experimentation. from fastai.text.all import * path = untar_data ( URLs . HUMAN_NUMBERS ) path . ls () (#2) [Path('/storage/data/human_numbers/valid.txt'),Path('/storage/data/human_numbers/train.txt')] Lake a look at some of the data lines = L () with open ( path / 'train.txt' ) as f : lines += L ( * f . readlines ()) with open ( path / 'valid.txt' ) as f : lines += L ( * f . readlines ()) lines (#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...] concat all into one big stream, with \".\" to separate text = ' . ' . join ([ l . strip () for l in lines ]) text [: 100 ] 'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo' use work tokenisation by splitting on spaces tokens = L ( text . split ( ' ' )) tokens [ 100 : 110 ] (#10) ['.','forty','two','.','forty','three','.','forty','four','.'] for numericalisation, we need to create a list of all unique words. we can then convert these into numbers vocab = L ( tokens ) . unique () vocab (#30) ['one','.','two','three','four','five','six','seven','eight','nine'...] word2idx = { w : i for i , w in enumerate ( vocab )} nums = L ( word2idx [ i ] for i in tokens ) tokens , nums ((#63095) ['one','.','two','.','three','.','four','.','five','.'...], (#63095) [0,1,2,1,3,1,4,1,5,1...]) We now have a small dataset that we can use for language modelling. Creating a Language Model For this simple example, we will predict the next word based on the previous 3 words. To do this, create a list with the independent variable being the first 3 words, and dependent variable being the 4th word. L (( tokens [ i : i + 3 ], tokens [ i + 3 ]) for i in range ( 0 , len ( tokens ) - 4 , 3 ))[ 0 ] ((#3) ['one','.','two'], '.') We can see from looking at the first items that ['one','.','two'] are the independent variable and '.' is the dependent variable. What the model will actually use are tensors of the numericalised values. seqs = L (( tensor ( nums [ i : i + 3 ]), nums [ i + 3 ]) for i in range ( 0 , len ( nums ) - 4 , 3 )) seqs (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] Create a DataLoader batch size of 64 split randomly, taking 80% bs = 64 cut = int ( len ( seqs ) * 0.8 ) dls = DataLoaders . from_dsets ( seqs [: cut ], seqs [ cut :], bs = 64 , shuffle = False ) The model in PyTorch A simple linear model has an input of size (batch size x #inputs), followed by a single hidden layer that computes a matrix product followed by ReLU; Out of which we will get some activations, the size of which will be (batch size x #activations). This is then followed by more computation, a matrix product followed by softmax. The final output size will be (batch size x #classes). We will take this approach and modify it for our model. Our model will be a Neural Net with 3 layers - The embedding layer (input to hidden i_h ) - The Linear Layer (hidden to hidden h_h ) - this layer created the activations for the next word - this layer will be used for words 1-3 - Final Linear layer to predict the fourth word (hidden to output layer h_o ) In the diagram below, the arrows represent the computational steps (a linear layer followed by non-linearity (ReLU)) To start, take the word 1 input and put it through the linear layer and ReLU to get first set of activations. Then put that through another linear layer and non-linearity. These activations are added (or concatenated would be fine) to the resulting activations of word 2 which is also run through a linear layer and non-linearity. Again the results are run through another linear layer and non-linearity while also adding in the result of putting word 3 through a computation layer as we did with word 2. These activations then go through a final linear layer and softmax to create the output activations. What is interesting about this model is that inputs are entering in later layer and added into the network. Also, arrows of the same colour mean that the same weight matrix is being used. In code we can represent this like so... To go from the input to hidden layer we use an embedding. We create one embedding which subsequent words will also go through, and each time we add this to the current set of activations. Why use the same embedding layer?? Conceptually, the words all represent english spellings of numbers, so they have the same meaning and therefore wouldn't need separate embeddings. Once we have the embedding, we send this through the linear layer, then through relu. As with embeddings, we can use the same Linear layer because we are doing the same kind of computation. The computation happens from the inner most brackets out so this... F.relu(self.h_h(self.i_h(x[:,0]))) - starts with sending word 1 x[:,0] through the embedding layer self.i_h(x[:,0]) - then through a Linear layer self.h_h(self.i_h(x[:,0])) - and finally through the relu F.relu(self.h_h(self.i_h(x[:,0]))) class LMModel1 ( Module ): # vocab_sz == vocab size def __init__ ( self , vocab_sz , n_hidden ): # the embedding layer self . i_h = nn . Embedding ( vocab_sz , n_hidden ) # the linear layer self . h_h = nn . Linear ( n_hidden , n_hidden ) # final linear layer self . h_o = nn . Linear ( n_hidden , vocab_sz ) def forward ( self , x ): # h is the hidden state # word 1 to embedding h = F . relu ( self . h_h ( self . i_h ( x [:, 0 ]))) # word 2 to same embedding h = h + self . i_h ( x :, 1 ) h = F . relu ( self . h_h ( h )) h = h + self . i_h ( x [:, 2 ]) # word 3 to same embedding h = F . relu ( self . h_h ( h )) # hidden to output return self . h_o ( h ) the activations in the model are known as the \"hidden state\" class LMModel1 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) def forward ( self , x ): h = F . relu ( self . h_h ( self . i_h ( x [:, 0 ]))) h = h + self . i_h ( x [:, 1 ]) h = F . relu ( self . h_h ( h )) h = h + self . i_h ( x [:, 2 ]) h = F . relu ( self . h_h ( h )) return self . h_o ( h ) learn = Learner ( dls , LMModel1 ( len ( vocab ), 64 ), loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 4 , 1e-3 ) epoch train_loss valid_loss accuracy time 0 1.863818 2.031583 0.464939 00:02 1 1.392999 1.803210 0.467079 00:02 2 1.410863 1.698382 0.490849 00:02 3 1.371146 1.703473 0.411457 00:02 So far our accuracy is just under 50%. Not bad. We can improve by first refactoring... LMModel1 has a few repeated steps, we can remove this by adding in a for loop. Our first Recurrent Neural Net class LMModel2 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) def forward ( self , x ): # initialise h as 0. # this gets braodcast to a tensor in the loop h = 0. for i in range ( 3 ): h = h + self . i_h ( x [:, i ]) h = F . relu ( self . h_h ( h )) return self . h_o ( h ) # check we get the same results learn = Learner ( dls , LMModel2 ( len ( vocab ), 64 ), loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 4 , 1e-3 ) epoch train_loss valid_loss accuracy time 0 1.877445 2.006204 0.479914 00:02 1 1.398311 1.774232 0.482054 00:03 2 1.421882 1.650312 0.492988 00:03 3 1.372779 1.634449 0.484906 00:02 We have actually just created a Recurrent Nuearal Net. Reminder - Hidden State represents the activations that are occurring inside the neural net. Maintaining the Hidden State we can do this by storing the hidden state and updating it. detach throws away the gradient history, also known as truncated back propagation . look into this!! class LMModel3 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = 0. def forward ( self , x ): for i in range ( 3 ): self . h = self . h + self . i_h ( x [:, i ]) self . h = F . relu ( self . h_h ( self . h )) out = self . h_o ( self . h ) self . h = self . h . detach () return out def reset ( self ): self . h = 0. m = len ( seqs ) // bs m , bs , len ( seqs ) (328, 64, 21031) def group_chunks ( ds , bds ): m = len ( ds ) // bs new_ds = L () for i in range ( m ): new_ds += L ( ds [ i + m * j ] for j in range ( bs )) return new_ds cut = int ( len ( seqs ) * 0.8 ) dls = DataLoaders . from_dsets ( group_chunks ( seqs [: cut ], bs ), group_chunks ( seqs [ cut :], bs ), bs = bs , drop_last = True , shuffle = False ) Callbacks ModelResetter is a fastai callback that resets the model at each training/validation step. learn = Learner ( dls , LMModel3 ( len ( vocab ), 64 ), loss_func = F . cross_entropy , metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 10 , 3e-3 ) epoch train_loss valid_loss accuracy time 0 1.720635 1.881263 0.397837 00:03 1 1.319839 1.718479 0.460337 00:03 2 1.100321 1.620753 0.508413 00:03 3 1.016059 1.486176 0.543750 00:02 4 0.995253 1.397851 0.554567 00:03 5 0.965056 1.494172 0.529567 00:03 6 0.928281 1.383823 0.590625 00:03 7 0.841828 1.437241 0.601442 00:03 8 0.796445 1.491238 0.609615 00:03 9 0.787689 1.509905 0.606731 00:03 This RNN keeps the state from batch to batch and the results show the uplift from this change. By only predicting every 4th word, we are throwing away signal, which seems wastful. By moving the output stage inside the loop (ie after every hidden state was created we make a prediction) it means we can predict the next word after every single word, rather than every 3 words. To do this we have to change our data so that the dependent variable has each of the three next words after each of out three input words. sl = 16 # sequence length seqs = L (( tensor ( nums [ i : i + sl ]), tensor ( nums [ i + 1 : i + sl + 1 ])) for i in range ( 0 , len ( nums ) - sl - 1 , sl )) cut = int ( len ( seqs ) * 0.8 ) dls = DataLoaders . from_dsets ( group_chunks ( seqs [: cut ], bs ), group_chunks ( seqs [ cut :], bs ), bs = bs , drop_last = True , shuffle = False ) We can see from the first two items in seqs that they are the same length but the second list is offset by 1 [ L ( vocab [ o ] for o in s ) for s in seqs [ 0 ]] [(#16) ['one','.','two','.','three','.','four','.','five','.'...], (#16) ['.','two','.','three','.','four','.','five','.','six'...]] update the model by creating a list to store outputs, then append to this after every element in the loop # Modify the model to output a prediction after every word class LMModel4 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = 0 def forward ( self , x ): outs = [] for i in range ( sl ): self . h = self . h + self . i_h ( x [:, i ]) self . h = F . relu ( self . h_h ( self . h )) outs . append ( self . h_o ( self . h )) self . h = self . h . detach () return torch . stack ( outs , dim = 1 ) def reset ( self ): self . h = 0 # flatten targets to fit loss function def loss_func ( inp , targ ): return F . cross_entropy ( inp . view ( - 1 , len ( vocab )), targ . view ( - 1 )) learn = Learner ( dls , LMModel4 ( len ( vocab ), 64 ), loss_func = loss_func , metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 15 , 3e-3 ) epoch train_loss valid_loss accuracy time 0 3.212160 3.065020 0.248291 00:01 1 2.303481 1.984242 0.440511 00:01 2 1.722320 1.854335 0.428141 00:01 3 1.448855 1.685089 0.516846 00:01 4 1.267960 1.905218 0.549316 00:01 5 1.135633 1.983428 0.591064 00:01 6 1.026269 2.132295 0.593994 00:01 7 0.943246 2.123302 0.622966 00:01 8 0.850973 2.263324 0.638346 00:01 9 0.784856 2.315861 0.662028 00:01 10 0.729662 2.344142 0.649821 00:01 11 0.689929 2.379879 0.648519 00:01 12 0.656299 2.397321 0.655355 00:01 13 0.634652 2.373445 0.661947 00:01 14 0.623158 2.386648 0.659424 00:01 Multilayer RNN Our model is deep but every hidden to hidden layer uses the same weight matrix which means it isn't that deep at all. It is using the same weight matrix every time, so not very sophisticated. Let's refactor again to pass the activations of our current net into a second recurrent neaural network. This is called a stacked or multilayered RNN. Using PyTorch's nn.RNN module lets us define the number of layers ( n_layers ). We can also remove the loop and just call self.rnn class LMModel5 ( Module ): def __init__ ( self , vocab_sz , n_hidden , n_layers ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . rnn = nn . RNN ( n_hidden , n_hidden , n_layers , batch_first = True ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = torch . zeros ( n_layers , bs , n_hidden ) def forward ( self , x ): res , h = self . rnn ( self . i_h ( x ), self . h ) self . h = h . detach () return self . h_o ( res ) def reset ( self ): self . h . zero_ () # using 2 layers learn = Learner ( dls , LMModel5 ( len ( vocab ), 64 , 2 ), loss_func = loss_func , metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 15 , 3e-3 ) epoch train_loss valid_loss accuracy time 0 3.054249 2.617002 0.445882 00:01 1 2.159026 1.819027 0.470703 00:01 2 1.711899 1.820897 0.402262 00:01 3 1.492573 1.740131 0.468669 00:01 4 1.336913 1.825282 0.494303 00:01 5 1.205158 1.927248 0.513591 00:01 6 1.079775 1.974853 0.543864 00:01 7 0.975060 2.035518 0.549235 00:01 8 0.899264 2.100957 0.536458 00:01 9 0.847659 2.070400 0.546956 00:01 10 0.796934 2.078454 0.546875 00:01 11 0.756011 2.080719 0.546956 00:01 12 0.725375 2.105812 0.549886 00:01 13 0.704403 2.089411 0.549723 00:01 14 0.693146 2.079454 0.550700 00:01 Our results are worse! Why? Deep models are hard to train. This can be due to exploding or disappearing activiations. This basically means that our results either become very very large or very very small. This causes an explosion or vanishing of a number and can be computationally intensive or the accuracy of the floating point numbers gets lost. We can avoid this in a number of ways... LSTM Replacing the matrix multiplication in an RNN with this architecture, basically means the model is able to make decisions about how much of an update to do each time. This helps the model to avoid updating too much or too little. Training a Language Model Using LSTMs This is the same network but the RNN is replaced with an LSTM. We need to increase the number of layers in our hidden state for this to work because the LSTM has more layers. # Training with LSTM class LMModel6 ( Module ): def __init__ ( self , vocab_sz , n_hidden , n_layers ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . rnn = nn . LSTM ( n_hidden , n_hidden , n_layers , batch_first = True ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = [ torch . zeros ( n_layers , bs , n_hidden ) for _ in range ( 2 )] def forward ( self , x ): res , h = self . rnn ( self . i_h ( x ), self . h ) self . h = [ h_ . detach () for h_ in h ] return self . h_o ( res ) def reset ( self ): for h in self . h : h . zero_ () learn = Learner ( dls , LMModel6 ( len ( vocab ), 64 , 2 ), loss_func = CrossEntropyLossFlat (), metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 15 , 1e-2 ) epoch train_loss valid_loss accuracy time 0 3.040422 2.731378 0.391113 00:02 1 2.212882 1.787726 0.447917 00:02 2 1.632609 1.882631 0.484049 00:02 3 1.323149 2.013697 0.508057 00:02 4 1.091380 2.002088 0.518880 00:02 5 0.832751 1.733890 0.622233 00:02 6 0.620027 1.593947 0.696370 00:03 7 0.412642 1.505920 0.715495 00:04 8 0.257724 1.480889 0.761475 00:04 9 0.157874 1.392939 0.771973 00:04 10 0.097578 1.393537 0.774984 00:02 11 0.064152 1.374384 0.778158 00:02 12 0.046606 1.387635 0.785889 00:02 13 0.037822 1.404215 0.781982 00:02 14 0.033807 1.398724 0.782389 00:03 Results are much much better! Regularising an LSTM Dropout Dropout improves neaural net training by deleting random activations. This reduces the computation but also prevents the model from overfitting. Dropout helps the model to generalise by ensuring certain activations don't over specialise during the learning process class Droput - p the probability that an activation gets deleted - only perform dropout in training - mask the mask a tensor with random zeros with probability ( p ) and ones with probability ( p -1) class Dropout ( Module ): def __init__ ( self , p ): self . p = p def forward ( self , x ): if not self . training : return x mask = x . new ( * x . shape ) . bernoulli ( 1 - p ) return x * mask . div_ ( 1 - p ) A simple example p = .3 B = torch . ones (( 3 , 3 )) . bernoulli ( 1 - p ) B tensor([[0., 1., 1.], [1., 0., 0.], [0., 0., 0.]]) In this example, 1- p adds 3 ones in the 3*3 matrix. Basically the probability of drawing a one here is 3/9 or 0.3. As we saw earlier with one hot encodings, this matrix will act as a lookup when you multiply it by another matrix. In context of what we are doing, by performing this multiplication you are randomly prunning elements of the other matrix A = tensor ([[ 1. , 2. , 3. ], [ 4. , 5. , 6. ], [ 7. , 8. , 9. ]]) A * B tensor([[0., 2., 3.], [4., 0., 0.], [0., 0., 0.]]) Corresponding elements of A are returned only if there is a 1 in the same position in matrix B AR and TAR regularisation AR (activation regularisation) and TAR (temporal activation regularisation) are very similar to weight decay but are applied to activations instead of weights. TAR is linked to the fact that we are trying to predict a sequence of tokens. So we take the difference of the activations between time steps. It limits the changes in activations between time steps. Weight Tying Sets the hidden to output weights equal to the input to hidden weights. The idea is that converting words to activations and activations to words should conceptually be the same thing since the language is consistent, and the computation is consistent so why would you need to change the weights? class LMModel7 ( Module ): def __init__ ( self , vocab_sz , n_hidden , n_layers , p ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . rnn = nn . LSTM ( n_hidden , n_hidden , n_layers , batch_first = True ) self . drop = nn . Dropout ( p ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h_o . weight = self . i_h . weight self . h = [ torch . zeros ( n_layers , bs , n_hidden ) for _ in range ( 2 )] def forward ( self , x ): raw , h = self . rnn ( self . i_h ( x ), self . h ) out = self . drop ( raw ) self . h = [ h_ . detach () for h_ in h ] return self . h_o ( out ), raw , out def reset ( self ): for h in self . h : h . zero_ () learn = Learner ( dls , LMModel7 ( len ( vocab ), 64 , 2 , 0.5 ), loss_func = CrossEntropyLossFlat (), metrics = accuracy , cbs = [ ModelResetter , RNNRegularizer ( alpha = 2 , beta = 1 )]) This is the same as above but TextLearner adds the additions peices for you learn = TextLearner ( dls , LMModel7 ( len ( vocab ), 64 , 2 , 0.4 ), loss_func = CrossEntropyLossFlat (), metrics = accuracy ) learn . fit_one_cycle ( 15 , 1e-2 , wd = 0.1 ) epoch train_loss valid_loss accuracy time 0 0.030605 1.401770 0.780680 00:02 1 0.029504 1.535201 0.766602 00:03 2 0.045721 1.465324 0.771484 00:03 3 0.057497 1.550894 0.807780 00:02 4 0.043013 1.394347 0.807292 00:02 5 0.029584 1.430816 0.807536 00:02 6 0.025191 1.391779 0.826009 00:02 7 0.021182 1.496358 0.825439 00:03 8 0.016158 1.389334 0.817139 00:02 9 0.014285 1.503886 0.828369 00:02 10 0.011608 1.421619 0.823079 00:02 11 0.009167 1.429033 0.825521 00:02 12 0.007534 1.449290 0.824382 00:02 13 0.006630 1.455278 0.824300 00:02 14 0.006208 1.456966 0.824056 00:02 Almost 85% accuracy!","title":"Lesson 08"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#lesson-8-deep-learning-for-coders","text":"","title":"Lesson 8: Deep Learning for Coders"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#nlp","text":"This notebook will take a dive into Natural Language Processing and will attempt to train an NLP classifyer. This is a binary classification task using movie review sentiment.","title":"NLP"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#the-pretrained-model","text":"In lesson 1, we acheived over 90% accuracy because we were using a pre-trained model that we fine-tuned further. So what is a pre-trained language model? A language model is one where we try to predict the next word in a sentence. For lesson one, this was a neaural net pre-trained on wiki articles (Wikitext 103). How does this help with sentiment anlysis? Like pre-trained image models, language models too contain a lot of information that can be leveraged rather than training from scratch. Fine-tuning will throw away the last layer(s) and train these rather than the entire model. Through transfer learning, we will create an Imdb language model using the wikitext model as a base.","title":"The pretrained model"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#text-preprocessing","text":"Tokenization : Convert the text into a list of words (or characters, or substrings, depending on the granularity of your model) Numericalization : Make a list of all of the unique words that appear (the vocab), convert each word into a number, by looking up its index in the vocab. Language model data loader creation : fastai provides an LMDataLoader class which automatically handles creating a dependent variable that is offset from the independent variable by one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required Language model creation : We need a special kind of model that does something we haven't seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network (RNN). source","title":"Text preprocessing"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#tokenisation","text":"There are different approaches to tokenisation these are... - Word based: which splits a sentence on spaces - Subword based: splits words into smaller parts based on the most commonly occuring substrings - Character bases: splits a sentence into individual characters","title":"Tokenisation"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#word-tokenisation-with-fastai","text":"there are a number of tokenisers out there, fastai makes it easy to switch between them. currently fastai default is from the spaCy library","title":"Word tokenisation with fastai"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#data","text":"The IMDB Large dataset contains 25,000 highly polar movie reviews for training, and 25,000 for testing. It is very large! from fastai.text.all import * path = untar_data ( URLs . IMDB ) get_text_files gets all the text files in a path. We can also optionally pass folders to restrict the search to a particular list of subfolders: # only using 50k sample due to size of dataset # results may vary from fastai book files = get_text_files ( path , folders = [ 'train' , 'test' , 'unsup' ])[: 50000 ] # print out slice of first review txt = files [ 0 ] . open () . read () txt [: 75 ] \"The worst movie I've ever seen, hands down. It is ten times more a rip-off \" first() - First element of x , or None if missing coll_repr - String repr of up to max_n items of (possibly lazy) collection c spacy = WordTokenizer () toks = first ( spacy ([ txt ])) print ( coll_repr ( toks , 30 )) (#156) ['The','worst','movie','I',\"'ve\",'ever','seen',',','hands','down','.','It','is','ten','times','more','a','rip','-','off','of','Lake','Placid','than','it','is','a','sequel','.','Director'...] fastai provides additional functionality to tokenisers, such as adding in special tokens like begining of string xxbos or lowercasing all strings and adding the xxmaj token before. This is done to preserve importance and reduce some complexity. tkn = Tokenizer ( spacy ) print ( coll_repr ( tkn ( txt ), 31 )) (#176) ['xxbos','xxmaj','the','worst','movie','xxmaj','i',\"'ve\",'ever','seen',',','hands','down','.','xxmaj','it','is','ten','times','more','a','rip','-','off','of','xxmaj','lake','xxmaj','placid','than','it'...] You can explore the rules like so defaults . text_proc_rules [<function fastai.text.core.fix_html(x)>, <function fastai.text.core.replace_rep(t)>, <function fastai.text.core.replace_wrep(t)>, <function fastai.text.core.spec_add_spaces(t)>, <function fastai.text.core.rm_useless_spaces(t)>, <function fastai.text.core.replace_all_caps(t)>, <function fastai.text.core.replace_maj(t)>, <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>] then check the source code for each using ?? like fix_html??","title":"Data"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#subword-tokenisation","text":"Word tokenisation relies on spaces within the document. Subword tokenisation does two things 1. analyses a corpus of documents to find the most commonly occurring groups of letters. These then become the vocab 2. Tokenise the corpus using this vocab of subword units txts = L ( o . open () . read () for o in files [: 2000 ]) We instantiate our tokeniser, by defining the size of the vocab, then training it. meaing, have the tokeniser read the documents, find the common sequences of characters then create the vocab. in fastai, this is done with setup . def subword ( sz ): sp = SubwordTokenizer ( vocab_sz = sz ) sp . setup ( txts ) return ' ' . join ( first ( sp ([ txt ]))[: 40 ]) subword ( 1000 ) \"\u2581The \u2581worst \u2581movie \u2581I ' ve \u2581ever \u2581seen , \u2581hand s \u2581down . \u2581It \u2581is \u2581t en \u2581time s \u2581more \u2581a \u2581 r i p - off \u2581of \u2581L ake \u2581P la ci d \u2581than \u2581it \u2581is \u2581a \u2581sequel .\" the special character \u2581 represents a space character in the original text. using a smaller vocab results in each token representing fewer characters, and will need more tokens to represent a sentence subword ( 200 ) \"\u2581The \u2581w or s t \u2581movie \u2581I ' ve \u2581 e ver \u2581s e en , \u2581 h an d s \u2581d o w n . \u2581I t \u2581is \u2581 t en \u2581 t i m es \u2581mo re \u2581a\" Using larger vocab will result in most common English words ending up in the vocab, and fewer tokens will be needed to represent a sentence subword ( 10000 ) \"\u2581The \u2581worst \u2581movie \u2581I ' ve \u2581ever \u2581seen , \u2581hands \u2581down . \u2581It \u2581is \u2581ten \u2581times \u2581more \u2581a \u2581rip - off \u2581of \u2581Lake \u2581Placid \u2581than \u2581it \u2581is \u2581a \u2581sequel . \u2581Director \u2581David \u2581F lo re s \u2581clearly \u2581did \u2581not \u2581go\" There are trade-off to be made here: larger vocab means fewer tokens per sentence leading to faster training and less memory and state required for the model. The downside is larger embedding matrices which require more data to learn. Subword tokenisation provides an easy way to scale between character and word tokenisation while also being useful for applications involving languages other than english.","title":"Subword Tokenisation"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#numericalisation","text":"This is the process of mapping tokens to integers. It is nearly identical to the steps necessary to create a Category variable Make a list of all possible levels of that categorical variable (vocab) replace each level with it's index in the vocab toks = tkn ( txt ) print ( coll_repr ( tkn ( txt ), 32 )) (#176) ['xxbos','xxmaj','the','worst','movie','xxmaj','i',\"'ve\",'ever','seen',',','hands','down','.','xxmaj','it','is','ten','times','more','a','rip','-','off','of','xxmaj','lake','xxmaj','placid','than','it','is'...] # a small example toks200 = txts [: 200 ] . map ( tkn ) toks200 [ 0 ] (#176) ['xxbos','xxmaj','the','worst','movie','xxmaj','i',\"'ve\",'ever','seen'...] num = Numericalize () num . setup ( toks200 ) coll_repr ( num . vocab , 20 ) \"(#2144) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj','the',',','.','and','a','of','to','is','in','it','i'...]\" this is our vocab, starting with special tokens, then english words in order of highest frequency. we can now use the Numericalize object as a function and apply it to our tokens to see the integers they now represent nums = num ( toks )[: 20 ] nums tensor([ 2, 8, 9, 310, 27, 8, 19, 218, 158, 141, 10, 0, 229, 11, 8, 18, 16, 550, 299, 66])","title":"Numericalisation"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#create-batches-for-language-model","text":"Batches are split based on the sequence length and batch size. Batches are created by concatenating individual texts into a stream. Order of inputs are randomised, meaning the order of the documents (not order of words in these) are shuffled. The stream is then divided into batches. This is done at every epoch - shuffle the collection of documents - concatenate them together into a stream of tokens - cut the stream into batches of fixed size consecutive mini streams This is all done in fastai using LMDataLoader . For example nums200 = toks200 . map ( num ) dl = LMDataLoader ( nums200 ) x , y = first ( dl ) x . shape , y . shape (torch.Size([64, 72]), torch.Size([64, 72])) batch size = 64 stream length = 72 Looking at the first row of the independent variable should contain the start of the text ' ' . join ( num . vocab [ o ] for o in x [ 0 ][: 20 ]) \"xxbos xxmaj the worst movie xxmaj i 've ever seen , xxunk down . xxmaj it is ten times more\" the dependent variable will be the same but offset by one token ' ' . join ( num . vocab [ o ] for o in y [ 0 ][: 20 ]) \"xxmaj the worst movie xxmaj i 've ever seen , xxunk down . xxmaj it is ten times more a\"","title":"Create batches for language model"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#pt-1-training-a-text-classifier-using-fastai","text":"The reason that TextBlock is special is because setting up the numericalizer's vocab can take a long time (we have to read and tokenize every document to get the vocab). To be as efficient as possible the TextBlock performs a few optimizations: It saves the tokenized documents in a temporary folder, so it doesn't have to tokenize them more than once It runs multiple tokenization processes in parallel, to take advantage of your computer's CPUs We need to tell TextBlock how to access the texts, so that it can do this initial preprocessing\u2014that's what from_folder does. show_batch then works in the usual way source get_imdb = partial ( get_text_files , folders = [ 'train' , 'test' , 'unsup' ]) dls_lm = DataBlock ( blocks = TextBlock . from_folder ( path , is_lm = True ), get_items = get_imdb , splitter = RandomSplitter ( 0.1 ) ) . dataloaders ( path , path = path , bs = 128 , seq_len = 80 ) dls_lm . show_batch ( max_n = 2 ) text text_ 0 xxbos xxmaj my sincere advice to all : do n't watch the movie . \\n\\n xxmaj do n't even go near to the theater where this movie is being played ! ! even a glimpse of it is bad for health . serious . no jokes . it 's xxunk am in the morning . and i returned from this crappiest movie on this universe . xxup four xxup hours xxup damn xxrep 3 ! i am proud that i xxmaj my sincere advice to all : do n't watch the movie . \\n\\n xxmaj do n't even go near to the theater where this movie is being played ! ! even a glimpse of it is bad for health . serious . no jokes . it 's xxunk am in the morning . and i returned from this crappiest movie on this universe . xxup four xxup hours xxup damn xxrep 3 ! i am proud that i survived 1 what has led to the overwhelmingly negative reaction . \\n\\n xxmaj the shock value is the least appealing thing about this film - a minor detail that has been blown out of proportion . xxmaj the story is of xxmaj pierre 's downfall - and the subsequent destruction of those around him - which is overtly demonstrated in his features , demeanour and xxunk . xxmaj the dialogue and soundtrack set this film apart from any other i have seen has led to the overwhelmingly negative reaction . \\n\\n xxmaj the shock value is the least appealing thing about this film - a minor detail that has been blown out of proportion . xxmaj the story is of xxmaj pierre 's downfall - and the subsequent destruction of those around him - which is overtly demonstrated in his features , demeanour and xxunk . xxmaj the dialogue and soundtrack set this film apart from any other i have seen ,","title":"Pt 1: Training a Text Classifier using fastai"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#fine-tuning","text":"To convert the integer word indices into activations for the neural net, we will use embeddings. These are then fed into the RNN using an architecture called AWD_LSTM cross entropy loss is sutable here since this is a classification problem. Often a metric called perplexity is used in NLP, this is the exponential of the loss ( torch.exp(cross_entropy) ). To this we will also add accuracy to determine how the model performs when trying to predict the next word. to_fp16 uses less GPU memory and trains faster learn = language_model_learner ( dls_lm , AWD_LSTM , drop_mult = 0.3 , metrics = [ accuracy , Perplexity ()]) . to_fp16 () learn . fit_one_cycle ( 1 , 2e-2 ) epoch train_loss valid_loss accuracy perplexity time 0 4.129613 3.911054 0.299887 49.951557 34:09","title":"Fine tuning"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#saving-and-loading-models","text":"learn . save ( '1epoch' ) learn = learn . load ( '1epoch' ) learn . unfreeze () learn . fit_one_cycle ( 5 , 2e-3 ) epoch train_loss valid_loss accuracy perplexity time 0 3.870227 3.766035 0.318266 43.208385 35:48 1 3.772985 3.673187 0.329042 39.377213 35:38 2 3.677068 3.615694 0.335646 37.177132 35:36 3 3.570553 3.582507 0.339907 35.963577 35:56 4 3.526067 3.577981 0.340754 35.801178 35:58 IOPub message rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_msg_rate_limit`. Current values: NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec) NotebookApp.rate_limit_window=3.0 (secs) IOPub message rate exceeded. The notebook server will temporarily stop sending output to the client in order to avoid crashing it. To change this limit, set the config variable `--NotebookApp.iopub_msg_rate_limit`. Current values: NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec) NotebookApp.rate_limit_window=3.0 (secs)","title":"Saving and Loading models"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#pt-2-a-language-model-from-scratch","text":"","title":"Pt 2: A Language Model from Scratch"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#data_1","text":"The Human Numbers data set contains the first 10,000 numbers written in english. It was created by Jeremy for experimentation. from fastai.text.all import * path = untar_data ( URLs . HUMAN_NUMBERS ) path . ls () (#2) [Path('/storage/data/human_numbers/valid.txt'),Path('/storage/data/human_numbers/train.txt')] Lake a look at some of the data lines = L () with open ( path / 'train.txt' ) as f : lines += L ( * f . readlines ()) with open ( path / 'valid.txt' ) as f : lines += L ( * f . readlines ()) lines (#9998) ['one \\n','two \\n','three \\n','four \\n','five \\n','six \\n','seven \\n','eight \\n','nine \\n','ten \\n'...] concat all into one big stream, with \".\" to separate text = ' . ' . join ([ l . strip () for l in lines ]) text [: 100 ] 'one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo' use work tokenisation by splitting on spaces tokens = L ( text . split ( ' ' )) tokens [ 100 : 110 ] (#10) ['.','forty','two','.','forty','three','.','forty','four','.'] for numericalisation, we need to create a list of all unique words. we can then convert these into numbers vocab = L ( tokens ) . unique () vocab (#30) ['one','.','two','three','four','five','six','seven','eight','nine'...] word2idx = { w : i for i , w in enumerate ( vocab )} nums = L ( word2idx [ i ] for i in tokens ) tokens , nums ((#63095) ['one','.','two','.','three','.','four','.','five','.'...], (#63095) [0,1,2,1,3,1,4,1,5,1...]) We now have a small dataset that we can use for language modelling.","title":"Data"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#creating-a-language-model","text":"For this simple example, we will predict the next word based on the previous 3 words. To do this, create a list with the independent variable being the first 3 words, and dependent variable being the 4th word. L (( tokens [ i : i + 3 ], tokens [ i + 3 ]) for i in range ( 0 , len ( tokens ) - 4 , 3 ))[ 0 ] ((#3) ['one','.','two'], '.') We can see from looking at the first items that ['one','.','two'] are the independent variable and '.' is the dependent variable. What the model will actually use are tensors of the numericalised values. seqs = L (( tensor ( nums [ i : i + 3 ]), nums [ i + 3 ]) for i in range ( 0 , len ( nums ) - 4 , 3 )) seqs (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...]","title":"Creating a Language Model"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#create-a-dataloader","text":"batch size of 64 split randomly, taking 80% bs = 64 cut = int ( len ( seqs ) * 0.8 ) dls = DataLoaders . from_dsets ( seqs [: cut ], seqs [ cut :], bs = 64 , shuffle = False )","title":"Create a DataLoader"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#the-model-in-pytorch","text":"A simple linear model has an input of size (batch size x #inputs), followed by a single hidden layer that computes a matrix product followed by ReLU; Out of which we will get some activations, the size of which will be (batch size x #activations). This is then followed by more computation, a matrix product followed by softmax. The final output size will be (batch size x #classes). We will take this approach and modify it for our model. Our model will be a Neural Net with 3 layers - The embedding layer (input to hidden i_h ) - The Linear Layer (hidden to hidden h_h ) - this layer created the activations for the next word - this layer will be used for words 1-3 - Final Linear layer to predict the fourth word (hidden to output layer h_o ) In the diagram below, the arrows represent the computational steps (a linear layer followed by non-linearity (ReLU)) To start, take the word 1 input and put it through the linear layer and ReLU to get first set of activations. Then put that through another linear layer and non-linearity. These activations are added (or concatenated would be fine) to the resulting activations of word 2 which is also run through a linear layer and non-linearity. Again the results are run through another linear layer and non-linearity while also adding in the result of putting word 3 through a computation layer as we did with word 2. These activations then go through a final linear layer and softmax to create the output activations. What is interesting about this model is that inputs are entering in later layer and added into the network. Also, arrows of the same colour mean that the same weight matrix is being used. In code we can represent this like so... To go from the input to hidden layer we use an embedding. We create one embedding which subsequent words will also go through, and each time we add this to the current set of activations. Why use the same embedding layer?? Conceptually, the words all represent english spellings of numbers, so they have the same meaning and therefore wouldn't need separate embeddings. Once we have the embedding, we send this through the linear layer, then through relu. As with embeddings, we can use the same Linear layer because we are doing the same kind of computation. The computation happens from the inner most brackets out so this... F.relu(self.h_h(self.i_h(x[:,0]))) - starts with sending word 1 x[:,0] through the embedding layer self.i_h(x[:,0]) - then through a Linear layer self.h_h(self.i_h(x[:,0])) - and finally through the relu F.relu(self.h_h(self.i_h(x[:,0]))) class LMModel1 ( Module ): # vocab_sz == vocab size def __init__ ( self , vocab_sz , n_hidden ): # the embedding layer self . i_h = nn . Embedding ( vocab_sz , n_hidden ) # the linear layer self . h_h = nn . Linear ( n_hidden , n_hidden ) # final linear layer self . h_o = nn . Linear ( n_hidden , vocab_sz ) def forward ( self , x ): # h is the hidden state # word 1 to embedding h = F . relu ( self . h_h ( self . i_h ( x [:, 0 ]))) # word 2 to same embedding h = h + self . i_h ( x :, 1 ) h = F . relu ( self . h_h ( h )) h = h + self . i_h ( x [:, 2 ]) # word 3 to same embedding h = F . relu ( self . h_h ( h )) # hidden to output return self . h_o ( h ) the activations in the model are known as the \"hidden state\" class LMModel1 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) def forward ( self , x ): h = F . relu ( self . h_h ( self . i_h ( x [:, 0 ]))) h = h + self . i_h ( x [:, 1 ]) h = F . relu ( self . h_h ( h )) h = h + self . i_h ( x [:, 2 ]) h = F . relu ( self . h_h ( h )) return self . h_o ( h ) learn = Learner ( dls , LMModel1 ( len ( vocab ), 64 ), loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 4 , 1e-3 ) epoch train_loss valid_loss accuracy time 0 1.863818 2.031583 0.464939 00:02 1 1.392999 1.803210 0.467079 00:02 2 1.410863 1.698382 0.490849 00:02 3 1.371146 1.703473 0.411457 00:02 So far our accuracy is just under 50%. Not bad. We can improve by first refactoring... LMModel1 has a few repeated steps, we can remove this by adding in a for loop.","title":"The model in PyTorch"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#our-first-recurrent-neural-net","text":"class LMModel2 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) def forward ( self , x ): # initialise h as 0. # this gets braodcast to a tensor in the loop h = 0. for i in range ( 3 ): h = h + self . i_h ( x [:, i ]) h = F . relu ( self . h_h ( h )) return self . h_o ( h ) # check we get the same results learn = Learner ( dls , LMModel2 ( len ( vocab ), 64 ), loss_func = F . cross_entropy , metrics = accuracy ) learn . fit_one_cycle ( 4 , 1e-3 ) epoch train_loss valid_loss accuracy time 0 1.877445 2.006204 0.479914 00:02 1 1.398311 1.774232 0.482054 00:03 2 1.421882 1.650312 0.492988 00:03 3 1.372779 1.634449 0.484906 00:02 We have actually just created a Recurrent Nuearal Net. Reminder - Hidden State represents the activations that are occurring inside the neural net.","title":"Our first Recurrent Neural Net"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#maintaining-the-hidden-state","text":"we can do this by storing the hidden state and updating it. detach throws away the gradient history, also known as truncated back propagation .","title":"Maintaining the Hidden State"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#look-into-this","text":"class LMModel3 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = 0. def forward ( self , x ): for i in range ( 3 ): self . h = self . h + self . i_h ( x [:, i ]) self . h = F . relu ( self . h_h ( self . h )) out = self . h_o ( self . h ) self . h = self . h . detach () return out def reset ( self ): self . h = 0. m = len ( seqs ) // bs m , bs , len ( seqs ) (328, 64, 21031) def group_chunks ( ds , bds ): m = len ( ds ) // bs new_ds = L () for i in range ( m ): new_ds += L ( ds [ i + m * j ] for j in range ( bs )) return new_ds cut = int ( len ( seqs ) * 0.8 ) dls = DataLoaders . from_dsets ( group_chunks ( seqs [: cut ], bs ), group_chunks ( seqs [ cut :], bs ), bs = bs , drop_last = True , shuffle = False )","title":"look into this!!"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#callbacks","text":"ModelResetter is a fastai callback that resets the model at each training/validation step. learn = Learner ( dls , LMModel3 ( len ( vocab ), 64 ), loss_func = F . cross_entropy , metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 10 , 3e-3 ) epoch train_loss valid_loss accuracy time 0 1.720635 1.881263 0.397837 00:03 1 1.319839 1.718479 0.460337 00:03 2 1.100321 1.620753 0.508413 00:03 3 1.016059 1.486176 0.543750 00:02 4 0.995253 1.397851 0.554567 00:03 5 0.965056 1.494172 0.529567 00:03 6 0.928281 1.383823 0.590625 00:03 7 0.841828 1.437241 0.601442 00:03 8 0.796445 1.491238 0.609615 00:03 9 0.787689 1.509905 0.606731 00:03 This RNN keeps the state from batch to batch and the results show the uplift from this change. By only predicting every 4th word, we are throwing away signal, which seems wastful. By moving the output stage inside the loop (ie after every hidden state was created we make a prediction) it means we can predict the next word after every single word, rather than every 3 words. To do this we have to change our data so that the dependent variable has each of the three next words after each of out three input words. sl = 16 # sequence length seqs = L (( tensor ( nums [ i : i + sl ]), tensor ( nums [ i + 1 : i + sl + 1 ])) for i in range ( 0 , len ( nums ) - sl - 1 , sl )) cut = int ( len ( seqs ) * 0.8 ) dls = DataLoaders . from_dsets ( group_chunks ( seqs [: cut ], bs ), group_chunks ( seqs [ cut :], bs ), bs = bs , drop_last = True , shuffle = False ) We can see from the first two items in seqs that they are the same length but the second list is offset by 1 [ L ( vocab [ o ] for o in s ) for s in seqs [ 0 ]] [(#16) ['one','.','two','.','three','.','four','.','five','.'...], (#16) ['.','two','.','three','.','four','.','five','.','six'...]] update the model by creating a list to store outputs, then append to this after every element in the loop # Modify the model to output a prediction after every word class LMModel4 ( Module ): def __init__ ( self , vocab_sz , n_hidden ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . h_h = nn . Linear ( n_hidden , n_hidden ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = 0 def forward ( self , x ): outs = [] for i in range ( sl ): self . h = self . h + self . i_h ( x [:, i ]) self . h = F . relu ( self . h_h ( self . h )) outs . append ( self . h_o ( self . h )) self . h = self . h . detach () return torch . stack ( outs , dim = 1 ) def reset ( self ): self . h = 0 # flatten targets to fit loss function def loss_func ( inp , targ ): return F . cross_entropy ( inp . view ( - 1 , len ( vocab )), targ . view ( - 1 )) learn = Learner ( dls , LMModel4 ( len ( vocab ), 64 ), loss_func = loss_func , metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 15 , 3e-3 ) epoch train_loss valid_loss accuracy time 0 3.212160 3.065020 0.248291 00:01 1 2.303481 1.984242 0.440511 00:01 2 1.722320 1.854335 0.428141 00:01 3 1.448855 1.685089 0.516846 00:01 4 1.267960 1.905218 0.549316 00:01 5 1.135633 1.983428 0.591064 00:01 6 1.026269 2.132295 0.593994 00:01 7 0.943246 2.123302 0.622966 00:01 8 0.850973 2.263324 0.638346 00:01 9 0.784856 2.315861 0.662028 00:01 10 0.729662 2.344142 0.649821 00:01 11 0.689929 2.379879 0.648519 00:01 12 0.656299 2.397321 0.655355 00:01 13 0.634652 2.373445 0.661947 00:01 14 0.623158 2.386648 0.659424 00:01","title":"Callbacks"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#multilayer-rnn","text":"Our model is deep but every hidden to hidden layer uses the same weight matrix which means it isn't that deep at all. It is using the same weight matrix every time, so not very sophisticated. Let's refactor again to pass the activations of our current net into a second recurrent neaural network. This is called a stacked or multilayered RNN. Using PyTorch's nn.RNN module lets us define the number of layers ( n_layers ). We can also remove the loop and just call self.rnn class LMModel5 ( Module ): def __init__ ( self , vocab_sz , n_hidden , n_layers ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . rnn = nn . RNN ( n_hidden , n_hidden , n_layers , batch_first = True ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = torch . zeros ( n_layers , bs , n_hidden ) def forward ( self , x ): res , h = self . rnn ( self . i_h ( x ), self . h ) self . h = h . detach () return self . h_o ( res ) def reset ( self ): self . h . zero_ () # using 2 layers learn = Learner ( dls , LMModel5 ( len ( vocab ), 64 , 2 ), loss_func = loss_func , metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 15 , 3e-3 ) epoch train_loss valid_loss accuracy time 0 3.054249 2.617002 0.445882 00:01 1 2.159026 1.819027 0.470703 00:01 2 1.711899 1.820897 0.402262 00:01 3 1.492573 1.740131 0.468669 00:01 4 1.336913 1.825282 0.494303 00:01 5 1.205158 1.927248 0.513591 00:01 6 1.079775 1.974853 0.543864 00:01 7 0.975060 2.035518 0.549235 00:01 8 0.899264 2.100957 0.536458 00:01 9 0.847659 2.070400 0.546956 00:01 10 0.796934 2.078454 0.546875 00:01 11 0.756011 2.080719 0.546956 00:01 12 0.725375 2.105812 0.549886 00:01 13 0.704403 2.089411 0.549723 00:01 14 0.693146 2.079454 0.550700 00:01 Our results are worse! Why? Deep models are hard to train. This can be due to exploding or disappearing activiations. This basically means that our results either become very very large or very very small. This causes an explosion or vanishing of a number and can be computationally intensive or the accuracy of the floating point numbers gets lost. We can avoid this in a number of ways...","title":"Multilayer RNN"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#lstm","text":"Replacing the matrix multiplication in an RNN with this architecture, basically means the model is able to make decisions about how much of an update to do each time. This helps the model to avoid updating too much or too little.","title":"LSTM"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#training-a-language-model-using-lstms","text":"This is the same network but the RNN is replaced with an LSTM. We need to increase the number of layers in our hidden state for this to work because the LSTM has more layers. # Training with LSTM class LMModel6 ( Module ): def __init__ ( self , vocab_sz , n_hidden , n_layers ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . rnn = nn . LSTM ( n_hidden , n_hidden , n_layers , batch_first = True ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h = [ torch . zeros ( n_layers , bs , n_hidden ) for _ in range ( 2 )] def forward ( self , x ): res , h = self . rnn ( self . i_h ( x ), self . h ) self . h = [ h_ . detach () for h_ in h ] return self . h_o ( res ) def reset ( self ): for h in self . h : h . zero_ () learn = Learner ( dls , LMModel6 ( len ( vocab ), 64 , 2 ), loss_func = CrossEntropyLossFlat (), metrics = accuracy , cbs = ModelResetter ) learn . fit_one_cycle ( 15 , 1e-2 ) epoch train_loss valid_loss accuracy time 0 3.040422 2.731378 0.391113 00:02 1 2.212882 1.787726 0.447917 00:02 2 1.632609 1.882631 0.484049 00:02 3 1.323149 2.013697 0.508057 00:02 4 1.091380 2.002088 0.518880 00:02 5 0.832751 1.733890 0.622233 00:02 6 0.620027 1.593947 0.696370 00:03 7 0.412642 1.505920 0.715495 00:04 8 0.257724 1.480889 0.761475 00:04 9 0.157874 1.392939 0.771973 00:04 10 0.097578 1.393537 0.774984 00:02 11 0.064152 1.374384 0.778158 00:02 12 0.046606 1.387635 0.785889 00:02 13 0.037822 1.404215 0.781982 00:02 14 0.033807 1.398724 0.782389 00:03 Results are much much better!","title":"Training a Language Model Using LSTMs"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#regularising-an-lstm","text":"","title":"Regularising an LSTM"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#dropout","text":"Dropout improves neaural net training by deleting random activations. This reduces the computation but also prevents the model from overfitting. Dropout helps the model to generalise by ensuring certain activations don't over specialise during the learning process class Droput - p the probability that an activation gets deleted - only perform dropout in training - mask the mask a tensor with random zeros with probability ( p ) and ones with probability ( p -1) class Dropout ( Module ): def __init__ ( self , p ): self . p = p def forward ( self , x ): if not self . training : return x mask = x . new ( * x . shape ) . bernoulli ( 1 - p ) return x * mask . div_ ( 1 - p )","title":"Dropout"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#a-simple-example","text":"p = .3 B = torch . ones (( 3 , 3 )) . bernoulli ( 1 - p ) B tensor([[0., 1., 1.], [1., 0., 0.], [0., 0., 0.]]) In this example, 1- p adds 3 ones in the 3*3 matrix. Basically the probability of drawing a one here is 3/9 or 0.3. As we saw earlier with one hot encodings, this matrix will act as a lookup when you multiply it by another matrix. In context of what we are doing, by performing this multiplication you are randomly prunning elements of the other matrix A = tensor ([[ 1. , 2. , 3. ], [ 4. , 5. , 6. ], [ 7. , 8. , 9. ]]) A * B tensor([[0., 2., 3.], [4., 0., 0.], [0., 0., 0.]]) Corresponding elements of A are returned only if there is a 1 in the same position in matrix B","title":"A simple example"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#ar-and-tar-regularisation","text":"AR (activation regularisation) and TAR (temporal activation regularisation) are very similar to weight decay but are applied to activations instead of weights. TAR is linked to the fact that we are trying to predict a sequence of tokens. So we take the difference of the activations between time steps. It limits the changes in activations between time steps.","title":"AR and TAR regularisation"},{"location":"fastai%20deep%20learning%202020/lesson%2008/#weight-tying","text":"Sets the hidden to output weights equal to the input to hidden weights. The idea is that converting words to activations and activations to words should conceptually be the same thing since the language is consistent, and the computation is consistent so why would you need to change the weights? class LMModel7 ( Module ): def __init__ ( self , vocab_sz , n_hidden , n_layers , p ): self . i_h = nn . Embedding ( vocab_sz , n_hidden ) self . rnn = nn . LSTM ( n_hidden , n_hidden , n_layers , batch_first = True ) self . drop = nn . Dropout ( p ) self . h_o = nn . Linear ( n_hidden , vocab_sz ) self . h_o . weight = self . i_h . weight self . h = [ torch . zeros ( n_layers , bs , n_hidden ) for _ in range ( 2 )] def forward ( self , x ): raw , h = self . rnn ( self . i_h ( x ), self . h ) out = self . drop ( raw ) self . h = [ h_ . detach () for h_ in h ] return self . h_o ( out ), raw , out def reset ( self ): for h in self . h : h . zero_ () learn = Learner ( dls , LMModel7 ( len ( vocab ), 64 , 2 , 0.5 ), loss_func = CrossEntropyLossFlat (), metrics = accuracy , cbs = [ ModelResetter , RNNRegularizer ( alpha = 2 , beta = 1 )]) This is the same as above but TextLearner adds the additions peices for you learn = TextLearner ( dls , LMModel7 ( len ( vocab ), 64 , 2 , 0.4 ), loss_func = CrossEntropyLossFlat (), metrics = accuracy ) learn . fit_one_cycle ( 15 , 1e-2 , wd = 0.1 ) epoch train_loss valid_loss accuracy time 0 0.030605 1.401770 0.780680 00:02 1 0.029504 1.535201 0.766602 00:03 2 0.045721 1.465324 0.771484 00:03 3 0.057497 1.550894 0.807780 00:02 4 0.043013 1.394347 0.807292 00:02 5 0.029584 1.430816 0.807536 00:02 6 0.025191 1.391779 0.826009 00:02 7 0.021182 1.496358 0.825439 00:03 8 0.016158 1.389334 0.817139 00:02 9 0.014285 1.503886 0.828369 00:02 10 0.011608 1.421619 0.823079 00:02 11 0.009167 1.429033 0.825521 00:02 12 0.007534 1.449290 0.824382 00:02 13 0.006630 1.455278 0.824300 00:02 14 0.006208 1.456966 0.824056 00:02 Almost 85% accuracy!","title":"Weight Tying"},{"location":"graph%20representation%20learning/","text":"About Graph Representation Learning","title":"About"},{"location":"graph%20representation%20learning/#about","text":"","title":"About"},{"location":"graph%20representation%20learning/#graph-representation-learning","text":"","title":"Graph Representation Learning"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Node Classification With Graph Neural Networks Generally speaking, there are three main prediction tasks on graphs: graph-level, node-level & edge-level. In graph-level tasks we are looking to make a prediction on the entire graph for example classifying molecules or proteins. For node-level, we aim to predict an attribute at the node level ie node classification or regression. Edge-level seeks to make predictions about the connections between nodes ie whether or not a connection exists between two nodes and potentially what the strength of that connection is. We will focus on node classification in the following example using the Cora dataset. \"The Cora dataset consists of 2708 scientific publications classified into one of seven classes... Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\" Class labels 0: theory 1: reinforcement_learning 2: genetic_algotrithms 3: neural_networks 4: probabilistic_methods 5: case_based 6: rule_learning The goal will be to correctly predict the class label of nodes in the graph. import torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.nn import GCNConv from torch_geometric.utils import to_networkx import random import networkx as nx import matplotlib.pyplot as plt from torch_geometric.datasets import Planetoid dataset = Planetoid ( root = '/tmp/Cora' , name = 'Cora' ) data = dataset [ 0 ] data Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) # edges are bidirectional, meaning messages can be passed # in both directions between a pair of nodes data . is_undirected () True Visualise the graph def convert_to_networkx ( graph , n_sample = None ): g = to_networkx ( graph , node_attrs = [ \"x\" ]) y = graph . y . numpy () if n_sample is not None : sampled_nodes = random . sample ( g . nodes , n_sample ) g = g . subgraph ( sampled_nodes ) y = y [ sampled_nodes ] return g , y def plot_graph ( g , y ): plt . figure ( figsize = ( 9 , 7 )) nx . draw_spring ( g , node_size = 30 , arrows = False , node_color = y ) plt . show () g , y = convert_to_networkx ( data , n_sample = 1000 ) plot_graph ( g , y ) Node Classification For the node classification problem, we are splitting the nodes into train, valid, and test using the RandomNodeSplit module from PyG (we are replacing the original split masks in the data as it has a too small train set). import torch_geometric.transforms as T split = T . RandomNodeSplit ( num_val = 0.1 , num_test = 0.2 ) data = split ( data ) data Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) Baseline MLP classifyer class MLP ( nn . Module ): \"\"\" simple MLP for classification \"\"\" def __init__ ( self , n_input , n_output , n_hidden = 64 ): super () . __init__ () self . layers = nn . Sequential ( nn . Linear ( n_input , n_hidden ), nn . ReLU (), nn . Linear ( n_hidden , n_hidden // 2 ), nn . ReLU (), nn . Linear ( n_hidden // 2 , n_output ) ) def forward ( self , data ): out = self . layers ( data . x ) return out def train_node_classifier ( model , data , optimiser , criterion , n_epochs = 200 ): for epoch in range ( 1 , n_epochs + 1 ): model . train () optimiser . zero_grad () out = model ( data ) loss = criterion ( out [ data . train_mask ], data . y [ data . train_mask ]) loss . backward () optimiser . step () pred = out . argmax ( dim = 1 ) acc = eval_node_classifier ( model , data ) log = f 'Epoch: { epoch : 03d } , Train Loss: { loss : .3f } , Val Acc: { acc : .3f } ' if epoch % 10 == 0 : print ( log ) return model def eval_node_classifier ( model , data ): model . eval () pred = model ( data ) . argmax ( dim = 1 ) correct = ( pred [ data . test_mask ] == data . y [ data . test_mask ]) . sum () acc = int ( correct ) / int ( data . test_mask . sum ()) return acc device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # init mlp = MLP ( n_input = dataset . num_node_features , n_output = dataset . num_classes ) . to ( device ) # optimiser and loss optimizer_mlp = torch . optim . Adam ( mlp . parameters (), lr = 0.01 , weight_decay = 5e-4 ) criterion = nn . CrossEntropyLoss () mlp ( data ) . shape #criterion(mlp(data), data.y) torch.Size([2708, 7]) # TRAIN # ------ mlp = train_node_classifier ( mlp , data , optimizer_mlp , criterion , n_epochs = 150 ) test_acc = eval_node_classifier ( mlp , data ) print ( f 'Test Acc: { test_acc : .3f } ' ) Epoch: 010, Train Loss: 0.958, Val Acc: 0.631 Epoch: 020, Train Loss: 0.109, Val Acc: 0.738 Epoch: 030, Train Loss: 0.017, Val Acc: 0.725 Epoch: 040, Train Loss: 0.011, Val Acc: 0.729 Epoch: 050, Train Loss: 0.013, Val Acc: 0.725 Epoch: 060, Train Loss: 0.011, Val Acc: 0.723 Epoch: 070, Train Loss: 0.009, Val Acc: 0.736 Epoch: 080, Train Loss: 0.008, Val Acc: 0.734 Epoch: 090, Train Loss: 0.008, Val Acc: 0.736 Epoch: 100, Train Loss: 0.007, Val Acc: 0.742 Epoch: 110, Train Loss: 0.007, Val Acc: 0.742 Epoch: 120, Train Loss: 0.006, Val Acc: 0.742 Epoch: 130, Train Loss: 0.006, Val Acc: 0.745 Epoch: 140, Train Loss: 0.006, Val Acc: 0.747 Epoch: 150, Train Loss: 0.006, Val Acc: 0.745 Test Acc: 0.745 GCN for Node Classification Architecture This is a simple GCN that we will train to compare performance against the baseline MLP. The architecture is fairly straightforward, two convolutional layers with a ReLU activation function between them. This is a very shallow network, in practice you would experiment with deep vs wide neural nets to understand the effects and trade-offs between computational complexity and predictive accuracy. Loss For classification tasks with $C$ targets, the Cross Entropy criterion is a good choice. The loss function computes the cross entropy between input logits and target. For cross entropy in PyTorch, the input is expected to contain the unnormalized logits for each class. In general, these do not need to be positive or sum to 1. class GCN ( nn . Module ): def __init__ ( self , in_features , out_features , n_hidden = 16 ): super () . __init__ () self . conv1 = GCNConv ( in_features , n_hidden ) self . conv2 = GCNConv ( n_hidden , out_features ) def forward ( self , data ): x , edge_index = data . x , data . edge_index x = self . conv1 ( x , edge_index ) x = F . relu ( x ) out = self . conv2 ( x , edge_index ) return out gcn = GCN ( dataset . num_node_features , dataset . num_classes ) . to ( device ) optimizer_gcn = torch . optim . Adam ( gcn . parameters (), lr = 0.01 , weight_decay = 5e-4 ) criterion = nn . CrossEntropyLoss () # FYI dataset . num_node_features , data . num_node_features (1433, 1433) gcn = train_node_classifier ( gcn , data , optimizer_gcn , criterion ) Epoch: 010, Train Loss: 0.682, Val Acc: 0.751 Epoch: 020, Train Loss: 0.121, Val Acc: 0.786 Epoch: 030, Train Loss: 0.029, Val Acc: 0.777 Epoch: 040, Train Loss: 0.014, Val Acc: 0.785 Epoch: 050, Train Loss: 0.013, Val Acc: 0.787 Epoch: 060, Train Loss: 0.014, Val Acc: 0.805 Epoch: 070, Train Loss: 0.016, Val Acc: 0.812 Epoch: 080, Train Loss: 0.017, Val Acc: 0.810 Epoch: 090, Train Loss: 0.017, Val Acc: 0.811 Epoch: 100, Train Loss: 0.016, Val Acc: 0.810 Epoch: 110, Train Loss: 0.015, Val Acc: 0.808 Epoch: 120, Train Loss: 0.014, Val Acc: 0.808 Epoch: 130, Train Loss: 0.013, Val Acc: 0.811 Epoch: 140, Train Loss: 0.013, Val Acc: 0.810 Epoch: 150, Train Loss: 0.012, Val Acc: 0.812 Epoch: 160, Train Loss: 0.012, Val Acc: 0.811 Epoch: 170, Train Loss: 0.011, Val Acc: 0.811 Epoch: 180, Train Loss: 0.011, Val Acc: 0.810 Epoch: 190, Train Loss: 0.011, Val Acc: 0.810 Epoch: 200, Train Loss: 0.010, Val Acc: 0.810 Slightly different GCN architecture This architecture is identical to the first but adds in a dropout layer. Dropout is a widely used technique to reduce overfitting in neural networks. During training, dropout randomly ignores some number of layer outputs. This has the effect of reducing the capacity or \"thinnning-out\" the network. It also regularises the network by helping ensure that nodes withing the network are not codependent on each other. The loss has been changed to NLLLoss which, when preceeded by LogSoftmax is an equivalent to CrossEntropyLoss . class GCN2 ( torch . nn . Module ): def __init__ ( self , in_features , out_features , n_hidden = 16 ): super () . __init__ () self . conv1 = GCNConv ( in_features , n_hidden ) self . conv2 = GCNConv ( n_hidden , out_features ) def forward ( self , data ): x , edge_index = data . x , data . edge_index x = self . conv1 ( x , edge_index ) x = F . relu ( x ) x = F . dropout ( x , training = self . training ) x = self . conv2 ( x , edge_index ) return F . log_softmax ( x , dim = 1 ) gcn2 = GCN2 ( dataset . num_node_features , dataset . num_classes ) . to ( device ) optimizer_gcn2 = torch . optim . Adam ( gcn2 . parameters (), lr = 0.01 , weight_decay = 5e-4 ) criterion2 = nn . NLLLoss () gcn2 = train_node_classifier ( gcn2 , data , optimizer_gcn2 , criterion2 ) Epoch: 010, Train Loss: 0.874, Val Acc: 0.755 Epoch: 020, Train Loss: 0.260, Val Acc: 0.800 Epoch: 030, Train Loss: 0.126, Val Acc: 0.799 Epoch: 040, Train Loss: 0.060, Val Acc: 0.783 Epoch: 050, Train Loss: 0.046, Val Acc: 0.790 Epoch: 060, Train Loss: 0.040, Val Acc: 0.791 Epoch: 070, Train Loss: 0.042, Val Acc: 0.791 Epoch: 080, Train Loss: 0.053, Val Acc: 0.791 Epoch: 090, Train Loss: 0.048, Val Acc: 0.796 Epoch: 100, Train Loss: 0.031, Val Acc: 0.804 Epoch: 110, Train Loss: 0.028, Val Acc: 0.799 Epoch: 120, Train Loss: 0.032, Val Acc: 0.808 Epoch: 130, Train Loss: 0.042, Val Acc: 0.793 Epoch: 140, Train Loss: 0.053, Val Acc: 0.812 Epoch: 150, Train Loss: 0.042, Val Acc: 0.796 Epoch: 160, Train Loss: 0.023, Val Acc: 0.805 Epoch: 170, Train Loss: 0.020, Val Acc: 0.801 Epoch: 180, Train Loss: 0.023, Val Acc: 0.809 Epoch: 190, Train Loss: 0.036, Val Acc: 0.804 Epoch: 200, Train Loss: 0.029, Val Acc: 0.813 Overall the first GCN performed slightly better than the second, however both performed better than the MLP (~17% uplift) . Let's check the first few predictions against the target to see the results # predictions preds = gcn ( data ) . argmax ( dim =- 1 ) # true values data . y [: 10 ] tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2]) preds [: 10 ] tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2])","title":"01 Node Prediction with Graph Deep Learning"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/#node-classification-with-graph-neural-networks","text":"Generally speaking, there are three main prediction tasks on graphs: graph-level, node-level & edge-level. In graph-level tasks we are looking to make a prediction on the entire graph for example classifying molecules or proteins. For node-level, we aim to predict an attribute at the node level ie node classification or regression. Edge-level seeks to make predictions about the connections between nodes ie whether or not a connection exists between two nodes and potentially what the strength of that connection is. We will focus on node classification in the following example using the Cora dataset. \"The Cora dataset consists of 2708 scientific publications classified into one of seven classes... Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\" Class labels 0: theory 1: reinforcement_learning 2: genetic_algotrithms 3: neural_networks 4: probabilistic_methods 5: case_based 6: rule_learning The goal will be to correctly predict the class label of nodes in the graph. import torch import torch.nn as nn import torch.nn.functional as F from torch_geometric.nn import GCNConv from torch_geometric.utils import to_networkx import random import networkx as nx import matplotlib.pyplot as plt from torch_geometric.datasets import Planetoid dataset = Planetoid ( root = '/tmp/Cora' , name = 'Cora' ) data = dataset [ 0 ] data Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708]) # edges are bidirectional, meaning messages can be passed # in both directions between a pair of nodes data . is_undirected () True","title":"Node Classification With Graph Neural Networks"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/#visualise-the-graph","text":"def convert_to_networkx ( graph , n_sample = None ): g = to_networkx ( graph , node_attrs = [ \"x\" ]) y = graph . y . numpy () if n_sample is not None : sampled_nodes = random . sample ( g . nodes , n_sample ) g = g . subgraph ( sampled_nodes ) y = y [ sampled_nodes ] return g , y def plot_graph ( g , y ): plt . figure ( figsize = ( 9 , 7 )) nx . draw_spring ( g , node_size = 30 , arrows = False , node_color = y ) plt . show () g , y = convert_to_networkx ( data , n_sample = 1000 ) plot_graph ( g , y )","title":"Visualise the graph"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/#node-classification","text":"For the node classification problem, we are splitting the nodes into train, valid, and test using the RandomNodeSplit module from PyG (we are replacing the original split masks in the data as it has a too small train set). import torch_geometric.transforms as T split = T . RandomNodeSplit ( num_val = 0.1 , num_test = 0.2 ) data = split ( data ) data Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])","title":"Node Classification"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/#baseline-mlp-classifyer","text":"class MLP ( nn . Module ): \"\"\" simple MLP for classification \"\"\" def __init__ ( self , n_input , n_output , n_hidden = 64 ): super () . __init__ () self . layers = nn . Sequential ( nn . Linear ( n_input , n_hidden ), nn . ReLU (), nn . Linear ( n_hidden , n_hidden // 2 ), nn . ReLU (), nn . Linear ( n_hidden // 2 , n_output ) ) def forward ( self , data ): out = self . layers ( data . x ) return out def train_node_classifier ( model , data , optimiser , criterion , n_epochs = 200 ): for epoch in range ( 1 , n_epochs + 1 ): model . train () optimiser . zero_grad () out = model ( data ) loss = criterion ( out [ data . train_mask ], data . y [ data . train_mask ]) loss . backward () optimiser . step () pred = out . argmax ( dim = 1 ) acc = eval_node_classifier ( model , data ) log = f 'Epoch: { epoch : 03d } , Train Loss: { loss : .3f } , Val Acc: { acc : .3f } ' if epoch % 10 == 0 : print ( log ) return model def eval_node_classifier ( model , data ): model . eval () pred = model ( data ) . argmax ( dim = 1 ) correct = ( pred [ data . test_mask ] == data . y [ data . test_mask ]) . sum () acc = int ( correct ) / int ( data . test_mask . sum ()) return acc device = torch . device ( 'cuda' if torch . cuda . is_available () else 'cpu' ) # init mlp = MLP ( n_input = dataset . num_node_features , n_output = dataset . num_classes ) . to ( device ) # optimiser and loss optimizer_mlp = torch . optim . Adam ( mlp . parameters (), lr = 0.01 , weight_decay = 5e-4 ) criterion = nn . CrossEntropyLoss () mlp ( data ) . shape #criterion(mlp(data), data.y) torch.Size([2708, 7]) # TRAIN # ------ mlp = train_node_classifier ( mlp , data , optimizer_mlp , criterion , n_epochs = 150 ) test_acc = eval_node_classifier ( mlp , data ) print ( f 'Test Acc: { test_acc : .3f } ' ) Epoch: 010, Train Loss: 0.958, Val Acc: 0.631 Epoch: 020, Train Loss: 0.109, Val Acc: 0.738 Epoch: 030, Train Loss: 0.017, Val Acc: 0.725 Epoch: 040, Train Loss: 0.011, Val Acc: 0.729 Epoch: 050, Train Loss: 0.013, Val Acc: 0.725 Epoch: 060, Train Loss: 0.011, Val Acc: 0.723 Epoch: 070, Train Loss: 0.009, Val Acc: 0.736 Epoch: 080, Train Loss: 0.008, Val Acc: 0.734 Epoch: 090, Train Loss: 0.008, Val Acc: 0.736 Epoch: 100, Train Loss: 0.007, Val Acc: 0.742 Epoch: 110, Train Loss: 0.007, Val Acc: 0.742 Epoch: 120, Train Loss: 0.006, Val Acc: 0.742 Epoch: 130, Train Loss: 0.006, Val Acc: 0.745 Epoch: 140, Train Loss: 0.006, Val Acc: 0.747 Epoch: 150, Train Loss: 0.006, Val Acc: 0.745 Test Acc: 0.745","title":"Baseline MLP classifyer"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/#gcn-for-node-classification","text":"Architecture This is a simple GCN that we will train to compare performance against the baseline MLP. The architecture is fairly straightforward, two convolutional layers with a ReLU activation function between them. This is a very shallow network, in practice you would experiment with deep vs wide neural nets to understand the effects and trade-offs between computational complexity and predictive accuracy. Loss For classification tasks with $C$ targets, the Cross Entropy criterion is a good choice. The loss function computes the cross entropy between input logits and target. For cross entropy in PyTorch, the input is expected to contain the unnormalized logits for each class. In general, these do not need to be positive or sum to 1. class GCN ( nn . Module ): def __init__ ( self , in_features , out_features , n_hidden = 16 ): super () . __init__ () self . conv1 = GCNConv ( in_features , n_hidden ) self . conv2 = GCNConv ( n_hidden , out_features ) def forward ( self , data ): x , edge_index = data . x , data . edge_index x = self . conv1 ( x , edge_index ) x = F . relu ( x ) out = self . conv2 ( x , edge_index ) return out gcn = GCN ( dataset . num_node_features , dataset . num_classes ) . to ( device ) optimizer_gcn = torch . optim . Adam ( gcn . parameters (), lr = 0.01 , weight_decay = 5e-4 ) criterion = nn . CrossEntropyLoss () # FYI dataset . num_node_features , data . num_node_features (1433, 1433) gcn = train_node_classifier ( gcn , data , optimizer_gcn , criterion ) Epoch: 010, Train Loss: 0.682, Val Acc: 0.751 Epoch: 020, Train Loss: 0.121, Val Acc: 0.786 Epoch: 030, Train Loss: 0.029, Val Acc: 0.777 Epoch: 040, Train Loss: 0.014, Val Acc: 0.785 Epoch: 050, Train Loss: 0.013, Val Acc: 0.787 Epoch: 060, Train Loss: 0.014, Val Acc: 0.805 Epoch: 070, Train Loss: 0.016, Val Acc: 0.812 Epoch: 080, Train Loss: 0.017, Val Acc: 0.810 Epoch: 090, Train Loss: 0.017, Val Acc: 0.811 Epoch: 100, Train Loss: 0.016, Val Acc: 0.810 Epoch: 110, Train Loss: 0.015, Val Acc: 0.808 Epoch: 120, Train Loss: 0.014, Val Acc: 0.808 Epoch: 130, Train Loss: 0.013, Val Acc: 0.811 Epoch: 140, Train Loss: 0.013, Val Acc: 0.810 Epoch: 150, Train Loss: 0.012, Val Acc: 0.812 Epoch: 160, Train Loss: 0.012, Val Acc: 0.811 Epoch: 170, Train Loss: 0.011, Val Acc: 0.811 Epoch: 180, Train Loss: 0.011, Val Acc: 0.810 Epoch: 190, Train Loss: 0.011, Val Acc: 0.810 Epoch: 200, Train Loss: 0.010, Val Acc: 0.810","title":"GCN for Node Classification"},{"location":"graph%20representation%20learning/01%20Node%20Prediction%20with%20Graph%20Deep%20Learning/#slightly-different-gcn-architecture","text":"This architecture is identical to the first but adds in a dropout layer. Dropout is a widely used technique to reduce overfitting in neural networks. During training, dropout randomly ignores some number of layer outputs. This has the effect of reducing the capacity or \"thinnning-out\" the network. It also regularises the network by helping ensure that nodes withing the network are not codependent on each other. The loss has been changed to NLLLoss which, when preceeded by LogSoftmax is an equivalent to CrossEntropyLoss . class GCN2 ( torch . nn . Module ): def __init__ ( self , in_features , out_features , n_hidden = 16 ): super () . __init__ () self . conv1 = GCNConv ( in_features , n_hidden ) self . conv2 = GCNConv ( n_hidden , out_features ) def forward ( self , data ): x , edge_index = data . x , data . edge_index x = self . conv1 ( x , edge_index ) x = F . relu ( x ) x = F . dropout ( x , training = self . training ) x = self . conv2 ( x , edge_index ) return F . log_softmax ( x , dim = 1 ) gcn2 = GCN2 ( dataset . num_node_features , dataset . num_classes ) . to ( device ) optimizer_gcn2 = torch . optim . Adam ( gcn2 . parameters (), lr = 0.01 , weight_decay = 5e-4 ) criterion2 = nn . NLLLoss () gcn2 = train_node_classifier ( gcn2 , data , optimizer_gcn2 , criterion2 ) Epoch: 010, Train Loss: 0.874, Val Acc: 0.755 Epoch: 020, Train Loss: 0.260, Val Acc: 0.800 Epoch: 030, Train Loss: 0.126, Val Acc: 0.799 Epoch: 040, Train Loss: 0.060, Val Acc: 0.783 Epoch: 050, Train Loss: 0.046, Val Acc: 0.790 Epoch: 060, Train Loss: 0.040, Val Acc: 0.791 Epoch: 070, Train Loss: 0.042, Val Acc: 0.791 Epoch: 080, Train Loss: 0.053, Val Acc: 0.791 Epoch: 090, Train Loss: 0.048, Val Acc: 0.796 Epoch: 100, Train Loss: 0.031, Val Acc: 0.804 Epoch: 110, Train Loss: 0.028, Val Acc: 0.799 Epoch: 120, Train Loss: 0.032, Val Acc: 0.808 Epoch: 130, Train Loss: 0.042, Val Acc: 0.793 Epoch: 140, Train Loss: 0.053, Val Acc: 0.812 Epoch: 150, Train Loss: 0.042, Val Acc: 0.796 Epoch: 160, Train Loss: 0.023, Val Acc: 0.805 Epoch: 170, Train Loss: 0.020, Val Acc: 0.801 Epoch: 180, Train Loss: 0.023, Val Acc: 0.809 Epoch: 190, Train Loss: 0.036, Val Acc: 0.804 Epoch: 200, Train Loss: 0.029, Val Acc: 0.813 Overall the first GCN performed slightly better than the second, however both performed better than the MLP (~17% uplift) . Let's check the first few predictions against the target to see the results # predictions preds = gcn ( data ) . argmax ( dim =- 1 ) # true values data . y [: 10 ] tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2]) preds [: 10 ] tensor([3, 4, 4, 0, 3, 2, 0, 3, 3, 2])","title":"Slightly different GCN architecture"},{"location":"machine%20learning/","text":"About Machine Learning with PyTorch and Scikit-Learn Notes from the excellent book by Sebastian Raschka","title":"About"},{"location":"machine%20learning/#about","text":"","title":"About"},{"location":"machine%20learning/#machine-learning-with-pytorch-and-scikit-learn","text":"Notes from the excellent book by Sebastian Raschka","title":"Machine Learning with PyTorch and Scikit-Learn"},{"location":"machine%20learning/01%20Topic%20Modeling/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Applying Machine Learning to Sentiment Analysis and Topic Modeling This notebook will explore two topics from Natural Language Processing. The first, sentiment analysis , where we will use machine learing to classify documents based on their positive or negative sentiment. Followed by topic modeling , where we will extract the main topics from these documents. We will be working with the IMDB movie reviews data set containing 50,000 reviews. topics covered - data cleaning and processing - feature axtraction from text - training a classifyer on positive and negative sentiment - topic modeling with LDA This notebook is based on code and material from the excellent book by S. Raschka Machine Learning with PyTorch and Scikit-Learn import numpy as np import pandas as pd import tarfile #import os #import sys from tqdm import tqdm from pathlib import Path p = Path . cwd () 1. Data Cleaning and Preprocessing The IMDB data set was produced by Andrew Mass and others (Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).) and contains 50,000 polar movie reviews, labeled either positive or negative. data can be downloaded from here with tarfile . open ( 'data/aclImdb_v1.tar.gz' , 'r:gz' ) as tar : tar . extractall () basepath = p / 'data/aclImdb' labels = { 'pos' : 1 , 'neg' : 0 } pbar = tqdm ( range ( 50000 )) df = pd . DataFrame () for s in ( 'test' , 'train' ): for l in ( 'pos' , 'neg' ): path = basepath / s / l for file in path . iterdir (): with open ( path / file , 'r' , encoding = 'utf-8' ) as infile : txt = infile . read () df = df . append ([[ txt , labels [ l ]]], ignore_index = True ) pbar . update () df . columns = [ 'review' , 'sentiment' ] 0%| | 0/50000 [00:00<?, ?it/s]/var/folders/h6/76mmjn5902lf0r8382f_r52r0000gn/T/ipykernel_56182/3956242205.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append([[txt, labels[l]]], ignore_index=True) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 49971/50000 [02:01<00:00, 258.76it/s] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } review sentiment 0 Based on an actual story, John Boorman shows t... 1 1 This is a gem. As a Film Four production - the... 1 2 I really like this show. It has drama, romance... 1 3 This is the best 3-D experience Disney has at ... 1 4 Of the Korean movies I've seen, only three had... 1 df . sentiment . value_counts () 1 25000 0 25000 Name: sentiment, dtype: int64 # shuffle index df = df . reindex ( np . random . permutation ( df . index )) # save for later df . to_csv ( p / 'data' / 'imdb_review_data.csv' , index = False , encoding = 'utf-8' ) 2. The Bag-of-Words Model Before text data can be passed onto a machine learning or deep learning model, it needs to be converted into numerical form. The bag-of-words model allows us to do just this by representing text as feature vectors. The model can be summarised as follows... create a vocabulary of unique tokens (words) from the endire set of documents construct a feature vector for each document that contains the frequency count of words as they appear in each particular document. These feature vectors are usually very sparse (containing mainly zeros) since the occurrance of unique words represents only a small subset of all words. 2.1 From Words to Feature Vectors Scikit-learn has implemented the CountVectorizer class that will take in an array of data (documents or sentences), and constructs the bag-of-words model for us. from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer () docs = np . array ([ 'the sun is shining' , 'the weather is sweet' , 'the sun is shining, the weather is sweet' , 'and one and one is two' ]) bag = vectorizer . fit_transform ( docs ) # list of unique words with integer indices # ie, sort alphabetically then assign index vectorizer . vocabulary_ {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7} # let's sort these for convenience sorted ( vectorizer . vocabulary_ . items (), key = lambda x : x [ 1 ]) [('and', 0), ('is', 1), ('one', 2), ('shining', 3), ('sun', 4), ('sweet', 5), ('the', 6), ('two', 7), ('weather', 8)] let's look at the feature vectors bag . toarray () array([[0, 1, 0, 1, 1, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 1], [0, 2, 0, 1, 1, 1, 2, 0, 1], [2, 1, 2, 0, 0, 0, 0, 1, 0]]) Each index position in the feature vectors corresponds to the sorted vocabulary, and represents the frequency of the word within that vector. For example... Looking at the last row ( [2, 1, 2, 0, 0, 0, 0, 1, 0] ), the word and appears at index position 0 and is represented by the frequency of the word (which is 2) within that particular sentence. The values in these feature vectors are also called the raw term frequencies: $tf(t,d)$ which is the number of times a term $t$, appears in a document $d$. 2.2 Assessing word relevancy via term frequency-inverse document frequency (tfidf) Often, when analysing text data, the same word will appear across both classes (in context this means, the same word would appear in positive and negative reviews). These words often don't contain useful or discrimatory information. The tfidf technique can be used to downweight frequentlty occuring words. tfidf can be defined as the product of the term frequency and the inverse document frequency $tfidf = tf(t,d) x idf(t,d)$ and is calculated like... $$idf(t,d) = log\\frac{n_d}{1+df(t,f)} $$ where $n_d$ is the total document count, $df(t,f)$ is the number of documents $d$ that contain the term $t$. The $log$ is used to ensure that low document frequencies are not given too much weight. from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tfidf . fit_transform ( vectorizer . fit_transform ( docs )) . toarray () array([[0. , 0.37632116, 0. , 0.56855566, 0.56855566, 0. , 0.46029481, 0. , 0. ], [0. , 0.37632116, 0. , 0. , 0. , 0.56855566, 0.46029481, 0. , 0.56855566], [0. , 0.4574528 , 0. , 0.3455657 , 0.3455657 , 0.3455657 , 0.55953044, 0. , 0.3455657 ], [0.65680405, 0.1713738 , 0.65680405, 0. , 0. , 0. , 0. , 0.32840203, 0. ]]) The word \"is\" appears in all 4 documents. We can see that the results of the tfid have downweighted its importance. This is evident in the 4th document where it has relatively low importance (0.171). The scikit-learn implementation is slightly different from the one above due to the smooth_idf=True argument which assigns zero weight to terms that appear in all documents. TfidfTransformer also normalises the tf-idfs directly bu applying L2-Normalisation, which returns a vector of length 1. The purpose for doing this is that the feature values become proportionate to each other. This can be verified like so... v = tfidf . fit_transform ( vectorizer . fit_transform ( docs )) . toarray () np . linalg . norm ( v [ 0 ]) 1.0 3. Cleaning Text Data remove punctuation and html markup tokenisation removing stop words The above steps are pretty typical in NLP pipeline. There are different approaches to these, ie for neural nets I've seen different encoding strategies where things like capitals , html tags, unknown words etc are replaced with tags which allows the model to capture this information which may (or may not) be useful. 3.1 Stripping Punctuation & html # source: this code comes straight from the book! # https://sebastianraschka.com/books/ import re def preprocessor ( text ): text = re . sub ( '<[^>]*>' , '' , text ) emoticons = re . findall ( '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)' , text ) text = ( re . sub ( '[\\W]+' , ' ' , text . lower ()) + ' ' . join ( emoticons ) . replace ( '-' , '' )) return text s = df . loc [ 37720 , 'review' ][: 50 ] s 'WARNING: REVIEW CONTAINS MILD SPOILERS<br /><br />' preprocessor ( s ) 'warning review contains mild spoilers' 3.2 Tokenisation Tokenisation is the process of splitting a document into individual elements (tokens). There are different strategies for doing this, ie word tokenisation, sentence tokenisation. Ontop of this are other techniques like word stemming - the process of transforming a word into it's root form ie running -> run . The NLTK library is one of many with tools to help with stemming and lemmatisation. def tokeniser ( text ): return text . split () tokeniser ( 'runners like running' ) ['runners', 'like', 'running'] from nltk.stem import PorterStemmer stemmer = PorterStemmer () def tokeniser_stemmer ( text ): return [ stemmer . stem ( word ) for word in text . split ()] tokeniser_stemmer ( 'runners like running' ) ['runner', 'like', 'run'] 3.3 Stop word removal Stop words are considered words that are extremely common and likely bear no useful or discrimatory information. Again, in the world of deep learning this is debateable and you should consider whether the task requires this and ultimately assess model performance to determine whether this is necessary. import nltk nltk . download ( 'stopwords' ) [nltk_data] Downloading package stopwords to [nltk_data] /Users/devindearaujo/nltk_data... [nltk_data] Package stopwords is already up-to-date! True from nltk.corpus import stopwords stop = stopwords . words ( 'english' ) s = 'a runner likes running and runs a lot' [ w for w in tokeniser_stemmer ( s ) if w not in stop ] ['runner', 'like', 'run', 'run', 'lot'] 4. Document Classification via logistic regression Classify movie reviews using logistic regressin, employing all of the preprocessing steps discussed above. # use grid search to find optimal model params from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline # combines TfidfTransformer & CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression # train test split X_train , X_test = df . loc [: 25000 , 'review' ] . values , df . loc [ 25000 :, 'review' ] . values y_train , y_test = df . loc [: 25000 , 'sentiment' ] . values , df . loc [ 25000 :, 'sentiment' ] . values 4.1 Finding optimal model params via GridSearchCV models parameter available to use with Grid Search... lr = LogisticRegression ( solver = 'liblinear' ) lr . get_params () {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False} tfidf = TfidfVectorizer ( strip_accents = None , lowercase = False , preprocessor = None ) # param grid param_grid = [ { 'vect__ngram_range' : [( 1 , 1 )], 'vect__stop_words' : [ None ], 'vect__tokenizer' : [ tokeniser , tokeniser_stemmer ], 'clf__penalty' : [ 'l2' ], 'clf__C' : [ 1. , 10. ] }, { 'vect__ngram_range' : [( 1 , 1 )], 'vect__stop_words' : [ stop , None ], 'vect__tokenizer' : [ tokeniser ], 'vect__use_idf' : [ False ], 'vect__norm' : [ None ], 'clf__penalty' : [ 'l2' ], 'clf__C' : [ 1. , 10. ] } ] # pipeline lr_tfidf = Pipeline ([ ( 'vect' , tfidf ), ( 'clf' , LogisticRegression ( solver = 'liblinear' )) ]) gs_lr_tfidf = GridSearchCV ( lr_tfidf , param_grid , scoring = 'accuracy' , cv = 5 , verbose = 2 , n_jobs =- 1 ) gs_lr_tfidf . fit ( X_train , y_train ) Fitting 5 folds for each of 8 candidates, totalling 40 fits #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} GridSearchCV(cv=5, estimator=Pipeline(steps=[('vect', TfidfVectorizer(lowercase=False)), ('clf', LogisticRegression(solver='liblinear'))]), n_jobs=-1, param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>, <function tokeniser_stemmer at 0x118344310>]}, {... 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', ...], None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>], 'vect__use_idf': [False]}], scoring='accuracy', verbose=2) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. GridSearchCV GridSearchCV(cv=5, estimator=Pipeline(steps=[('vect', TfidfVectorizer(lowercase=False)), ('clf', LogisticRegression(solver='liblinear'))]), n_jobs=-1, param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>, <function tokeniser_stemmer at 0x118344310>]}, {... 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', ...], None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>], 'vect__use_idf': [False]}], scoring='accuracy', verbose=2) estimator: Pipeline Pipeline(steps=[('vect', TfidfVectorizer(lowercase=False)), ('clf', LogisticRegression(solver='liblinear'))]) TfidfVectorizer TfidfVectorizer(lowercase=False) LogisticRegression LogisticRegression(solver='liblinear') gs_lr_tfidf . best_params_ {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function __main__.tokeniser(text)>} print ( f 'Average CV Accuracy: { gs_lr_tfidf . best_score_ : .3f } ' ) Average CV Accuracy: 0.888 Using the best estimator, check classification accuracy on the training set. clf = gs_lr_tfidf . best_estimator_ print ( f 'Test Accuracy: { clf . score ( X_test , y_test ) : .3f } ' ) Test Accuracy: 0.893 4.2 Updating the Pipeline with best parameters The results demonstrate that the logistic regression model can predict whether a movie is positive or negative with 86% accuracy. Using the best parameters, retrain the logistic regression model. The lr_tfidf pipeline can be updated using the set_params method and passing in the best_params_ from gs_lr_tfidf # create inference pipeline & # update tfidf and logistic regression params inf_pl = lr_tfidf . set_params ( ** gs_lr_tfidf . best_params_ ) # refit with best params inf_pl . fit ( X_train , y_train ) # score on training inf_pl . score ( X_train , y_train ) 0.9967112810707457 print ( f \"Test Accuracy: { inf_pl . score ( X_test , y_test ) : .3f } \" ) Test Accuracy: 0.893 # check on some random text s = np . array ([ \"\"\"terminator 2 was a horrible movie. the effects were good, \\n but i just couldn't get onboard with Robert Patrick's character\"\"\" ]) inf_pl . predict ( s ) array([0]) 5. Topic Modeling with Latent Dirichlet Allocation (LDA) Broadly speaking, topic modeling describes a method for assigning topics to unlabelled documents. For example, categorising a large text corpus of newspaper articles, or wiki pages. This can also be considdered a clustering task - assigning a label to simmilar sets of items, here, the items are documents. LDA is not to be confused with the matrix decomposition method Linear Discriminant Analysis, also abbreviated... to LDA. Latent Dirichlet Allocation ( LDA ) is a generative probabilistic model that aims to find groups of words that frequently appear together across a corpus of documents. This works on the assumption that each document is made up of mixtures of different words. The words that appear together often, become topics. The input to an LDA model is a bag-of-words model. Given this, LDA decomposes it into two new matrices.. - a document-to-topic matrix - a word-to-topic matrix The decompostion works in such a way that we are able to reconstruct (with the lowest possible error) the original matrix by multiplying the two latent feature matrices together. The downside to LDA, is that the number of topics is a hyperparameter, that must be specified manually beforehand. 5.1 Bag-of-words on Movie Reviews Fit a bag-of-words model using CountVectorizer on the movie reviews data. We can exclude words that appear too frequently across documents by setting max_df to 10%. The dimensionality of tha dataset can be controlled using the max_features argument, here 5000 is chosen arbitrarily. from sklearn.feature_extraction.text import CountVectorizer vect = CountVectorizer ( stop_words = 'english' , max_df = .1 , max_features = 5000 ) X = vect . fit_transform ( df [ 'review' ] . values ) 5.1 Fitting the LDA model With a total of 10 topics... from sklearn.decomposition import LatentDirichletAllocation lda = LatentDirichletAllocation ( n_components = 10 , random_state = 123 , learning_method = 'batch' , n_jobs =- 1 ) X_topics = lda . fit_transform ( X ) lda . components_ . shape (10, 5000) n_top_words = 6 feature_names = vect . get_feature_names_out () for topic_idx , topic in enumerate ( lda . components_ ): print ( f 'Topic { ( topic_idx + 1 ) } : ' ) print ( ' ' . join ([ feature_names [ i ] for i in topic . argsort () \\ [: - n_top_words - 1 : - 1 ]])) Topic 1 : worst minutes script awful stupid terrible Topic 2 : family mother father children girl women Topic 3 : war american dvd music tv history Topic 4 : human audience cinema art sense feel Topic 5 : police guy car dead murder goes Topic 6 : horror house sex blood girl woman Topic 7 : role performance comedy actor plays performances Topic 8 : series episode episodes tv season original Topic 9 : book version original read effects fi Topic 10 : action fight guy guys fun cool based on the most important words for each topic we can make a general assumption about the review topics... generally terrible movie reviews movies about families history/war movies art/arthouse movies crime films horror films comedy films tv series or shows movies based on books action movies To confirm our assumptions, we can print out sections of reviews from a particular category, say crime films. horror_idx = X_topics [:, 5 ] . argsort ()[:: - 1 ] # sort descending for iter_idx , movie_idx in enumerate ( horror_idx [: 3 ]): print ( f ' \\n Horror movie # { ( iter_idx + 1 ) } :' ) print ( df [ 'review' ] . iloc [ movie_idx ][: 300 ], '...' ) Horror movie #1: <br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ... Horror movie #2: Before I talk about the ending of this film I will talk about the plot. Some dude named Gerald breaks his engagement to Kitty and runs off to Craven Castle in Scotland. After several months Kitty and her aunt venture off to Scottland. Arriving at Craven Castle Kitty finds that Gerald has aged and he ... Horror movie #3: This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ... comedy_idx = X_topics [:, 6 ] . argsort ()[:: - 1 ] # sort descending for iter_idx , movie_idx in enumerate ( comedy_idx [: 3 ]): print ( f ' \\n Comedy movie # { ( iter_idx + 1 ) } :' ) print ( df [ 'review' ] . iloc [ movie_idx ][: 300 ], '...' ) Comedy movie #1: From producer/writer/Golden Globe nominated director James L. Brooks (Terms of Endearment, As Good as It Gets) this is a really good satirical comedy film showing behind the scenes in the life of a news reporter/anchor/journalist or producer might be like. Basically Jane Craig (Oscar and Golden Glob ... Comedy movie #2: THE SUNSHINE BOYS was the hilarious 1975 screen adaptation of Neil Simon's play about a retired vaudevillian team, played by Walter Matthau and George Burns, who had a very bitter breakup and have been asked to reunite one more time for a television special or something like that. The problem is tha ... Comedy movie #3: As far as I know the real guy that the main actor is playing saw his performance and said it was an outstanding portrayal, I'd agree with him. This is a fantastic film about a quite gifted boy/man with a special body part helping him. Oscar and BAFTA winning, and Golden Globe nominated Daniel Day-Le ... Conclusion In the following notebook we can see how even a vanilla implementation of document classification with logistic regression is able to accurately predict whether a review is positive or negative. Following this, using LDA is an effective method for classifying documents based on topics extracted from the raw text input.","title":"01 Topic Modeling"},{"location":"machine%20learning/01%20Topic%20Modeling/#applying-machine-learning-to-sentiment-analysis-and-topic-modeling","text":"This notebook will explore two topics from Natural Language Processing. The first, sentiment analysis , where we will use machine learing to classify documents based on their positive or negative sentiment. Followed by topic modeling , where we will extract the main topics from these documents. We will be working with the IMDB movie reviews data set containing 50,000 reviews. topics covered - data cleaning and processing - feature axtraction from text - training a classifyer on positive and negative sentiment - topic modeling with LDA This notebook is based on code and material from the excellent book by S. Raschka Machine Learning with PyTorch and Scikit-Learn import numpy as np import pandas as pd import tarfile #import os #import sys from tqdm import tqdm from pathlib import Path p = Path . cwd ()","title":"Applying Machine Learning to Sentiment Analysis and Topic Modeling"},{"location":"machine%20learning/01%20Topic%20Modeling/#1-data-cleaning-and-preprocessing","text":"The IMDB data set was produced by Andrew Mass and others (Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).) and contains 50,000 polar movie reviews, labeled either positive or negative. data can be downloaded from here with tarfile . open ( 'data/aclImdb_v1.tar.gz' , 'r:gz' ) as tar : tar . extractall () basepath = p / 'data/aclImdb' labels = { 'pos' : 1 , 'neg' : 0 } pbar = tqdm ( range ( 50000 )) df = pd . DataFrame () for s in ( 'test' , 'train' ): for l in ( 'pos' , 'neg' ): path = basepath / s / l for file in path . iterdir (): with open ( path / file , 'r' , encoding = 'utf-8' ) as infile : txt = infile . read () df = df . append ([[ txt , labels [ l ]]], ignore_index = True ) pbar . update () df . columns = [ 'review' , 'sentiment' ] 0%| | 0/50000 [00:00<?, ?it/s]/var/folders/h6/76mmjn5902lf0r8382f_r52r0000gn/T/ipykernel_56182/3956242205.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. df = df.append([[txt, labels[l]]], ignore_index=True) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 49971/50000 [02:01<00:00, 258.76it/s] df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } review sentiment 0 Based on an actual story, John Boorman shows t... 1 1 This is a gem. As a Film Four production - the... 1 2 I really like this show. It has drama, romance... 1 3 This is the best 3-D experience Disney has at ... 1 4 Of the Korean movies I've seen, only three had... 1 df . sentiment . value_counts () 1 25000 0 25000 Name: sentiment, dtype: int64 # shuffle index df = df . reindex ( np . random . permutation ( df . index )) # save for later df . to_csv ( p / 'data' / 'imdb_review_data.csv' , index = False , encoding = 'utf-8' )","title":"1. Data Cleaning and Preprocessing"},{"location":"machine%20learning/01%20Topic%20Modeling/#2-the-bag-of-words-model","text":"Before text data can be passed onto a machine learning or deep learning model, it needs to be converted into numerical form. The bag-of-words model allows us to do just this by representing text as feature vectors. The model can be summarised as follows... create a vocabulary of unique tokens (words) from the endire set of documents construct a feature vector for each document that contains the frequency count of words as they appear in each particular document. These feature vectors are usually very sparse (containing mainly zeros) since the occurrance of unique words represents only a small subset of all words.","title":"2. The Bag-of-Words Model"},{"location":"machine%20learning/01%20Topic%20Modeling/#21-from-words-to-feature-vectors","text":"Scikit-learn has implemented the CountVectorizer class that will take in an array of data (documents or sentences), and constructs the bag-of-words model for us. from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer () docs = np . array ([ 'the sun is shining' , 'the weather is sweet' , 'the sun is shining, the weather is sweet' , 'and one and one is two' ]) bag = vectorizer . fit_transform ( docs ) # list of unique words with integer indices # ie, sort alphabetically then assign index vectorizer . vocabulary_ {'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7} # let's sort these for convenience sorted ( vectorizer . vocabulary_ . items (), key = lambda x : x [ 1 ]) [('and', 0), ('is', 1), ('one', 2), ('shining', 3), ('sun', 4), ('sweet', 5), ('the', 6), ('two', 7), ('weather', 8)] let's look at the feature vectors bag . toarray () array([[0, 1, 0, 1, 1, 0, 1, 0, 0], [0, 1, 0, 0, 0, 1, 1, 0, 1], [0, 2, 0, 1, 1, 1, 2, 0, 1], [2, 1, 2, 0, 0, 0, 0, 1, 0]]) Each index position in the feature vectors corresponds to the sorted vocabulary, and represents the frequency of the word within that vector. For example... Looking at the last row ( [2, 1, 2, 0, 0, 0, 0, 1, 0] ), the word and appears at index position 0 and is represented by the frequency of the word (which is 2) within that particular sentence. The values in these feature vectors are also called the raw term frequencies: $tf(t,d)$ which is the number of times a term $t$, appears in a document $d$.","title":"2.1 From Words to Feature Vectors"},{"location":"machine%20learning/01%20Topic%20Modeling/#22-assessing-word-relevancy-via-term-frequency-inverse-document-frequency-tfidf","text":"Often, when analysing text data, the same word will appear across both classes (in context this means, the same word would appear in positive and negative reviews). These words often don't contain useful or discrimatory information. The tfidf technique can be used to downweight frequentlty occuring words. tfidf can be defined as the product of the term frequency and the inverse document frequency $tfidf = tf(t,d) x idf(t,d)$ and is calculated like... $$idf(t,d) = log\\frac{n_d}{1+df(t,f)} $$ where $n_d$ is the total document count, $df(t,f)$ is the number of documents $d$ that contain the term $t$. The $log$ is used to ensure that low document frequencies are not given too much weight. from sklearn.feature_extraction.text import TfidfTransformer tfidf = TfidfTransformer ( use_idf = True , norm = 'l2' , smooth_idf = True ) tfidf . fit_transform ( vectorizer . fit_transform ( docs )) . toarray () array([[0. , 0.37632116, 0. , 0.56855566, 0.56855566, 0. , 0.46029481, 0. , 0. ], [0. , 0.37632116, 0. , 0. , 0. , 0.56855566, 0.46029481, 0. , 0.56855566], [0. , 0.4574528 , 0. , 0.3455657 , 0.3455657 , 0.3455657 , 0.55953044, 0. , 0.3455657 ], [0.65680405, 0.1713738 , 0.65680405, 0. , 0. , 0. , 0. , 0.32840203, 0. ]]) The word \"is\" appears in all 4 documents. We can see that the results of the tfid have downweighted its importance. This is evident in the 4th document where it has relatively low importance (0.171). The scikit-learn implementation is slightly different from the one above due to the smooth_idf=True argument which assigns zero weight to terms that appear in all documents. TfidfTransformer also normalises the tf-idfs directly bu applying L2-Normalisation, which returns a vector of length 1. The purpose for doing this is that the feature values become proportionate to each other. This can be verified like so... v = tfidf . fit_transform ( vectorizer . fit_transform ( docs )) . toarray () np . linalg . norm ( v [ 0 ]) 1.0","title":"2.2 Assessing word relevancy via term frequency-inverse document frequency (tfidf)"},{"location":"machine%20learning/01%20Topic%20Modeling/#3-cleaning-text-data","text":"remove punctuation and html markup tokenisation removing stop words The above steps are pretty typical in NLP pipeline. There are different approaches to these, ie for neural nets I've seen different encoding strategies where things like capitals , html tags, unknown words etc are replaced with tags which allows the model to capture this information which may (or may not) be useful.","title":"3. Cleaning Text Data"},{"location":"machine%20learning/01%20Topic%20Modeling/#31-stripping-punctuation-html","text":"# source: this code comes straight from the book! # https://sebastianraschka.com/books/ import re def preprocessor ( text ): text = re . sub ( '<[^>]*>' , '' , text ) emoticons = re . findall ( '(?::|;|=)(?:-)?(?:\\)|\\(|D|P)' , text ) text = ( re . sub ( '[\\W]+' , ' ' , text . lower ()) + ' ' . join ( emoticons ) . replace ( '-' , '' )) return text s = df . loc [ 37720 , 'review' ][: 50 ] s 'WARNING: REVIEW CONTAINS MILD SPOILERS<br /><br />' preprocessor ( s ) 'warning review contains mild spoilers'","title":"3.1 Stripping Punctuation &amp; html"},{"location":"machine%20learning/01%20Topic%20Modeling/#32-tokenisation","text":"Tokenisation is the process of splitting a document into individual elements (tokens). There are different strategies for doing this, ie word tokenisation, sentence tokenisation. Ontop of this are other techniques like word stemming - the process of transforming a word into it's root form ie running -> run . The NLTK library is one of many with tools to help with stemming and lemmatisation. def tokeniser ( text ): return text . split () tokeniser ( 'runners like running' ) ['runners', 'like', 'running'] from nltk.stem import PorterStemmer stemmer = PorterStemmer () def tokeniser_stemmer ( text ): return [ stemmer . stem ( word ) for word in text . split ()] tokeniser_stemmer ( 'runners like running' ) ['runner', 'like', 'run']","title":"3.2 Tokenisation"},{"location":"machine%20learning/01%20Topic%20Modeling/#33-stop-word-removal","text":"Stop words are considered words that are extremely common and likely bear no useful or discrimatory information. Again, in the world of deep learning this is debateable and you should consider whether the task requires this and ultimately assess model performance to determine whether this is necessary. import nltk nltk . download ( 'stopwords' ) [nltk_data] Downloading package stopwords to [nltk_data] /Users/devindearaujo/nltk_data... [nltk_data] Package stopwords is already up-to-date! True from nltk.corpus import stopwords stop = stopwords . words ( 'english' ) s = 'a runner likes running and runs a lot' [ w for w in tokeniser_stemmer ( s ) if w not in stop ] ['runner', 'like', 'run', 'run', 'lot']","title":"3.3 Stop word removal"},{"location":"machine%20learning/01%20Topic%20Modeling/#4-document-classification-via-logistic-regression","text":"Classify movie reviews using logistic regressin, employing all of the preprocessing steps discussed above. # use grid search to find optimal model params from sklearn.model_selection import GridSearchCV from sklearn.pipeline import Pipeline # combines TfidfTransformer & CountVectorizer from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.linear_model import LogisticRegression # train test split X_train , X_test = df . loc [: 25000 , 'review' ] . values , df . loc [ 25000 :, 'review' ] . values y_train , y_test = df . loc [: 25000 , 'sentiment' ] . values , df . loc [ 25000 :, 'sentiment' ] . values","title":"4. Document Classification via logistic regression"},{"location":"machine%20learning/01%20Topic%20Modeling/#41-finding-optimal-model-params-via-gridsearchcv","text":"models parameter available to use with Grid Search... lr = LogisticRegression ( solver = 'liblinear' ) lr . get_params () {'C': 1.0, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'liblinear', 'tol': 0.0001, 'verbose': 0, 'warm_start': False} tfidf = TfidfVectorizer ( strip_accents = None , lowercase = False , preprocessor = None ) # param grid param_grid = [ { 'vect__ngram_range' : [( 1 , 1 )], 'vect__stop_words' : [ None ], 'vect__tokenizer' : [ tokeniser , tokeniser_stemmer ], 'clf__penalty' : [ 'l2' ], 'clf__C' : [ 1. , 10. ] }, { 'vect__ngram_range' : [( 1 , 1 )], 'vect__stop_words' : [ stop , None ], 'vect__tokenizer' : [ tokeniser ], 'vect__use_idf' : [ False ], 'vect__norm' : [ None ], 'clf__penalty' : [ 'l2' ], 'clf__C' : [ 1. , 10. ] } ] # pipeline lr_tfidf = Pipeline ([ ( 'vect' , tfidf ), ( 'clf' , LogisticRegression ( solver = 'liblinear' )) ]) gs_lr_tfidf = GridSearchCV ( lr_tfidf , param_grid , scoring = 'accuracy' , cv = 5 , verbose = 2 , n_jobs =- 1 ) gs_lr_tfidf . fit ( X_train , y_train ) Fitting 5 folds for each of 8 candidates, totalling 40 fits #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} GridSearchCV(cv=5, estimator=Pipeline(steps=[('vect', TfidfVectorizer(lowercase=False)), ('clf', LogisticRegression(solver='liblinear'))]), n_jobs=-1, param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>, <function tokeniser_stemmer at 0x118344310>]}, {... 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', ...], None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>], 'vect__use_idf': [False]}], scoring='accuracy', verbose=2) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. GridSearchCV GridSearchCV(cv=5, estimator=Pipeline(steps=[('vect', TfidfVectorizer(lowercase=False)), ('clf', LogisticRegression(solver='liblinear'))]), n_jobs=-1, param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'], 'vect__ngram_range': [(1, 1)], 'vect__stop_words': [None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>, <function tokeniser_stemmer at 0x118344310>]}, {... 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', ...], None], 'vect__tokenizer': [<function tokeniser at 0x11831fdc0>], 'vect__use_idf': [False]}], scoring='accuracy', verbose=2) estimator: Pipeline Pipeline(steps=[('vect', TfidfVectorizer(lowercase=False)), ('clf', LogisticRegression(solver='liblinear'))]) TfidfVectorizer TfidfVectorizer(lowercase=False) LogisticRegression LogisticRegression(solver='liblinear') gs_lr_tfidf . best_params_ {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function __main__.tokeniser(text)>} print ( f 'Average CV Accuracy: { gs_lr_tfidf . best_score_ : .3f } ' ) Average CV Accuracy: 0.888 Using the best estimator, check classification accuracy on the training set. clf = gs_lr_tfidf . best_estimator_ print ( f 'Test Accuracy: { clf . score ( X_test , y_test ) : .3f } ' ) Test Accuracy: 0.893","title":"4.1 Finding optimal model params via GridSearchCV"},{"location":"machine%20learning/01%20Topic%20Modeling/#42-updating-the-pipeline-with-best-parameters","text":"The results demonstrate that the logistic regression model can predict whether a movie is positive or negative with 86% accuracy. Using the best parameters, retrain the logistic regression model. The lr_tfidf pipeline can be updated using the set_params method and passing in the best_params_ from gs_lr_tfidf # create inference pipeline & # update tfidf and logistic regression params inf_pl = lr_tfidf . set_params ( ** gs_lr_tfidf . best_params_ ) # refit with best params inf_pl . fit ( X_train , y_train ) # score on training inf_pl . score ( X_train , y_train ) 0.9967112810707457 print ( f \"Test Accuracy: { inf_pl . score ( X_test , y_test ) : .3f } \" ) Test Accuracy: 0.893 # check on some random text s = np . array ([ \"\"\"terminator 2 was a horrible movie. the effects were good, \\n but i just couldn't get onboard with Robert Patrick's character\"\"\" ]) inf_pl . predict ( s ) array([0])","title":"4.2 Updating the Pipeline with best parameters"},{"location":"machine%20learning/01%20Topic%20Modeling/#5-topic-modeling-with-latent-dirichlet-allocation-lda","text":"Broadly speaking, topic modeling describes a method for assigning topics to unlabelled documents. For example, categorising a large text corpus of newspaper articles, or wiki pages. This can also be considdered a clustering task - assigning a label to simmilar sets of items, here, the items are documents. LDA is not to be confused with the matrix decomposition method Linear Discriminant Analysis, also abbreviated... to LDA. Latent Dirichlet Allocation ( LDA ) is a generative probabilistic model that aims to find groups of words that frequently appear together across a corpus of documents. This works on the assumption that each document is made up of mixtures of different words. The words that appear together often, become topics. The input to an LDA model is a bag-of-words model. Given this, LDA decomposes it into two new matrices.. - a document-to-topic matrix - a word-to-topic matrix The decompostion works in such a way that we are able to reconstruct (with the lowest possible error) the original matrix by multiplying the two latent feature matrices together. The downside to LDA, is that the number of topics is a hyperparameter, that must be specified manually beforehand.","title":"5. Topic Modeling with Latent Dirichlet Allocation (LDA)"},{"location":"machine%20learning/01%20Topic%20Modeling/#51-bag-of-words-on-movie-reviews","text":"Fit a bag-of-words model using CountVectorizer on the movie reviews data. We can exclude words that appear too frequently across documents by setting max_df to 10%. The dimensionality of tha dataset can be controlled using the max_features argument, here 5000 is chosen arbitrarily. from sklearn.feature_extraction.text import CountVectorizer vect = CountVectorizer ( stop_words = 'english' , max_df = .1 , max_features = 5000 ) X = vect . fit_transform ( df [ 'review' ] . values )","title":"5.1 Bag-of-words on Movie Reviews"},{"location":"machine%20learning/01%20Topic%20Modeling/#51-fitting-the-lda-model","text":"With a total of 10 topics... from sklearn.decomposition import LatentDirichletAllocation lda = LatentDirichletAllocation ( n_components = 10 , random_state = 123 , learning_method = 'batch' , n_jobs =- 1 ) X_topics = lda . fit_transform ( X ) lda . components_ . shape (10, 5000) n_top_words = 6 feature_names = vect . get_feature_names_out () for topic_idx , topic in enumerate ( lda . components_ ): print ( f 'Topic { ( topic_idx + 1 ) } : ' ) print ( ' ' . join ([ feature_names [ i ] for i in topic . argsort () \\ [: - n_top_words - 1 : - 1 ]])) Topic 1 : worst minutes script awful stupid terrible Topic 2 : family mother father children girl women Topic 3 : war american dvd music tv history Topic 4 : human audience cinema art sense feel Topic 5 : police guy car dead murder goes Topic 6 : horror house sex blood girl woman Topic 7 : role performance comedy actor plays performances Topic 8 : series episode episodes tv season original Topic 9 : book version original read effects fi Topic 10 : action fight guy guys fun cool based on the most important words for each topic we can make a general assumption about the review topics... generally terrible movie reviews movies about families history/war movies art/arthouse movies crime films horror films comedy films tv series or shows movies based on books action movies To confirm our assumptions, we can print out sections of reviews from a particular category, say crime films. horror_idx = X_topics [:, 5 ] . argsort ()[:: - 1 ] # sort descending for iter_idx , movie_idx in enumerate ( horror_idx [: 3 ]): print ( f ' \\n Horror movie # { ( iter_idx + 1 ) } :' ) print ( df [ 'review' ] . iloc [ movie_idx ][: 300 ], '...' ) Horror movie #1: <br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ... Horror movie #2: Before I talk about the ending of this film I will talk about the plot. Some dude named Gerald breaks his engagement to Kitty and runs off to Craven Castle in Scotland. After several months Kitty and her aunt venture off to Scottland. Arriving at Craven Castle Kitty finds that Gerald has aged and he ... Horror movie #3: This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ... comedy_idx = X_topics [:, 6 ] . argsort ()[:: - 1 ] # sort descending for iter_idx , movie_idx in enumerate ( comedy_idx [: 3 ]): print ( f ' \\n Comedy movie # { ( iter_idx + 1 ) } :' ) print ( df [ 'review' ] . iloc [ movie_idx ][: 300 ], '...' ) Comedy movie #1: From producer/writer/Golden Globe nominated director James L. Brooks (Terms of Endearment, As Good as It Gets) this is a really good satirical comedy film showing behind the scenes in the life of a news reporter/anchor/journalist or producer might be like. Basically Jane Craig (Oscar and Golden Glob ... Comedy movie #2: THE SUNSHINE BOYS was the hilarious 1975 screen adaptation of Neil Simon's play about a retired vaudevillian team, played by Walter Matthau and George Burns, who had a very bitter breakup and have been asked to reunite one more time for a television special or something like that. The problem is tha ... Comedy movie #3: As far as I know the real guy that the main actor is playing saw his performance and said it was an outstanding portrayal, I'd agree with him. This is a fantastic film about a quite gifted boy/man with a special body part helping him. Oscar and BAFTA winning, and Golden Globe nominated Daniel Day-Le ...","title":"5.1 Fitting the LDA model"},{"location":"machine%20learning/01%20Topic%20Modeling/#conclusion","text":"In the following notebook we can see how even a vanilla implementation of document classification with logistic regression is able to accurately predict whether a review is positive or negative. Following this, using LDA is an effective method for classifying documents based on topics extracted from the raw text input.","title":"Conclusion"},{"location":"machine%20learning/02%20Regression%20Analysis/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Regression Analysis for Continuous Target Variables Unlike classification models, regression models are used to predict target variables on a continuous scale and for understanding relationships between variables, analysing trends and even making forecasts which makes them valuable within industry. Univariate Linear Regression Linear regression makes use of single or multiple exogenous or explanatory variables and models their relationship with a continuous target variable. Univariate (or simple) linear regression is the most basic form, and uses only a single explanatory variable as a predictor of the target variable. A univariate model can be described as follows... $$y = w_1x +b$$ Where $w_1$, the weight coefficient, represents the slope of the line, the bias unit $b$ represents the y intercept . The goal is to optimise this equation and learn the parameters ($w_1$ and $b$) to produce a line the best describes the relationship between predictor $X$ and target $y$. This optimised equation can then be used to predict new variables that are outside the observed training samples. The best fitting line is called the regression line . Multiple Linear Regression The linear regression model can be generalised to multiple explanatory variables, where the equation is extended to... $$y = w_1x_1 + ... + w_mx_m + b = \\sum_{i=1}^{m} w_ix_i + b = W^{T}x + b$$ To explore linear models further, let's look at some data. 1. Data Analysis The Ames housing dataset has been produced by Dean De Cock in 2011. It contains information about individual residential properties in Ames, Iowa. Documentation for this data can be found here with information on all 80 features. For simplicity, a subset of the variables will be used... Columns Overall Qual (Ordinal): Rates the overall material and finish of the house Overall Cond (Ordinal): Rates the overall condition of the house Gr Liv Area (Continuous): Above grade (ground) living area square feet Central Air (Nominal): Central air conditioning Total Bsmt SF (Continuous): Total square feet of basement area SalePrice (Continuous): Sale price $$ import pandas as pd import numpy as np import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' url = 'http://jse.amstat.org/v19n3/decock/AmesHousing.txt' cols = [ 'Overall Qual' , 'Overall Cond' , 'Gr Liv Area' , 'Central Air' , 'Total Bsmt SF' , 'SalePrice' ] df = pd . read_csv ( url , sep = ' \\t ' , usecols = cols ) # clean up column labels df . columns = [ c . lower () . replace ( ' ' , '_' ) for c in cols ] # encode \"central air\" # Y=1, N=0 df [ 'central_air' ] = df [ 'central_air' ] . map ({ 'Y' : 1 , 'N' : 0 }) # drop NANs df = df . dropna ( axis = 0 ) df . to_csv ( 'data/ames_housing_data.csv' , index = False ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } overall_qual overall_cond gr_liv_area central_air total_bsmt_sf saleprice 0 6 5 1080.0 1 1656 215000 1 5 6 882.0 1 896 105000 2 6 6 1329.0 1 1329 172000 3 7 5 2110.0 1 2110 244000 4 5 5 928.0 1 1629 189900 1.1 Visual inspection import seaborn as sns g = sns . PairGrid ( df ) g . map_diag ( sns . histplot ) g . map_offdiag ( sns . scatterplot ) g . add_legend (); looking at the pairplot, it is easy to see that somewhat linear relationships exist between total_bsmt_sf and salesprice , or between gr_liv_area and salesprice . Looking at the histograms for all three variables, they appear to be skewed by outliers. Linear regression models do not require that variables are normally distributed, unless analysis for certain statistics or hypothesis tests are being conducted. 1.2 Correlation The correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficients abbreviated to Pearson's r, which is a measure of linear dependence between pairs of features. Coefficients range between -1 (perfect negative correlation), to 0 (no correlation) to 1 (perfect positive). The Pearsons correlation coefficient can be calculated as the covariance between two variables (x and y) divided by their standard deviation. $$\\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y}$$ cm = np . corrcoef ( df . T ) f , ax = plt . subplots ( figsize = ( 5 , 4 )) sns . heatmap ( cm , annot = True , linewidths = .5 , ax = ax , xticklabels = df . T . index , yticklabels = df . T . index ); Out of the continuous variables, total_bsmt_sf and gr_liv_area have the stronges correlation with the target variable, saleprice . 1.3 Fitting an OLS linear model from scratch Ordinary least squares (OLS) is a method for estimmating the parameters of the linear regression line that minimises the sum of the squared error between training examples. There is a relationship between linear regression and the Adaptive Linear Neuron (Adaline) mode. The Adeline model uses a linear activation function followed by a threshold function for the task of classification. By dropping the threshold function, we can use Adaline to solve OLS regression. This means gradient descent can be used to minimise the mean squared error loss function defined as... $$L(w,b) = \\frac{1}{2n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y^{(i)}})^2$$ Where $\\hat{y}$ is the predicted value $\\hat{y} = W^Tx + b$. # based on implementation from the excellent book # https://sebastianraschka.com/books/ class LinearRegressionGD (): def __init__ ( self , lr = 0.01 , n_iter = 50 , random_state = 1 ): self . lr = lr self . n_iter = n_iter self . random_state = random_state def fit ( self , X , y ): rgen = np . random . RandomState ( self . random_state ) self . w_ = rgen . normal ( loc = 0. , scale = 1. , size = X . shape [ 1 ]) self . b_ = np . array ([ 0. ]) self . losses_ = [] for i in range ( self . n_iter ): out = self . net_input ( X ) errors = ( y - out ) self . w_ += self . lr * 2.0 * X . T . dot ( errors ) / X . shape [ 0 ] self . b_ += self . lr * 2.0 * errors . mean () loss = ( errors ** 2 ) . mean () self . losses_ . append ( loss ) return self def net_input ( self , X ): return np . dot ( X , self . w_ ) + self . b_ def predict ( self , X ): return self . net_input ( X ) X = df [[ 'total_bsmt_sf' ]] . values y = df [[ 'saleprice' ]] . values # standardise input and target from sklearn.preprocessing import StandardScaler sc_x , sc_y = StandardScaler (), StandardScaler () X_std , y_std = sc_x . fit_transform ( X ), sc_y . fit_transform ( y ) . flatten () lr = LinearRegressionGD ( lr = 0.1 ) lr . fit ( X_std , y_std ) <__main__.LinearRegressionGD at 0x11d3e4a30> plt . plot ( range ( 1 , lr . n_iter + 1 ), lr . losses_ ) plt . ylabel ( 'MSE' ) plt . xlabel ( 'Epoch' ); def regression_plot ( X , y , model ): \"\"\" plot the line of best fit from a linear regression model \"\"\" fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax . scatter ( X , y , alpha = .6 ) ax . plot ( X , model . predict ( X ), c = 'red' , lw = 1 ) regression_plot ( X_std , y_std , lr ) plt . xlabel ( 'Total square feet of basement area' ) plt . ylabel ( 'Sale Price' ); # model coefficients print ( f 'Slope: { lr . w_ [ 0 ] : .3f } ' ) print ( f 'Intercept: { lr . b_ [ 0 ] : .3f } ' ) Slope: 0.707 Intercept: -0.000 This vanilla implementation demonstrated how an OLS model can be fit using SGD, which iteratively updated the weight and bias paramenters by minimising the squared errod between the predicted and actual values. Sklearn provides additional features to assess model fit and accuracy from sklearn.linear_model import LinearRegression slr = LinearRegression () slr . fit ( X_std , y_std ) y_pred = slr . predict ( X_std ) # model coefficients print ( f 'Slope: { slr . coef_ [ 0 ] . item () : .3f } ' ) print ( f 'Intercept: { slr . intercept_ : .3f } ' ) Slope: 0.707 Intercept: -0.000 regression_plot ( X_std , y_std , slr ) plt . xlabel ( 'Total square feet of basement area' ) plt . ylabel ( 'Sale Price' ); The results match the from-scratch implementation. In both models, the presence of outliers has pulled the line away from what looks like could be a better fit (with a slighlty steeper slope). 2. RANSAC a robust regression model In linear regression, outliers can have a big effect on the model fit by skewing the estimated model coefficients away from what could otherwise be the \"best fit\". Removing outliers involves both judgement and domain knowledge. There is an alternative to outlier removal: The RANSA (RANdom SAmple Consensus) model deals with outliers by fitting the model to a subset of the data (inliers). The model is first fitted on subset of data (inliers), then, all other samples are are tested against these inliers based on a user-defined tolerance. The model is again fitted on the new subset of data. The error is then estimated and the algorithm is stopped if the performance reaches a user defined threshold or if the fixed number of iterations has been reached. from sklearn.linear_model import RANSACRegressor ransac = RANSACRegressor ( LinearRegression (), max_trials = 100 , min_samples = .95 , residual_threshold = None , random_state = 123 ) ransac . fit ( X , y ) #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} RANSACRegressor(estimator=LinearRegression(), min_samples=0.95, random_state=123) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RANSACRegressor RANSACRegressor(estimator=LinearRegression(), min_samples=0.95, random_state=123) estimator: LinearRegression LinearRegression() LinearRegression LinearRegression() Here, a linear model was fit with the RANSAC algorithm so that the minimum number of training examples used was 95% of the data ( min_samples=.95 ). The algorithm uses the median absolute deviation by default to select the inliers ( residual_threshold=None ). As expected, the model coefficients differ from the previous linear model. print ( f 'Slope: { ransac . estimator_ . coef_ [ 0 ] . item () : .3f } ' ) print ( f 'Intercept: { ransac . estimator_ . intercept_ [ 0 ] : .3f } ' ) Slope: 106.348 Intercept: 20190.093 Visualising the results of the model can be useful for understanding how it all works. inlier_mask = ransac . inlier_mask_ outlier_mask = np . logical_not ( inlier_mask ) line_X = np . arange ( 3 , 10 , 1 ) line_y = ransac . predict ( line_X [:, np . newaxis ]) # plot inliers plt . scatter ( X [ inlier_mask ], y [ inlier_mask ], c = 'steelblue' , edgecolor = 'white' , marker = 'o' , label = 'Inliers' ) # plot outliers plt . scatter ( X [ outlier_mask ], y [ outlier_mask ], c = 'green' , edgecolor = 'white' , marker = 's' , label = 'Outliers' ) # line of fit plt . plot ( line_X , line_y , color = 'red' , lw = 2 ) plt . xlabel ( 'Total Basement Square Footage' ) plt . ylabel ( 'Sales Price in U.S. dollars' ) plt . legend ( loc = 'upper left' ) plt . tight_layout (); To identify fewer outliers, the residual_threshold can be set > than the median absolute deviation. $${\\displaystyle \\operatorname {MAD} =\\operatorname {median} (|X_{i}-{\\tilde {X}}|)}$$ where $$\\tilde {X} = \\operatorname {median}(X)$$ def median_absolute_deviation ( data ): return np . median ( np . abs ( data - np . median ( data ))) median_absolute_deviation ( y ) 37000.0 3. Evaluating the performance of linear regression models The goal of this section is to go beyond univariate models by introducing additional exogenous variables. However, in doing so it is not then possible to visualise the regression line (or hyperplane) using a two dimensional plot. Alternatively, residual plots are commonly used to diagnose regression models. They can be used to detect non-linearity and outliers, and to assess whether the errors are randomly distributed. # train test split from sklearn.model_selection import train_test_split target = 'saleprice' features = df . columns [ df . columns != target ] X = df [ features ] . values y = df [ target ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = .3 , random_state = 123 ) # model mlr = LinearRegression () # fit mlr . fit ( X_train , y_train ) # predict y_train_pred = mlr . predict ( X_train ) y_test_pred = mlr . predict ( X_test ) # straight from the book! # max values for hlines x_max = np . max ([ np . max ( y_train_pred ), np . max ( y_test_pred )]) x_min = np . min ([ np . min ( y_train_pred ), np . min ( y_test_pred )]) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 7 , 3 ), sharey = True ) ax1 . scatter ( y_test_pred , y_test_pred - y_test , c = 'green' , marker = 's' , edgecolor = 'white' , label = 'Test data' ) ax2 . scatter ( y_train_pred , y_train_pred - y_train , c = 'steelblue' , marker = 'o' , edgecolor = 'white' , label = 'Training data' ) ax1 . set_ylabel ( 'Residuals' ) for ax in ( ax1 , ax2 ): ax . set_xlabel ( 'Predicted values' ) ax . legend ( loc = 'upper left' ) ax . hlines ( y = 0 , xmin = x_min , xmax = x_max , color = 'red' , lw = 1.5 ) plt . tight_layout () plt . show () A perfect prediction would result in the residuals sitting at exactly zero. A good model would also result in the residuals being randomly scattered around 0. When this is not the case (as can be seen above) it means that the model has not captured some explanatory information. This information has leaked into the residuals. ax = df [[ 'total_bsmt_sf' , 'gr_liv_area' ]] . boxplot ( figsize = ( 6 , 8 )) ax . set_title ( 'Example of outliers' ); 4. Modeling non-linear relationships - polynomial regression Given the information learned above, try constructing polynomial features and removing outliers to see if a better fit can be achieved. To illustrate, overall_qual will be used as the relationship to saleprice is clearly non-linear. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) plt . scatter ( 'overall_qual' , 'saleprice' , data = df , alpha = .5 ) plt . xlabel ( 'overall quality' ) plt . ylabel ( 'sales price' ); # set X, y X = df [ 'overall_qual' ] . values . reshape ( - 1 , 1 ) y = df [ 'saleprice' ] . values from sklearn.preprocessing import PolynomialFeatures # fit regression model lm = LinearRegression () # feature engineering (quadratic, cubic) quadr = PolynomialFeatures ( degree = 2 ) cubic = PolynomialFeatures ( degree = 3 ) X_quad = quadr . fit_transform ( X ) X_cubic = cubic . fit_transform ( X ) # fit linear X_fit = np . arange ( X . min () - 1 , X . max () + 2 , 1 )[:, np . newaxis ] regr = lm . fit ( X , y ) y_lin_fit = regr . predict ( X_fit ) linear_r2 = r2_score ( y , regr . predict ( X )) # fit quadratic regr = lm . fit ( X_quad , y ) y_quad_fit = regr . predict ( quadr . fit_transform ( X_fit )) quad_r2 = r2_score ( y , regr . predict ( X_quad )) # fit cubic regr = lm . fit ( X_cubic , y ) y_cubic_fit = regr . predict ( cubic . fit_transform ( X_fit )) cubic_r2 = r2_score ( y , regr . predict ( X_cubic )) fig , ax = plt . subplots ( figsize = ( 10 , 8 )) plt . scatter ( X , y , label = 'Training points' , color = 'lightgrey' , alpha = .6 ) # linear regression line plt . plot ( X_fit , y_lin_fit , label = f 'Linear (d=1), $R^2$= { linear_r2 : .2f } ' , color = 'steelblue' , lw = 2 , linestyle = ':' ); # cubic regression line plt . plot ( X_fit , y_cubic_fit , label = f 'Cubic (d=2), $R^2$= { cubic_r2 : .2f } ' , color = 'red' , lw = 2 , linestyle = '-' ); # quadratic regression line plt . plot ( X_fit , y_quad_fit , label = f 'Quadratic (d=3), $R^2$= { quad_r2 : .2f } ' , color = 'green' , lw = 2 , linestyle = '--' ); plt . legend ( loc = 'upper left' ) plt . xlabel ( 'overall quality' ) plt . ylabel ( 'sales price' );","title":"02 Regression Analysis"},{"location":"machine%20learning/02%20Regression%20Analysis/#regression-analysis-for-continuous-target-variables","text":"Unlike classification models, regression models are used to predict target variables on a continuous scale and for understanding relationships between variables, analysing trends and even making forecasts which makes them valuable within industry.","title":"Regression Analysis for Continuous Target Variables"},{"location":"machine%20learning/02%20Regression%20Analysis/#univariate-linear-regression","text":"Linear regression makes use of single or multiple exogenous or explanatory variables and models their relationship with a continuous target variable. Univariate (or simple) linear regression is the most basic form, and uses only a single explanatory variable as a predictor of the target variable. A univariate model can be described as follows... $$y = w_1x +b$$ Where $w_1$, the weight coefficient, represents the slope of the line, the bias unit $b$ represents the y intercept . The goal is to optimise this equation and learn the parameters ($w_1$ and $b$) to produce a line the best describes the relationship between predictor $X$ and target $y$. This optimised equation can then be used to predict new variables that are outside the observed training samples. The best fitting line is called the regression line .","title":"Univariate Linear Regression"},{"location":"machine%20learning/02%20Regression%20Analysis/#multiple-linear-regression","text":"The linear regression model can be generalised to multiple explanatory variables, where the equation is extended to... $$y = w_1x_1 + ... + w_mx_m + b = \\sum_{i=1}^{m} w_ix_i + b = W^{T}x + b$$ To explore linear models further, let's look at some data.","title":"Multiple Linear Regression"},{"location":"machine%20learning/02%20Regression%20Analysis/#1-data-analysis","text":"The Ames housing dataset has been produced by Dean De Cock in 2011. It contains information about individual residential properties in Ames, Iowa. Documentation for this data can be found here with information on all 80 features. For simplicity, a subset of the variables will be used... Columns Overall Qual (Ordinal): Rates the overall material and finish of the house Overall Cond (Ordinal): Rates the overall condition of the house Gr Liv Area (Continuous): Above grade (ground) living area square feet Central Air (Nominal): Central air conditioning Total Bsmt SF (Continuous): Total square feet of basement area SalePrice (Continuous): Sale price $$ import pandas as pd import numpy as np import matplotlib.pyplot as plt % config InlineBackend . figure_format = 'retina' url = 'http://jse.amstat.org/v19n3/decock/AmesHousing.txt' cols = [ 'Overall Qual' , 'Overall Cond' , 'Gr Liv Area' , 'Central Air' , 'Total Bsmt SF' , 'SalePrice' ] df = pd . read_csv ( url , sep = ' \\t ' , usecols = cols ) # clean up column labels df . columns = [ c . lower () . replace ( ' ' , '_' ) for c in cols ] # encode \"central air\" # Y=1, N=0 df [ 'central_air' ] = df [ 'central_air' ] . map ({ 'Y' : 1 , 'N' : 0 }) # drop NANs df = df . dropna ( axis = 0 ) df . to_csv ( 'data/ames_housing_data.csv' , index = False ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } overall_qual overall_cond gr_liv_area central_air total_bsmt_sf saleprice 0 6 5 1080.0 1 1656 215000 1 5 6 882.0 1 896 105000 2 6 6 1329.0 1 1329 172000 3 7 5 2110.0 1 2110 244000 4 5 5 928.0 1 1629 189900","title":"1. Data Analysis"},{"location":"machine%20learning/02%20Regression%20Analysis/#11-visual-inspection","text":"import seaborn as sns g = sns . PairGrid ( df ) g . map_diag ( sns . histplot ) g . map_offdiag ( sns . scatterplot ) g . add_legend (); looking at the pairplot, it is easy to see that somewhat linear relationships exist between total_bsmt_sf and salesprice , or between gr_liv_area and salesprice . Looking at the histograms for all three variables, they appear to be skewed by outliers. Linear regression models do not require that variables are normally distributed, unless analysis for certain statistics or hypothesis tests are being conducted.","title":"1.1 Visual inspection"},{"location":"machine%20learning/02%20Regression%20Analysis/#12-correlation","text":"The correlation matrix is a square matrix that contains the Pearson product-moment correlation coefficients abbreviated to Pearson's r, which is a measure of linear dependence between pairs of features. Coefficients range between -1 (perfect negative correlation), to 0 (no correlation) to 1 (perfect positive). The Pearsons correlation coefficient can be calculated as the covariance between two variables (x and y) divided by their standard deviation. $$\\rho = \\frac{\\text{cov}(X,Y)}{\\sigma_x \\sigma_y}$$ cm = np . corrcoef ( df . T ) f , ax = plt . subplots ( figsize = ( 5 , 4 )) sns . heatmap ( cm , annot = True , linewidths = .5 , ax = ax , xticklabels = df . T . index , yticklabels = df . T . index ); Out of the continuous variables, total_bsmt_sf and gr_liv_area have the stronges correlation with the target variable, saleprice .","title":"1.2 Correlation"},{"location":"machine%20learning/02%20Regression%20Analysis/#13-fitting-an-ols-linear-model-from-scratch","text":"Ordinary least squares (OLS) is a method for estimmating the parameters of the linear regression line that minimises the sum of the squared error between training examples. There is a relationship between linear regression and the Adaptive Linear Neuron (Adaline) mode. The Adeline model uses a linear activation function followed by a threshold function for the task of classification. By dropping the threshold function, we can use Adaline to solve OLS regression. This means gradient descent can be used to minimise the mean squared error loss function defined as... $$L(w,b) = \\frac{1}{2n}\\sum_{i=1}^{n}(y^{(i)} - \\hat{y^{(i)}})^2$$ Where $\\hat{y}$ is the predicted value $\\hat{y} = W^Tx + b$. # based on implementation from the excellent book # https://sebastianraschka.com/books/ class LinearRegressionGD (): def __init__ ( self , lr = 0.01 , n_iter = 50 , random_state = 1 ): self . lr = lr self . n_iter = n_iter self . random_state = random_state def fit ( self , X , y ): rgen = np . random . RandomState ( self . random_state ) self . w_ = rgen . normal ( loc = 0. , scale = 1. , size = X . shape [ 1 ]) self . b_ = np . array ([ 0. ]) self . losses_ = [] for i in range ( self . n_iter ): out = self . net_input ( X ) errors = ( y - out ) self . w_ += self . lr * 2.0 * X . T . dot ( errors ) / X . shape [ 0 ] self . b_ += self . lr * 2.0 * errors . mean () loss = ( errors ** 2 ) . mean () self . losses_ . append ( loss ) return self def net_input ( self , X ): return np . dot ( X , self . w_ ) + self . b_ def predict ( self , X ): return self . net_input ( X ) X = df [[ 'total_bsmt_sf' ]] . values y = df [[ 'saleprice' ]] . values # standardise input and target from sklearn.preprocessing import StandardScaler sc_x , sc_y = StandardScaler (), StandardScaler () X_std , y_std = sc_x . fit_transform ( X ), sc_y . fit_transform ( y ) . flatten () lr = LinearRegressionGD ( lr = 0.1 ) lr . fit ( X_std , y_std ) <__main__.LinearRegressionGD at 0x11d3e4a30> plt . plot ( range ( 1 , lr . n_iter + 1 ), lr . losses_ ) plt . ylabel ( 'MSE' ) plt . xlabel ( 'Epoch' ); def regression_plot ( X , y , model ): \"\"\" plot the line of best fit from a linear regression model \"\"\" fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax . scatter ( X , y , alpha = .6 ) ax . plot ( X , model . predict ( X ), c = 'red' , lw = 1 ) regression_plot ( X_std , y_std , lr ) plt . xlabel ( 'Total square feet of basement area' ) plt . ylabel ( 'Sale Price' ); # model coefficients print ( f 'Slope: { lr . w_ [ 0 ] : .3f } ' ) print ( f 'Intercept: { lr . b_ [ 0 ] : .3f } ' ) Slope: 0.707 Intercept: -0.000 This vanilla implementation demonstrated how an OLS model can be fit using SGD, which iteratively updated the weight and bias paramenters by minimising the squared errod between the predicted and actual values. Sklearn provides additional features to assess model fit and accuracy from sklearn.linear_model import LinearRegression slr = LinearRegression () slr . fit ( X_std , y_std ) y_pred = slr . predict ( X_std ) # model coefficients print ( f 'Slope: { slr . coef_ [ 0 ] . item () : .3f } ' ) print ( f 'Intercept: { slr . intercept_ : .3f } ' ) Slope: 0.707 Intercept: -0.000 regression_plot ( X_std , y_std , slr ) plt . xlabel ( 'Total square feet of basement area' ) plt . ylabel ( 'Sale Price' ); The results match the from-scratch implementation. In both models, the presence of outliers has pulled the line away from what looks like could be a better fit (with a slighlty steeper slope).","title":"1.3 Fitting an OLS linear model from scratch"},{"location":"machine%20learning/02%20Regression%20Analysis/#2-ransac-a-robust-regression-model","text":"In linear regression, outliers can have a big effect on the model fit by skewing the estimated model coefficients away from what could otherwise be the \"best fit\". Removing outliers involves both judgement and domain knowledge. There is an alternative to outlier removal: The RANSA (RANdom SAmple Consensus) model deals with outliers by fitting the model to a subset of the data (inliers). The model is first fitted on subset of data (inliers), then, all other samples are are tested against these inliers based on a user-defined tolerance. The model is again fitted on the new subset of data. The error is then estimated and the algorithm is stopped if the performance reaches a user defined threshold or if the fixed number of iterations has been reached. from sklearn.linear_model import RANSACRegressor ransac = RANSACRegressor ( LinearRegression (), max_trials = 100 , min_samples = .95 , residual_threshold = None , random_state = 123 ) ransac . fit ( X , y ) #sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\u25b8\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\u25be\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;} RANSACRegressor(estimator=LinearRegression(), min_samples=0.95, random_state=123) In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org. RANSACRegressor RANSACRegressor(estimator=LinearRegression(), min_samples=0.95, random_state=123) estimator: LinearRegression LinearRegression() LinearRegression LinearRegression() Here, a linear model was fit with the RANSAC algorithm so that the minimum number of training examples used was 95% of the data ( min_samples=.95 ). The algorithm uses the median absolute deviation by default to select the inliers ( residual_threshold=None ). As expected, the model coefficients differ from the previous linear model. print ( f 'Slope: { ransac . estimator_ . coef_ [ 0 ] . item () : .3f } ' ) print ( f 'Intercept: { ransac . estimator_ . intercept_ [ 0 ] : .3f } ' ) Slope: 106.348 Intercept: 20190.093 Visualising the results of the model can be useful for understanding how it all works. inlier_mask = ransac . inlier_mask_ outlier_mask = np . logical_not ( inlier_mask ) line_X = np . arange ( 3 , 10 , 1 ) line_y = ransac . predict ( line_X [:, np . newaxis ]) # plot inliers plt . scatter ( X [ inlier_mask ], y [ inlier_mask ], c = 'steelblue' , edgecolor = 'white' , marker = 'o' , label = 'Inliers' ) # plot outliers plt . scatter ( X [ outlier_mask ], y [ outlier_mask ], c = 'green' , edgecolor = 'white' , marker = 's' , label = 'Outliers' ) # line of fit plt . plot ( line_X , line_y , color = 'red' , lw = 2 ) plt . xlabel ( 'Total Basement Square Footage' ) plt . ylabel ( 'Sales Price in U.S. dollars' ) plt . legend ( loc = 'upper left' ) plt . tight_layout (); To identify fewer outliers, the residual_threshold can be set > than the median absolute deviation. $${\\displaystyle \\operatorname {MAD} =\\operatorname {median} (|X_{i}-{\\tilde {X}}|)}$$ where $$\\tilde {X} = \\operatorname {median}(X)$$ def median_absolute_deviation ( data ): return np . median ( np . abs ( data - np . median ( data ))) median_absolute_deviation ( y ) 37000.0","title":"2. RANSAC a robust regression model"},{"location":"machine%20learning/02%20Regression%20Analysis/#3-evaluating-the-performance-of-linear-regression-models","text":"The goal of this section is to go beyond univariate models by introducing additional exogenous variables. However, in doing so it is not then possible to visualise the regression line (or hyperplane) using a two dimensional plot. Alternatively, residual plots are commonly used to diagnose regression models. They can be used to detect non-linearity and outliers, and to assess whether the errors are randomly distributed. # train test split from sklearn.model_selection import train_test_split target = 'saleprice' features = df . columns [ df . columns != target ] X = df [ features ] . values y = df [ target ] . values X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = .3 , random_state = 123 ) # model mlr = LinearRegression () # fit mlr . fit ( X_train , y_train ) # predict y_train_pred = mlr . predict ( X_train ) y_test_pred = mlr . predict ( X_test ) # straight from the book! # max values for hlines x_max = np . max ([ np . max ( y_train_pred ), np . max ( y_test_pred )]) x_min = np . min ([ np . min ( y_train_pred ), np . min ( y_test_pred )]) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 7 , 3 ), sharey = True ) ax1 . scatter ( y_test_pred , y_test_pred - y_test , c = 'green' , marker = 's' , edgecolor = 'white' , label = 'Test data' ) ax2 . scatter ( y_train_pred , y_train_pred - y_train , c = 'steelblue' , marker = 'o' , edgecolor = 'white' , label = 'Training data' ) ax1 . set_ylabel ( 'Residuals' ) for ax in ( ax1 , ax2 ): ax . set_xlabel ( 'Predicted values' ) ax . legend ( loc = 'upper left' ) ax . hlines ( y = 0 , xmin = x_min , xmax = x_max , color = 'red' , lw = 1.5 ) plt . tight_layout () plt . show () A perfect prediction would result in the residuals sitting at exactly zero. A good model would also result in the residuals being randomly scattered around 0. When this is not the case (as can be seen above) it means that the model has not captured some explanatory information. This information has leaked into the residuals. ax = df [[ 'total_bsmt_sf' , 'gr_liv_area' ]] . boxplot ( figsize = ( 6 , 8 )) ax . set_title ( 'Example of outliers' );","title":"3. Evaluating the performance of linear regression models"},{"location":"machine%20learning/02%20Regression%20Analysis/#4-modeling-non-linear-relationships-polynomial-regression","text":"Given the information learned above, try constructing polynomial features and removing outliers to see if a better fit can be achieved. To illustrate, overall_qual will be used as the relationship to saleprice is clearly non-linear. fig , ax = plt . subplots ( figsize = ( 8 , 6 )) plt . scatter ( 'overall_qual' , 'saleprice' , data = df , alpha = .5 ) plt . xlabel ( 'overall quality' ) plt . ylabel ( 'sales price' ); # set X, y X = df [ 'overall_qual' ] . values . reshape ( - 1 , 1 ) y = df [ 'saleprice' ] . values from sklearn.preprocessing import PolynomialFeatures # fit regression model lm = LinearRegression () # feature engineering (quadratic, cubic) quadr = PolynomialFeatures ( degree = 2 ) cubic = PolynomialFeatures ( degree = 3 ) X_quad = quadr . fit_transform ( X ) X_cubic = cubic . fit_transform ( X ) # fit linear X_fit = np . arange ( X . min () - 1 , X . max () + 2 , 1 )[:, np . newaxis ] regr = lm . fit ( X , y ) y_lin_fit = regr . predict ( X_fit ) linear_r2 = r2_score ( y , regr . predict ( X )) # fit quadratic regr = lm . fit ( X_quad , y ) y_quad_fit = regr . predict ( quadr . fit_transform ( X_fit )) quad_r2 = r2_score ( y , regr . predict ( X_quad )) # fit cubic regr = lm . fit ( X_cubic , y ) y_cubic_fit = regr . predict ( cubic . fit_transform ( X_fit )) cubic_r2 = r2_score ( y , regr . predict ( X_cubic )) fig , ax = plt . subplots ( figsize = ( 10 , 8 )) plt . scatter ( X , y , label = 'Training points' , color = 'lightgrey' , alpha = .6 ) # linear regression line plt . plot ( X_fit , y_lin_fit , label = f 'Linear (d=1), $R^2$= { linear_r2 : .2f } ' , color = 'steelblue' , lw = 2 , linestyle = ':' ); # cubic regression line plt . plot ( X_fit , y_cubic_fit , label = f 'Cubic (d=2), $R^2$= { cubic_r2 : .2f } ' , color = 'red' , lw = 2 , linestyle = '-' ); # quadratic regression line plt . plot ( X_fit , y_quad_fit , label = f 'Quadratic (d=3), $R^2$= { quad_r2 : .2f } ' , color = 'green' , lw = 2 , linestyle = '--' ); plt . legend ( loc = 'upper left' ) plt . xlabel ( 'overall quality' ) plt . ylabel ( 'sales price' );","title":"4. Modeling non-linear relationships - polynomial regression"}]}