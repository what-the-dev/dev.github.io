{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastai 2019: notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- in fastai, the following applies (this is mainly how kaggle does it too)\n",
    "- `train` training data with labels\n",
    "- `valid` a validation set for assessing and testing. Valid DS has labels\n",
    "- `test` a test data set which has **NO** labels\n",
    "\n",
    "## Gradient Descent\n",
    "- will take our prediction and try to make it better using the `intercept` and `slope`\n",
    "- for each \"movement\", calculate the loss, if it is better we keep that\n",
    "- we do this by calculating the `derivative` (ie calculating the gradient)\n",
    "\n",
    "**Gradient Descent** is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves towards a set of parameter values that minimies the function. This is done by taking steps in the negative direction of the function gradient\n",
    "\n",
    "## Weight Decay\n",
    "- is a type of regulariztion\n",
    "- we throw away a certain % of weights for each layer of the model to stop it from learning the data it is training on.\n",
    "- Weight decay is\n",
    "    - where we take the loss function and add the sum of squared of the parameters x by some number (wd)\n",
    "    - `wd should be 0.1`\n",
    "    - according to J Howard. You should try using this when you train\n",
    "- $w_t = w_{t-1} - lr \\times {{dl}\\over{dw}_{t-1}}$\n",
    "    - update $w$ weight at epoch/time t\n",
    "    - This means, our $w_{t-1}$ weights at previous time \n",
    "    - minus learning rate $lr$ times \n",
    "    - the derivative of our $lr$ with respect to derivative of our weights $w$\n",
    "\n",
    "## Momentum\n",
    "- `0.9` is really common\n",
    "- momentum is what affects our step size when we are exploring the weight space\n",
    "- it is the exponentially weighted moving average of the gradient\n",
    "\n",
    "## RMS Prop\n",
    "- similar to momentum but is..\n",
    "    - exponentially weighted moving average of the gradient squared\n",
    "\n",
    "## Adam\n",
    "- does **both** momentumn and weight decay\n",
    "- keep track of \n",
    "    - exponentially weighted moving average of the gradient squared\n",
    "    - exponentially weighted moving average of my steps\n",
    "    - both divide by exponentially weighted moving average of the squared terms\n",
    "    - take .9 of a step in the same direct as last time\n",
    "\n",
    "\n",
    "### Image regression\n",
    "- different to classification\n",
    "    - in classification problems, we predict a discrete variable (ie category)\n",
    "    - in regression, we are trying to predict continuous variables\n",
    "- look at the 'heads' data set\n",
    "    - [lesson 3](https://www.youtube.coam/watch?v=MpZxV6DVsmM&t=619s)\n",
    "    - it is predicting a set of coordinates on an image\n",
    "    \n",
    "    \n",
    "## Loss\n",
    "- is some function of our independent variables and our weights\n",
    "- $L(x,w) = mse(\\hat{y}, y)$\n",
    "- we used MSE for eg\n",
    "    - between predictions `y_hat` and actuals `y`\n",
    "    - predictions come from running a model on those predictions, and the model contains some weights\n",
    "    - this creates `a.graf` \n",
    "    - then we add weight decay\n",
    "- $L(x,w) = mse(\\hat{y}, y) + wd \\times \\sum{w}^2$\n",
    "\n",
    "### cross entropy loss\n",
    "- this is the loss function you want when doing\n",
    "    - single label, multi-class classificaiton\n",
    "- for classification, we need a loss function where\n",
    "    - predicting the right thing confidently should have low loss\n",
    "    - predicting the wrong thing confidently should have high loss\n",
    "- sum of one hot encoded variables, times, all of your activations\n",
    "- requires `softmax` which says\n",
    "    - e to the activations, div sum of e to the activations\n",
    "    - it requires that\n",
    "    - all activations sum to 1\n",
    "    - all activations are > 0\n",
    "    - all activations are less than 1\n",
    "\n",
    "\n",
    "### Loss functions to use\n",
    "- Classification\n",
    "    - cross entropy loss\n",
    "- Regression\n",
    "    - MSE\n",
    "\n",
    "\n",
    "### data augmentation\n",
    "- called by `get_transforms` in fastai\n",
    "- particularly important to image data as the transforms are things like brightness, flipping, skewing/perspective warping, padding (zeros, border, reflection)\n",
    "    - \"reflection is nearly always better\" J Howard\n",
    "- there are transformations that your preprocessing will add to you data. \n",
    "- the purpose is \"teach\" the model that a cat is still a cat even if the image is too bright/dark/blury\n",
    "- you should assess that data to see whether the data requires augmentation\n",
    "- data augmentation creates more versions of each individual image, thereby increasing the size of your dataset, providing you with more training data\n",
    "- everytime you grab something, fastai randomly transforms it so potentially every image will look a little bit different. You can see this by plotting something a few times (check 2019 lesson 7 video around 10:30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks\n",
    "- like the neural nets we have seen before, so doing matrix multiplication but a slightly different type of matrix multiplication\n",
    "\n",
    "### Convolutional Kernal\n",
    "- explained well [here](https://setosa.io/ev/image-kernels/)\n",
    "- as the kernal passes over the image, the resulting mat mul and addition is creating a negative image\n",
    "- all a convolution can do is find edges and gradients\n",
    "- each layer takes the results of the previous to create more complex shapes (see Zyler and Fergus visualizing layers of nets)\n",
    "- each output is the result of a linear equation\n",
    "- convolutions can be implemented with matrix multiplication but we generally don't do it because it is slow\n",
    "\n",
    "### rank three tensor and kernals\n",
    "- think of a cube\n",
    "- think of a colour image as having 3 channels (R,G,B) - rank 3 tensor\n",
    "- the kernal now becomes a rank 3 kernal (3x3x3 kernal)\n",
    "- we now do an element wise mult of 27 things instead of 9\n",
    "- we then add all 27 together to end up with one number\n",
    "- there need to be 3 kernels to create rank 3 tensor as an output\n",
    "    - however we get to choose how ever many kernels we need\n",
    "    - ofter 16 in the first layer\n",
    "    - these will create 16 channels representing\n",
    "        - how much left edge, top, edge, gradient, blue etc etc\n",
    "- this is repeated many times\n",
    "    - we want to have more and more channels as we go deeper into the network\n",
    "    - this creates memory issues\n",
    "    - to avoid this we use a kernel that skips over pixels\n",
    "    - **called STRIDE 2** convolution\n",
    "    \n",
    "\n",
    "### weight tying\n",
    "- when you have multiple things with the same weight it's called weight tying\n",
    "\n",
    "### kernel size in model\n",
    "- generally speaking we start with a larger kernel first which then reduces\n",
    "- stride size will reduce the image size ie image of 224 will become 112x112 with a stride 2 conv\n",
    "\n",
    "### how to get to final output\n",
    "- for every channel in the final output, we take an average, which will give us an vector of x length\n",
    "- then we pop through a single matmul of vector of size x by the number of categories \n",
    "- this is called average pooling\n",
    "\n",
    "\n",
    "# Simple CNN\n",
    "- sequential layers\n",
    "- conv2d\n",
    "\n",
    "# ResNet\n",
    "- adds skip onnectionsd to sequential architecture\n",
    "\n",
    "# DenseNet\n",
    "- like resnet but instead of + x it concatenates\n",
    "- see lesson 7.\n",
    "- not too clear on this yet\n",
    "- dens blocks get bigger and bigger but the original layer features are still there\n",
    "- these nets are very memory intensive because of this\n",
    "- though they do have fewer parameters\n",
    "- they work really well for small data sets and for segmentation\n",
    "    - maybe for generation too??\n",
    "\n",
    "# U-Net\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Tabular Data\n",
    "- what architecture is this? It's not a `CNN` so maybe `RNN` or a linear model??\n",
    "- you need to specify you categorical and continuous variables\n",
    "- if this is a regression problem, ie you dependent variable is continuous then you need to... **this is discussed somewhere** **add notes here**\n",
    "- `Normalization`\n",
    "    - takes continuous variables, subtracts their mean and div by standard deviation (converts to 0, 1)\n",
    "- whatever you do to training, you have to do to validation set (re: pre-processing)\n",
    "- `layers=[200,100]`\n",
    "    - this is the embedding size of the last two layers??\n",
    "- Time Series Tabular Data\n",
    "    - generally **don't use RNN** for time series tabular\n",
    "    - add additional categorical variables dor date columns\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "## Collaborative filtering\n",
    "- **linear model**\n",
    "    - it's basically a regression\n",
    "    - which means we only have one layer so no point with discriminative `learning rates`\n",
    "        - so for fit, just pass in one `lr`\n",
    "- `n_factors=50`\n",
    "    - this is the width of the embedding size\n",
    "    - factors is what they call the term in this collab filtering domain.\n",
    "- `min_score` & `max_score`\n",
    "    - the min and max of the 'ratings'\n",
    "    - **replaced by `y_range`**\n",
    "    - these are the bounds where the `sigmoid` function will truncate\n",
    "        - we need to go a bit above the max \"rating\" number so that the actual number can be reached.\n",
    "        - ie if you have ratings from 1-5, you would pick a `y_range=[0, 5.5]`\n",
    "        - this is a way of improving the network by limiting the range. We want it to be as good as possible at predicting scores between 0-5 so no use allowing any numbers above ~5.\n",
    "- embedding matrix\n",
    "    - these are vectors with a baised term added on\n",
    "    - the biased term is like the score for all \"movies/product\"\n",
    "    - the biased term is a way to say some products/movies are better than others so it's not surprising that they are liked more.\n",
    "    - **an embedding means, look something up in an array**\n",
    "        - this is the same as doing a matrix product by a one hot encoded matrix\n",
    "        - embedding is a memory efficient way of doing the multiplicaiton\n",
    "- latent factors or features\n",
    "    - these are the hidden features that are revealed through training our model\n",
    "    - the bias term is a weight that basically give better items more weight, worse items less weight\n",
    "    \n",
    "\n",
    "\n",
    "-----------------\n",
    "# Classification\n",
    "### loss functions, regularization and activations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------\n",
    "\n",
    "# Reading\n",
    "- [neuralnetworks and deep learning](http://neuralnetworksanddeeplearning.com/)\n",
    "- [chroma](https://en.wikipedia.org/wiki/Chroma_feature)\n",
    "- matrix products \n",
    "    - be familiar with the output of a matrix of size x * size y = size ?\n",
    "    \n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training notes\n",
    "- `lr` of `3e-3` generally works well for the first round of training before unfreezing\n",
    "- then for second round, for the first part of the slice use 10x lower for second part of slice, then whatever `lr_finder` found for the first part of the slice\n",
    "    - `learn.fit_one_cycle(4, 3e-3)`\n",
    "    - `learn.unfreeze()`\n",
    "    - `learn.fit_one_cycle(4, slice(\"lr_finder number\",3e-4))`\n",
    "- `learn.recorder.plot_losses()` will show you the loss plotted out. \n",
    "    - You **want** to see something that goes down, then increases a bit then goes down again. That is a good sign\n",
    "    - ![lr_good](../img/good_loss.png)\n",
    "    - if it is ALWAYS going down, then you can bump your learning rate up a bit.\n",
    "- if you are overfitting\n",
    "    - add more `wd`\n",
    "\n",
    "\n",
    "## `Learner`\n",
    "- we can pass in `data`, `model`, `metrics`, `loss function`\n",
    "- it is a convenience function for us\n",
    "\n",
    "## `fit_one_cycle`\n",
    "- we use (in fastai) something like `Adam` by default\n",
    "- fit one cycle implements\n",
    "    - discriminative learning rate and learning rate annealing\n",
    "        - increase the `lr` if you are doing well, then decrease after half way\n",
    "        - start slow when exploring the weight space, then increase towards the end\n",
    "    - as `lr` increases, `momentum` decreases, then towards the end, `lr` decreases, `momentum` increases\n",
    "- \n",
    "\n",
    "\n",
    "## over and underfitting\n",
    "- **training loss should always be lower than validation loss**\n",
    "- **`lr` too high**\n",
    "    - validation loss will be very high\n",
    "        - lower the `lr`\n",
    "- **`lr` too low**\n",
    "    - error_rate will reduce but very very slowly\n",
    "    - increase `lr` a bit\n",
    "    - **training loss will be higher than validation loss**\n",
    "        - you never want this\n",
    "        - this means you are **underftting**\n",
    "        - num epochs too low or `lr` too low\n",
    "        - see [48:50](https://www.youtube.com/watch?v=ccMHJeQU4Qw&t=3219s) in lesson 2 video\n",
    "- **too few epochs**\n",
    "        - this looks dimilar to low `lr`\n",
    "        - so try more epochs first\n",
    "        - then if `lr` goes over the top, lower it\n",
    "- **too many epochs**\n",
    "    - overfitting\n",
    "    - it is really hard to overfit\n",
    "    - how to tell\n",
    "        - **error rate improves for a while, then gets worse again**\n",
    "\n",
    "    \n",
    "    \n",
    "---------  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms\n",
    "- Learning rate\n",
    "    - is the thing we mult gradient by to decide how much we update weights by\n",
    "- Epoch\n",
    "    - one complete run through all data points\n",
    "- Minibatch\n",
    "    - random bunch of points to update weights\n",
    "- SGD\n",
    "    - stochastic gradient descent\n",
    "- Model/Architecture\n",
    "    - function we are fitting the parameters to\n",
    "- Parameters / coefficients / weights\n",
    "    - the numbers we are updating \n",
    "- Afine function\n",
    "    - linear function\n",
    "    - multiply things together then add them up\n",
    "- Loss Function\n",
    "    - how far away or how close you are to the correct answer\n",
    "- ReLU\n",
    "    - is a \"filter\" where any number below 0 is cut off and set to 0\n",
    "- **Activations**\n",
    "    - numbers\n",
    "    - are the result of either a matrix multiply or an activation function such as(ReLU)\n",
    "    - sometimes called nonlinearities\n",
    "- **Parameters/Weights**\n",
    "    - numbers inside the weights that we multipy\n",
    "    - that are stored to make a calculation\n",
    "    - this is what the model learns\n",
    "    - we use gradient descent on the parameters to update them\n",
    "    - `parameters -= lr * parameters.grad`\n",
    "- Layers\n",
    "    - everything in the network that does a calculation\n",
    "    - every layer results in a set of activations\n",
    "    - Start layer\n",
    "        - input layer\n",
    "    - End layer\n",
    "        - output (final set of activations)\n",
    "- Back Propagation\n",
    "    - the process of updating the parameters with gradient descent\n",
    "- Fine Tuning\n",
    "    - Resnet34 was trained on imagenet so the final weight matrix is of len 1000 because you need to predict 1000 categories. We generally don't need to do that so that final set of weights is thrown away and replaced by 2 new weight matrices with a ReLU in between. \n",
    "    - these originally have random numbers in them\n",
    "    - this is what we train first while the start layers are frozen\n",
    "    - this ensures we don't back propagate the weights back into the initial layers\n",
    "        - this must be why when you unfreeze then re-train, you get worse before you get better. Makes sense!\n",
    "    - fastai by default splits the model into different sections and applied different learning rates to each part. this is because we don't need to train the early layers by much. So those weights won't be trained a lot.\n",
    "    - this is called using **discriminative learning rates** see Leslie Smith\n",
    "    - after unfreezing you can call\n",
    "        - `fit(epochs=1, max_lr=1e-3)`\n",
    "            - single lr throughout\n",
    "        - `fit(epochs=1, max_lr=slice(1e-3))`\n",
    "            - evenly split `lr` between layers based on divisions of 3 (ie 1e-3/3)\n",
    "        - `fit(epochs=1, max_lr=slice(1e-5,1e-3))`\n",
    "            - will apply 1e-5 to start group, then 1e-4 for middle group then 1e-3 for last layer group\n",
    "- `fit_one_cycle`\n",
    "    - `epoch` and `cyc_len` are the same\n",
    "    - both represent the number of times you scan through your items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project notes\n",
    "\n",
    "## Multilabel Classificaiton with Audio data\n",
    "\n",
    "- What labels can you predict?\n",
    "    - using a spectrogram, you could classify\n",
    "    - `key`, `scale`, 'instrument`, `tambre` etc\n",
    "    - using vaoice it could be the tone of the voice\n",
    "        - `hooty`, `squeezed`, `breathy`, `chest`, `head`, `etc`\n",
    "        \n",
    "- Metrics\n",
    "    - use `accuracy_thresh` with a selected threshold\n",
    "    - check the [video](https://www.youtube.coam/watch?v=MpZxV6DVsmM&t=619s)\n",
    "    - you need to update the accuracy metric\n",
    "\n",
    "### Creating data\n",
    "- you might want to use the actual matrices instead of images\n",
    "- pytorch has a `TensorDataset()` function that will converts any 2 tensor into a dataset. You can then use `DataBunch.create()` to create a databunch iterator\n",
    "- [lesson 5 at 1:27](https://www.youtube.com/watch?v=CJKnDu2dxOE)shows this\n",
    "\n",
    "# Reading\n",
    "### links\n",
    "- [musical freqs](https://pages.mtu.edu/~suits/notefreqs.html)\n",
    "- [ISMIR](https://www.ismir.net/)\n",
    "    - librosa creators\n",
    "- [MIR](https://musicinformationretrieval.com/index.html)\n",
    "    - [THIS IS USEFUL](https://musicinformationretrieval.com/pitch_transcription_exercise.html)\n",
    "    - **Pitch Detection**\n",
    "### Papers\n",
    "- [Detecting Musical Key With Supervised Learning](http://cs229.stanford.edu/proj2016/report/Mahieu-DetectingMusicalKeyWithSupervisedLearning-report.pdf)\n",
    "- [Deep residual learning for image recognition](https://arxiv.org/abs/1512.03385)\n",
    "    - ResBlocks and resnets\n",
    "\n",
    "- [Visualising the loss landscape of neural nets](https://arxiv.org/abs/1712.09913)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
