{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee11430f-03fc-41e1-bac1-1b88001b955b",
   "metadata": {},
   "source": [
    "# Applying Machine Learning to Sentiment Analysis and Topic Modeling\n",
    "\n",
    "\n",
    "This notebook will explore two topics from Natural Language Processing. The first, **sentiment analysis**, where we will use machine learing to classify documents based on their positive or negative sentiment. Followed by **topic modeling**, where we will extract the main topics from these documents.\n",
    "\n",
    "We will be working with the IMDB movie reviews data set containing 50,000 reviews.\n",
    "\n",
    "topics covered\n",
    "- data cleaning and processing\n",
    "- feature axtraction from text\n",
    "- training a classifyer on positive and negative sentiment\n",
    "- topic modeling with LDA\n",
    "\n",
    "This notebook is based on code and material from the excellent book by S. Raschka [Machine Learning with PyTorch and Scikit-Learn](https://sebastianraschka.com/books/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e364b023-d144-458c-b9aa-225606d2ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "\n",
    "#import os\n",
    "#import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffe4d3b-08d6-446e-a1fe-fd15ba6834f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b6500-4db8-4cc2-b074-a29c49965fb2",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning and Preprocessing\n",
    "\n",
    "The IMDB data set was produced by Andrew Mass and others *(Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).)* and contains 50,000 polar movie reviews, labeled either positive or negative. \n",
    "\n",
    "data can be downloaded from [here](https://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d9b5c5-1b8e-4134-97b7-e4407f04308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('data/aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa145285-9857-4621-b33b-651e07afc017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                         | 0/50000 [00:00<?, ?it/s]/var/folders/h6/76mmjn5902lf0r8382f_r52r0000gn/T/ipykernel_56182/3956242205.py:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append([[txt, labels[l]]], ignore_index=True)\n",
      "100%|███████████████████████████████████████████████████████████▉| 49971/50000 [02:01<00:00, 258.76it/s]"
     ]
    }
   ],
   "source": [
    "basepath = p/'data/aclImdb'\n",
    "labels = {'pos':1, 'neg':0}\n",
    "\n",
    "pbar = tqdm(range(50000))\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test','train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = basepath/s/l\n",
    "        for file in path.iterdir():\n",
    "            with open(path/file, 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            \n",
    "            pbar.update()\n",
    "            \n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021ab7eb-820d-41de-a2b8-1af9f211a56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on an actual story, John Boorman shows t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a gem. As a Film Four production - the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like this show. It has drama, romance...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the best 3-D experience Disney has at ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of the Korean movies I've seen, only three had...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  Based on an actual story, John Boorman shows t...          1\n",
       "1  This is a gem. As a Film Four production - the...          1\n",
       "2  I really like this show. It has drama, romance...          1\n",
       "3  This is the best 3-D experience Disney has at ...          1\n",
       "4  Of the Korean movies I've seen, only three had...          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28542f04-2c11-4670-a503-ebe5f7e5c814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    25000\n",
       "0    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4886e6b-4017-4f89-a11f-86d65741a028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle index\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# save for later\n",
    "df.to_csv(p/'data'/'imdb_review_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c9797e-aaad-420d-8d78-06feb5905d69",
   "metadata": {},
   "source": [
    "# 2. The Bag-of-Words Model\n",
    "\n",
    "Before text data can be passed onto a machine learning or deep learning model, it needs to be converted into numerical form. The bag-of-words model allows us to do just this by representing text as feature vectors. The model can be summarised as follows...\n",
    "\n",
    "1. create a vocabulary of unique tokens (words) from the endire set of documents\n",
    "2. construct a feature vector for each document that contains the frequency count of words as they appear in each particular document. \n",
    "\n",
    "These feature vectors are usually very sparse (containing mainly zeros) since the occurrance of unique words represents only a small subset of *all* words. \n",
    "\n",
    "## 2.1 From Words to Feature Vectors\n",
    "\n",
    "Scikit-learn has implemented the `CountVectorizer` class that will take in an array of data (documents or sentences), and constructs the bag-of-words model for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfb9b1ca-a482-4091-b1fb-d014a5ecd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'the sun is shining',\n",
    "    'the weather is sweet',\n",
    "    'the sun is shining, the weather is sweet',\n",
    "    'and one and one is two'\n",
    "])\n",
    "\n",
    "bag = vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5d91f2-1a93-45ba-a6d7-8d48b763ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 6,\n",
       " 'sun': 4,\n",
       " 'is': 1,\n",
       " 'shining': 3,\n",
       " 'weather': 8,\n",
       " 'sweet': 5,\n",
       " 'and': 0,\n",
       " 'one': 2,\n",
       " 'two': 7}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of unique words with integer indices \n",
    "# ie, sort alphabetically then assign index\n",
    "\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ed73e5d-759c-4e84-b8c3-9bfc4689dd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 0),\n",
       " ('is', 1),\n",
       " ('one', 2),\n",
       " ('shining', 3),\n",
       " ('sun', 4),\n",
       " ('sweet', 5),\n",
       " ('the', 6),\n",
       " ('two', 7),\n",
       " ('weather', 8)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's sort these for convenience\n",
    "sorted(vectorizer.vocabulary_.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a98e6-f2d0-45bd-8e4f-78db5fb81ec9",
   "metadata": {},
   "source": [
    "let's look at the feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c87f6e-f89b-4059-8e92-35259c5a66dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 1, 1, 2, 0, 1],\n",
       "       [2, 1, 2, 0, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731196a2-75e3-4983-b0ba-bffb6ed26e1a",
   "metadata": {},
   "source": [
    "Each index position in the feature vectors corresponds to the sorted vocabulary, and represents the frequency of the word within that vector. For example...\n",
    "\n",
    "Looking at the last row (`[2, 1, 2, 0, 0, 0, 0, 1, 0]`), the word `and` appears at index position `0` and is represented by the frequency of the word (which is 2) within that particular sentence.\n",
    "\n",
    "The values in these feature vectors are also called the *raw term frequencies:* $tf(t,d)$ which is the number of times a term $t$, appears in a document $d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3f467e-5f65-48cc-add1-68e8e8ee6ff7",
   "metadata": {},
   "source": [
    "## 2.2 Assessing word relevancy via term frequency-inverse document frequency (tfidf)\n",
    "\n",
    "Often, when analysing text data, the same word will appear across both classes (in context this means, the same word would appear in positive and negative reviews). These words often don't contain useful or discrimatory information. The tfidf technique can be used to downweight frequentlty occuring words. \n",
    "\n",
    "tfidf can be defined as the product of the term frequency and the inverse document frequency $tfidf = tf(t,d) x idf(t,d)$ and is calculated like...\n",
    "\n",
    "$$idf(t,d) = log\\frac{n_d}{1+df(t,f)} $$\n",
    "\n",
    "where $n_d$ is the total document count, $df(t,f)$ is the number of documents $d$ that contain the term $t$. The $log$ is used to ensure that low document frequencies are not given too much weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "639c0f04-6448-4ed5-88d3-08b9377496f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.37632116, 0.        , 0.56855566, 0.56855566,\n",
       "        0.        , 0.46029481, 0.        , 0.        ],\n",
       "       [0.        , 0.37632116, 0.        , 0.        , 0.        ,\n",
       "        0.56855566, 0.46029481, 0.        , 0.56855566],\n",
       "       [0.        , 0.4574528 , 0.        , 0.3455657 , 0.3455657 ,\n",
       "        0.3455657 , 0.55953044, 0.        , 0.3455657 ],\n",
       "       [0.65680405, 0.1713738 , 0.65680405, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.32840203, 0.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "tfidf.fit_transform(vectorizer.fit_transform(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac533a12-c835-4096-ab22-e305aecbbc45",
   "metadata": {},
   "source": [
    "The word \"is\" appears in all 4 documents. We can see that the results of the tfid have downweighted its importance. This is evident in the 4th document where it has relatively low importance (0.171).\n",
    "\n",
    "The scikit-learn implementation is slightly different from the one above due to the `smooth_idf=True` argument which assigns zero weight to terms that appear in all documents.\n",
    "\n",
    "`TfidfTransformer` also normalises the tf-idfs directly bu applying L2-Normalisation, which returns a vector of length 1. The purpose for doing this is that the feature values become proportionate to each other.\n",
    "\n",
    "This can be verified like so..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92595c96-0c61-4e0b-a08f-150d3432723e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = tfidf.fit_transform(vectorizer.fit_transform(docs)).toarray()\n",
    "np.linalg.norm(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1659d35-6f7d-4f9a-ad33-9c2cbd032b06",
   "metadata": {},
   "source": [
    "# 3. Cleaning Text Data\n",
    "\n",
    "1. remove punctuation and html markup\n",
    "2. tokenisation\n",
    "3. removing stop words\n",
    "\n",
    "The above steps are pretty typical in NLP pipeline. There are different approaches to these, ie for neural nets I've seen different encoding strategies where things like [capitals](https://docs.fast.ai/text.core.html#replace_all_caps), html tags, unknown words etc are replaced with tags which allows the model to capture this information which may (or may not) be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f0d7c-8046-4a10-8ec6-46229d4f7d9b",
   "metadata": {},
   "source": [
    "## 3.1 Stripping Punctuation & html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3a7bc9-42f4-4ef6-97ae-3ac7c092d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: this code comes straight from the book!\n",
    "# https://sebastianraschka.com/books/\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97b812fe-d6b6-47c4-8d9e-8432c10f8690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WARNING: REVIEW CONTAINS MILD SPOILERS<br /><br />'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = df.loc[37720, 'review'][:50]\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e09bd4d6-5578-466e-a391-b0fef75be318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'warning review contains mild spoilers'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e65494e-3b66-4ba7-a65f-f83da47c3b39",
   "metadata": {},
   "source": [
    "## 3.2 Tokenisation\n",
    "\n",
    "Tokenisation is the process of splitting a document into individual elements (tokens). There are different strategies for doing this, ie word tokenisation, sentence tokenisation. Ontop of this are other techniques like *word stemming* - the process of transforming a word into it's root form ie `running` -> `run`.\n",
    "\n",
    "The [NLTK library](https://www.nltk.org/) is one of many with tools to help with stemming and lemmatisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e482e597-27ad-4fc0-bce2-00e11a0909fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokeniser(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19f64155-7640-4b86-bcb4-f98c4f8d76a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser('runners like running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f41a1699-f75d-4730-b659-3be814eb0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokeniser_stemmer(text):\n",
    "    return [stemmer.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e96b61ef-9c9d-45fe-a718-32d450b10507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser_stemmer('runners like running')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c699930-ac8b-4672-ab5b-07fb9bc2b62c",
   "metadata": {},
   "source": [
    "## 3.3 Stop word removal\n",
    "\n",
    "Stop words are considered words that are extremely common and likely bear no useful or discrimatory information. Again, in the world of deep learning this is debateable and you should consider whether the task requires this and ultimately assess model performance to determine whether this is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97110ccf-7c8d-4878-9497-b399ab5d3788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/devindearaujo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6c60c7e-70e9-4b31-8018-0e0e30ed1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "s = 'a runner likes running and runs a lot'\n",
    "\n",
    "[w for w in tokeniser_stemmer(s) if w not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43975752-33ab-4586-9602-2b419e3e132f",
   "metadata": {},
   "source": [
    "# 4. Document Classification via logistic regression\n",
    "\n",
    "Classify movie reviews using logistic regressin, employing all of the preprocessing steps discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6237f9a7-3288-4165-aee5-aa37a0b101d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grid search to find optimal model params\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# combines TfidfTransformer & CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d7e0de8-7dca-400e-b0e7-a7b087bc3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X_train, X_test = df.loc[:25000, 'review'].values, df.loc[25000:, 'review'].values\n",
    "y_train, y_test = df.loc[:25000, 'sentiment'].values, df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869d956-f2ca-4560-a806-79b04a5fb4ec",
   "metadata": {},
   "source": [
    "## 4.1 Finding optimal model params via GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663ebc3-013d-4ef6-9403-3ec2a95063a3",
   "metadata": {},
   "source": [
    "models parameter available to use with Grid Search... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd437134-6346-4b1f-a01b-b2af7408c5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b7c87fc-cfe9-437c-b10f-a8da8a8764bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokeniser at 0x11831fdc0&gt;,\n",
       "                                              &lt;function tokeniser_stemmer at 0x118344310&gt;]},\n",
       "                         {...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokeniser at 0x11831fdc0&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       (&#x27;clf&#x27;,\n",
       "                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n",
       "                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n",
       "                          &#x27;vect__stop_words&#x27;: [None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokeniser at 0x11831fdc0&gt;,\n",
       "                                              &lt;function tokeniser_stemmer at 0x118344310&gt;]},\n",
       "                         {...\n",
       "                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n",
       "                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n",
       "                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n",
       "                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n",
       "                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n",
       "                                                &#x27;itself&#x27;, ...],\n",
       "                                               None],\n",
       "                          &#x27;vect__tokenizer&#x27;: [&lt;function tokeniser at 0x11831fdc0&gt;],\n",
       "                          &#x27;vect__use_idf&#x27;: [False]}],\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(lowercase=False)),\n",
       "                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(lowercase=False)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(solver='liblinear'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n",
       "                          'vect__ngram_range': [(1, 1)],\n",
       "                          'vect__stop_words': [None],\n",
       "                          'vect__tokenizer': [<function tokeniser at 0x11831fdc0>,\n",
       "                                              <function tokeniser_stemmer at 0x118344310>]},\n",
       "                         {...\n",
       "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokeniser at 0x11831fdc0>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    strip_accents=None,\n",
    "    lowercase=False,\n",
    "    preprocessor=None\n",
    ")\n",
    "\n",
    "# param grid\n",
    "param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokeniser, tokeniser_stemmer],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1., 10.]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokeniser],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1., 10.]   \n",
    "    }\n",
    "]\n",
    "\n",
    "# pipeline\n",
    "lr_tfidf = Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5,\n",
    "                          verbose=2, n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68abbce7-87eb-4537-9f68-c9542f3a86c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__C': 10.0,\n",
       " 'clf__penalty': 'l2',\n",
       " 'vect__ngram_range': (1, 1),\n",
       " 'vect__stop_words': None,\n",
       " 'vect__tokenizer': <function __main__.tokeniser(text)>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "348fbfaa-2ddc-47cb-b40d-069522ff1f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CV Accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "print(f'Average CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1998c79-3d4f-4190-bf06-9e1011bbc5eb",
   "metadata": {},
   "source": [
    "Using the best estimator, check classification accuracy on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0083acd7-bd20-4f22-b119-9144f3d5aee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.893\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73b8d8b-0aaf-4d4d-b145-24744aa2a8fb",
   "metadata": {},
   "source": [
    "## 4.2 Updating the Pipeline with best parameters\n",
    "\n",
    "The results demonstrate that the logistic regression model can predict whether a movie is positive or negative with 86% accuracy. Using the best parameters, retrain the logistic regression model. The `lr_tfidf` pipeline can be updated using the `set_params` method and passing in the `best_params_` from `gs_lr_tfidf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c443d13-951c-49da-a515-25112bda5ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9967112810707457"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create inference pipeline &\n",
    "# update tfidf and logistic regression params\n",
    "\n",
    "inf_pl = lr_tfidf.set_params(**gs_lr_tfidf.best_params_)\n",
    "\n",
    "# refit with best params\n",
    "inf_pl.fit(X_train, y_train)\n",
    "\n",
    "# score on training\n",
    "inf_pl.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1112711-a509-4ef5-a74f-eeb77178a855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {inf_pl.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36198b27-9f04-40f6-a25f-99b25b2634f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check on some random text\n",
    "s = np.array([\"\"\"terminator 2 was a horrible movie. the effects were good, \\n\n",
    "              but i just couldn't get onboard with Robert Patrick's character\"\"\"])\n",
    "\n",
    "inf_pl.predict(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c716a85-fdba-42fd-b6ab-3a3af88a76a4",
   "metadata": {},
   "source": [
    "# 5. Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Broadly speaking, ***topic modeling*** describes a method for assigning topics to unlabelled documents. For example, categorising a large text corpus of newspaper articles, or wiki pages. This can also be considdered a clustering task - assigning a label to simmilar sets of items, here, the items are documents.\n",
    "\n",
    "LDA is not to be confused with the matrix decomposition method Linear Discriminant Analysis, also abbreviated... to LDA. \n",
    "\n",
    "**Latent Dirichlet Allocation** (*LDA*) is a generative probabilistic model that aims to find groups of words that frequently appear together across a corpus of documents. This works on the assumption that each document is made up of mixtures of different words. The words that appear together often, become topics.\n",
    "\n",
    "The input to an LDA model is a bag-of-words model. Given this, LDA decomposes it into two new matrices..\n",
    "- a document-to-topic matrix\n",
    "- a word-to-topic matrix\n",
    "\n",
    "The decompostion works in such a way that we are able to reconstruct (with the lowest possible error) the original matrix by multiplying the two latent feature matrices together. The downside to LDA, is that the number of topics is a hyperparameter, that must be specified manually beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be6ab8-c68c-4d60-9035-9abd978f9ae3",
   "metadata": {},
   "source": [
    "## 5.1 Bag-of-words on Movie Reviews\n",
    "\n",
    "Fit a bag-of-words model using `CountVectorizer` on the movie reviews data. We can exclude words that appear too frequently across documents by setting `max_df` to 10%. The dimensionality of tha dataset can be controlled using the `max_features` argument, here 5000 is chosen arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1917d528-b4dc-4371-bfab-0053f99c6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer(\n",
    "    stop_words='english',\n",
    "    max_df=.1,\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "X = vect.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37522829-148a-432f-bf16-3244ff12f3a3",
   "metadata": {},
   "source": [
    "## 5.1 Fitting the LDA model\n",
    "\n",
    "With a total of 10 topics..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0a8f3d71-c308-416b-af71-1498dd41acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    random_state=123,\n",
    "    learning_method='batch',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aec57f5e-00c5-4645-a0ff-fe456415efe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15d1a979-01d9-4ce1-9fdd-400ba8b6e5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : \n",
      "worst minutes script awful stupid terrible\n",
      "Topic 2 : \n",
      "family mother father children girl women\n",
      "Topic 3 : \n",
      "war american dvd music tv history\n",
      "Topic 4 : \n",
      "human audience cinema art sense feel\n",
      "Topic 5 : \n",
      "police guy car dead murder goes\n",
      "Topic 6 : \n",
      "horror house sex blood girl woman\n",
      "Topic 7 : \n",
      "role performance comedy actor plays performances\n",
      "Topic 8 : \n",
      "series episode episodes tv season original\n",
      "Topic 9 : \n",
      "book version original read effects fi\n",
      "Topic 10 : \n",
      "action fight guy guys fun cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 6\n",
    "feature_names = vect.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)} : ')\n",
    "    print(' '.join([feature_names[i] \n",
    "                   for i in topic.argsort()\\\n",
    "                   [:-n_top_words -1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d056249-472a-41d3-a641-7cd4eec19f1a",
   "metadata": {},
   "source": [
    "based on the most important words for each topic we can make a general assumption about the review topics...\n",
    "\n",
    "1. generally terrible movie reviews\n",
    "2. movies about families\n",
    "3. history/war movies\n",
    "4. art/arthouse movies\n",
    "5. crime films\n",
    "6. horror films\n",
    "7. comedy films\n",
    "8. tv series or shows\n",
    "9. movies based on books\n",
    "10. action movies\n",
    "\n",
    "To confirm our assumptions, we can print out sections of reviews from a particular category, say crime films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d164b747-a78f-4628-be16-bedb23a7197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n",
      "\n",
      "Horror movie #2:\n",
      "Before I talk about the ending of this film I will talk about the plot. Some dude named Gerald breaks his engagement to Kitty and runs off to Craven Castle in Scotland. After several months Kitty and her aunt venture off to Scottland. Arriving at Craven Castle Kitty finds that Gerald has aged and he ...\n",
      "\n",
      "Horror movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror_idx = X_topics[:, 5].argsort()[::-1] # sort descending\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(horror_idx[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'].iloc[movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "c0682833-5f32-41be-b169-289fad3c9eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comedy movie #1:\n",
      "From producer/writer/Golden Globe nominated director James L. Brooks (Terms of Endearment, As Good as It Gets) this is a really good satirical comedy film showing behind the scenes in the life of a news reporter/anchor/journalist or producer might be like. Basically Jane Craig (Oscar and Golden Glob ...\n",
      "\n",
      "Comedy movie #2:\n",
      "THE SUNSHINE BOYS was the hilarious 1975 screen adaptation of Neil Simon's play about a retired vaudevillian team, played by Walter Matthau and George Burns, who had a very bitter breakup and have been asked to reunite one more time for a television special or something like that. The problem is tha ...\n",
      "\n",
      "Comedy movie #3:\n",
      "As far as I know the real guy that the main actor is playing saw his performance and said it was an outstanding portrayal, I'd agree with him. This is a fantastic film about a quite gifted boy/man with a special body part helping him. Oscar and BAFTA winning, and Golden Globe nominated Daniel Day-Le ...\n"
     ]
    }
   ],
   "source": [
    "comedy_idx = X_topics[:, 6].argsort()[::-1] # sort descending\n",
    "\n",
    "for iter_idx, movie_idx in enumerate(comedy_idx[:3]):\n",
    "    print(f'\\nComedy movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'].iloc[movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3647b214-8940-4e0d-871d-696b5941247c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In the following notebook we can see how even a vanilla implementation of document classification with logistic regression is able to accurately predict whether a review is positive or negative. Following this, using LDA is an effective method for classifying documents based on topics extracted from the raw text input."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
